[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Project portfolio",
    "section": "",
    "text": "From clouds to code: mapping big climate data with Python\n\n\n\nGeospatial-analysis\n\n\nPython\n\n\nMESM\n\n\n\nAn open-source Python workflow for accessing, processing, and mapping large-scale historical and projected climate model data\n\n\n\nMaxwell Pepperdine\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping suitable nesting habitat for an endangered bird in the SCM\n\n\n\nGeospatial-analysis\n\n\nMachine-learning\n\n\nConservation-planning\n\n\nRStudio\n\n\nArcGIS Pro\n\n\nMaxEnt\n\n\nMESM\n\n\n\nDelineating suitable nesting habitat for marbled murrelets following severe wildfires in the Santa Cruz Mountains (SCM)\n\n\n\nMaxwell Pepperdine\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine aquacultire suitability on the West Coast\n\n\n\nGeospatial-analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nBuilding reproducible workflows to model aquaculture habitat through spatial analysis\n\n\n\nMaxwell Pepperdine\n\n\nNov 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating EJ implications of the Houston blackout\n\n\n\nGeospatial-analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nA spatial analysis of the Houston blackout and affected communities\n\n\n\nMaxwell Pepperdine\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring patterns of environmental justice in Los Angeles County\n\n\n\nGeospatial-analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nA spatial analysis to analyze the legacy of historical redlining in Los Angeles County\n\n\n\nMaxwell Pepperdine\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the distribution of native and invasive marine species in the Indo-Pacific region\n\n\n\nConservation-planning\n\n\nRStudio\n\n\nArcGIS Pro\n\n\nMaxEnt\n\n\nMESM\n\n\n\nUsing MaxEnt to model current and future distributions of grape coral and crown-of-thorn starfish\n\n\n\nMaxwell Pepperdine, Zoe Zhou\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInforming reserve design in the Morro Bay Watershed\n\n\n\nConservation-planning\n\n\nRStudio\n\n\nArcGIS Pro\n\n\nPrioritizR\n\n\nMESM\n\n\n\nReserve design in the Morro Bay watershed prioritizing special status species\n\n\n\nMaxwell Pepperdine\n\n\nSep 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring habitat connectivity between core areas in a Costa Rican landscape\n\n\n\nGeospatial-analysis\n\n\nConservation-planning\n\n\nArcGIS Pro\n\n\nMESM\n\n\n\nModeling movement patterns and connectivity for the jaguar (Panthera onca) across southern Costa Rica with Circuitscape\n\n\n\nMaxwell Pepperdine\n\n\nSep 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing crop yields with data analysis\n\n\n\nData analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nRunning non-linear least squares to predict crop yields in Greece\n\n\n\nMaxwell Pepperdine\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlant classification using logistic regression\n\n\n\nData analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nBuilding models to identify plant species with binary logistic regression and cross validation\n\n\n\nMaxwell Pepperdine\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing water chemistry in Santa Barbara streams with a cluster analysis\n\n\n\nData analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nUsing complete linkage agglomerative hierarchical clustering to compare water chemistry by stream sites\n\n\n\nMaxwell Pepperdine\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime series analysis\n\n\n\nData analysis\n\n\nRStudio\n\n\nQuarto\n\n\nMESM\n\n\n\nAssessing temporal patterns of coho salmon and steelhead trout migration\n\n\n\nMaxwell Pepperdine\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-01-16-mamu-scm/index.html",
    "href": "posts/2025-01-16-mamu-scm/index.html",
    "title": "Mapping suitable nesting habitat for an endangered bird in the SCM",
    "section": "",
    "text": "This analysis was completed as part of a quarter-long project to develop a conservation plan in ESM 270p (Conservation Planning Practicum) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The project focused on delineating suitable nesting habitat for the marbled murrelet (Brachyramphus marmoratus) following severe wildfires in the Santa Cruz Mountains (SCM). The background and objectives of the project are described below, as well as some of the main figures. If you’re interested in reading and learning more about the project, please find links to the full project report and GitHub repository below:\n\nFull Project Report\nGitHub Repository: mamu-conservation-scm"
  },
  {
    "objectID": "posts/2025-01-16-mamu-scm/index.html#overview",
    "href": "posts/2025-01-16-mamu-scm/index.html#overview",
    "title": "Mapping suitable nesting habitat for an endangered bird in the SCM",
    "section": "",
    "text": "This analysis was completed as part of a quarter-long project to develop a conservation plan in ESM 270p (Conservation Planning Practicum) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The project focused on delineating suitable nesting habitat for the marbled murrelet (Brachyramphus marmoratus) following severe wildfires in the Santa Cruz Mountains (SCM). The background and objectives of the project are described below, as well as some of the main figures. If you’re interested in reading and learning more about the project, please find links to the full project report and GitHub repository below:\n\nFull Project Report\nGitHub Repository: mamu-conservation-scm"
  },
  {
    "objectID": "posts/2025-01-16-mamu-scm/index.html#background",
    "href": "posts/2025-01-16-mamu-scm/index.html#background",
    "title": "Mapping suitable nesting habitat for an endangered bird in the SCM",
    "section": "Background",
    "text": "Background\nRecovery and preservation of imperiled species are a major focus of conservation planning and one of the key conservation outcomes driving management actions. An avian species that is both imperiled and considered an umbrella species is the marbled murrelet (Brachyramphus marmoratus), a small seabird in the family Alcidae that forages at sea and nests on the large limbs of coastal old-growth forests. They spend most of their lives in near-shore marine environments, flying inland only to breed and nest in the tall canopies of coastal forests. Their at-sea distribution runs along the Pacific Coast from the Aleutian Islands and southern Alaska to southern California, and their breeding range extends from southern Alaska to central California.\nPrimarily due to loss of habitat from logging and other anthropogenic pressures, the marbled murrelet was listed as “threatened” by the U.S. Fish and Wildlife Service (USFWS) under the Endangered Species Act in 1992 in California, Oregon, and Washington. Later that year, they were listed as “endangered” by the State of California under the California Endangered Species Act. The USFWS designated critical habitat for marbled murrelets in 1995 and then prepared a Federal Recovery Plan in 1997. This plan divided the breeding range of murrelets into six different conservation zones. Zone 6 includes the forested coastal habitats up to 24 km inland from San Francisco Bay to Point Sur in Monterey County, and the population of marbled murrelets in this zone nests primarily in the Santa Cruz Mountain (SCM) region. The SCM population of murrelets has been declining due to a variety of factors, including but not limited to wildfire events, land use change limiting the distribution and availability of contiguous breeding habitat, and increased predation from predators like corvids.\nThe 2020 CZU Lightning Complex wildfires burned more than 35,000 ha of forest land in Santa Cruz and San Mateo Counties. The majority of areas characterized by high burn severity fell within Big Basin Redwoods State Park (BBRSP), which is a known historic stronghold of suitable nesting trees for marbled murrelets. Figure 1 portrays the USFWS-designated critical habitat for murrelets overlaid with the CZU fire soil burn severity, showing the amount of critical habitat within BBRSP that burned severely during the fires. This geographically separated and endangered population of marbled murrelets was declining prior to these fires decimating their prime nesting habitat. These cumulative threats and recent losses emphasize a need to focus on the top conservation actions for murrelets’ recovery in the Santa Cruz Mountains. A recommended strategy for murrelets in USFWS Zone 6 is to identify, protect, and restore key habitats that can serve as suitable nesting sites (Halbert & Singer, 2017). This project aims to help address this conservation goal through the objectives:\n\nEmploy a science-based process to inform habitat preservation, conservation, and recovery efforts for murrelets in the SCM.\nMap and delineate the occurrence probability and geographical distribution of suitable nesting habitat using MaxEnt modeling, before and after the 2020 CZU Lighting Complex Fires.\nPrioritize county parcels for future land acquisition and protection efforts, conservation easements with local land owners, and acoustic monitoring through a series of spatial overlays and intersections with the MaxEnt results and key data layers (e.g., post-fire suitable habitat, county parcels, and protected areas)."
  },
  {
    "objectID": "posts/2025-01-16-mamu-scm/index.html#figures",
    "href": "posts/2025-01-16-mamu-scm/index.html#figures",
    "title": "Mapping suitable nesting habitat for an endangered bird in the SCM",
    "section": "Figures",
    "text": "Figures\n\n\n\nFigure 1. CZU Lightning Complex burn severity overlaid with marbled murrelet critical habitat. Much of the USFWS-designated critical habitat was burned during these fires, especially regions of Big Basin Redwoods State Park which lies at the heart of the burn scar and map inset.\n\n\n\n\n\nFigure 2. Environmental predictor variables used in the analysis\n\n\n\n\n\nFigure 3. Marbled murrelet probability of occurrence throughout the Santa Cruz Mountains using predictions from the best performing MaxEnt model (fc.LQH_rm.1). The top maps (A/B) show the model generated with pre-fire occurrence data and vegetation conditions, and the bottom maps (C/D) show the predictions of this model transferred to the post-fire forest structure conditions. Looking within the boundaries of the CZU Lightning Complex Fire burn scar, delineated by the black line, it is evident that a significant area of suitable habitat was lost.\n\n\n\n\n\nFigure 4. Habitat suitability models for marbled murrelets under pre-fire (A) and post-fie (B) vegetation structure conditions. The differences between these models (C) show areas that gained, lost, or did not change suitability for nesting habitat due to the CZU fires.\n\n\n\n\n\nFigure 5. Post-CZU fire modeled suitable nesting habitat for marbled murrelets overlaid with all county parcels in the Santa Cruz Mountain region. In the land prioritization analysis, the area of post-fire suitable habitat within each parcel was calculated to determine parcels where conservation actions and surveying efforts should be focused.\n\n\n\n\n\nFigure 6. Protected or unprotected county parcels (green) and only unprotected county parcels (blue) in the SCM with the largest area of suitable marbled murrelet habitat within their boundaries.\n\n\n\n\n\nFigure 7. County parcels in the SCM region that have the greatest area (ha) of suitable habitat for marbled murrelets within their boundary. (A) Shows the results when considering all county parcels in the SCM region, whether protected or unprotected. (B) Shows the results of only unprotected county parcels. The unprotected parcel with the highest area of suitable habitat (083320070) was also in the top 10 of all parcels in the SCM region whether protected or unprotected. Protected status was determined using the California Protected Areas Database (CPAD)."
  },
  {
    "objectID": "posts/2025-01-16-mamu-scm/index.html#acknowledgements",
    "href": "posts/2025-01-16-mamu-scm/index.html#acknowledgements",
    "title": "Mapping suitable nesting habitat for an endangered bird in the SCM",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized Ashley Larsen, an Associate Professor at the Bren School and the instructor for ESM 270p. ESM 270p (Conservation Planning Practicum) is offered in the Master of Environmental Science & Management (MESM) program at the Bren School as one the foundational courses for the Conservation Planning specialization."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html",
    "href": "posts/2024-11-05-redlining-legacies/index.html",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "",
    "text": "Legacies of historical injustices are often reflected in present-day environmental injustices. One example of this is racial segregation in the United States, in which a long history of these injustices are still visible today. As part of the New Deal in the 1930’s, the Home Owners’ Loan Corporation (HOLC) classified neighborhoods based on their perceived safety or risk for mortgage lending (hereby refered to as ‘HOLC grade’). Thy used this ranking system (HOLC grades of A, B, C, D) to block access to loans for home ownership, delineating maps to indicate where mortgages and loans should not be invested. This practice, colloquially known as “redlining,” disproportionately affected communities of color and has had lasting impacts on the socio-economic and environmental conditions of these neighborhoods that are still evident today.\nThis analysis aims to explore the legacy of redlining in Los Angeles County by examining the distribution of environmental and biodiversity observations within each HOLC grade. The analysis will be divided into two parts:\n\nLegacy of redlining in current environmental (in)justice: This section will explore the relationship between HOLC grades and three environmental justice (EJ) screen variables in Los Angeles County: % low income, percentile for PM 2.5, and percentile for low life expectancy. The analysis will examine how the current conditions of these variables differ with the four HOLC grades.\nLegacy of redlining in biodiversity observations: This section will examine the distribution of bird observations from 2022 within each HOLC grade in Los Angeles County to explore how the distribution of biodiversity observations has been influenced by historical redlining practices."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#background",
    "href": "posts/2024-11-05-redlining-legacies/index.html#background",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "",
    "text": "Legacies of historical injustices are often reflected in present-day environmental injustices. One example of this is racial segregation in the United States, in which a long history of these injustices are still visible today. As part of the New Deal in the 1930’s, the Home Owners’ Loan Corporation (HOLC) classified neighborhoods based on their perceived safety or risk for mortgage lending (hereby refered to as ‘HOLC grade’). Thy used this ranking system (HOLC grades of A, B, C, D) to block access to loans for home ownership, delineating maps to indicate where mortgages and loans should not be invested. This practice, colloquially known as “redlining,” disproportionately affected communities of color and has had lasting impacts on the socio-economic and environmental conditions of these neighborhoods that are still evident today.\nThis analysis aims to explore the legacy of redlining in Los Angeles County by examining the distribution of environmental and biodiversity observations within each HOLC grade. The analysis will be divided into two parts:\n\nLegacy of redlining in current environmental (in)justice: This section will explore the relationship between HOLC grades and three environmental justice (EJ) screen variables in Los Angeles County: % low income, percentile for PM 2.5, and percentile for low life expectancy. The analysis will examine how the current conditions of these variables differ with the four HOLC grades.\nLegacy of redlining in biodiversity observations: This section will examine the distribution of bird observations from 2022 within each HOLC grade in Los Angeles County to explore how the distribution of biodiversity observations has been influenced by historical redlining practices."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "href": "posts/2024-11-05-redlining-legacies/index.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "Part 1: Legacy of redlining in current environmental (in)justice",
    "text": "Part 1: Legacy of redlining in current environmental (in)justice\n\nLoad packages\n\n\nShow packages used in this analysis\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\nlibrary(tmap)\nlibrary(kableExtra)\nlibrary(tmaptools)\nlibrary(leaflet)\nlibrary(patchwork)\n\n\n\n\nLoad data\n\n\nShow the code\n# redlining data \nholc_redlining &lt;- st_read(here(\"posts/2024-11-05-redlining-legacies/data/mapping-inequality/mapping-inequality-los-angeles.json\"))\nholc_redlining &lt;- holc_redlining %&gt;% \n  filter(st_is_valid(holc_redlining)) # remove invalid geometries\n\n# EJ screen data\nej_screen &lt;- st_read(here(\"posts/2024-11-05-redlining-legacies/data/ejscreen/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\"))\nej_screen &lt;- ej_screen %&gt;% \n  filter(st_is_valid(ej_screen)) %&gt;% # remove invalid geometries\n  st_transform(crs = st_crs(holc_redlining)) # set same CRS as redlining data\n\n# filter EJ screen data to LA county\nej_screen_la &lt;- ej_screen %&gt;% \n  filter(STATE_NAME == \"California\" & CNTY_NAME == \"Los Angeles County\")\n\n# LA county boundary; sourced from City of Los Angeles Hub\nla_county &lt;- st_read(here(\"posts/2024-11-05-redlining-legacies/data/la_county_boundary/County_Boundary.shp\")) %&gt;% \n  st_transform(crs = st_crs(holc_redlining)) # set same CRS as redlining data\nla_county &lt;- la_county %&gt;%\n  filter(st_is_valid(la_county)) # remove invalid geometries\n\n\n\nQC #1: Are there any invalid geometries in the layers that will be used?\n\n\nShow the code\n# check for invalid geometries in the redlining data with testthat\nif(testthat::expect_true(all(st_is_valid(holc_redlining)))) {\n  print(\"No invalid geometries in the redlining data\")\n} else {\n  warning(\"There are invalid geometries in the redlining data\")\n}\n\n\n[1] \"No invalid geometries in the redlining data\"\n\n\n\n\nShow the code\n# check for invalid geometries in the EJ screen data\nif(testthat::expect_true(all(st_is_valid(ej_screen_la)))) {\n  print(\"No invalid geometries in the EJ screen data\")\n} else {\n  warning(\"There are invalid geometries in the EJ screen data\")\n}\n\n\n[1] \"No invalid geometries in the EJ screen data\"\n\n\n\n\n\nMake a map with tmap\n\n\nShow the map code\n# set plot mode to interactive to activate basemaps\ntmap_mode(\"view\")\n\n# make the map\ntm_basemap(\"Esri.WorldTopoMap\", alpha = 0.7) + # add a basemap\ntm_shape(holc_redlining, \n         name = \"Historical redlining\") +\n  tm_fill(col = \"grade\", palette = \"-RdYlBu\", # color by grade; reverse pal\n          title = \"HOLC Grade\") + # clean up the layer name\n  tm_borders() + \ntm_shape(la_county, \n         name = \"LA County boundary\") +\n  tm_borders(lwd = 1.5) + \n  tm_scale_bar(position = c(\"top\", \"right\")) \n\n\n\n\n\n\n\n\nFigure 1: Historical redlining neighborhoods in Los Angeles County. HOLC grades of A are represented in dark blue, B by light blue, C by yellow, and D by red. Areas with no HOLC grade, or ‘missing’ grades, are shown in grey. The Los Angeles County boundary is also portrayed with the black line to provide additional geospatial context. HOLC grade data was sourced from Nelson et al. (2023) and the LA County boundary was sourced from the City of Los Angeles GeoHub.\n\n\n\n\nShow the map code\n## commented out for interactive maps; the compass function doesn't work\n# tm_layout(legend.position = c(\"right\", \"bottom\")) +\n#   tm_compass(position = c(\"left\", \"bottom\"))\n\n\n\n\nDetermine the % of current census block groups within each HOLC grade\nPseudocode outline:\n\nCheck if the EJ screen and HOLC redlining data are in the same Coordinate Reference System (CRS) using st_crs(). If they are not in the same CRS, reproject one of the datasets to match the other.\nJoin the HOLC redlining data to the EJ screen data. In this step, I used a spatial join with st_join() because this joins based on geometries that intersect by default. st_join() also performs a left join by default, so this will keep all the EJ screen data which is already at the block group level and add the HOLC grade to each observation.\nCalculate the % census blocks groups within in HOLC grade (A, B, C, D). Group the data by HOLC grade using group_by() and then calculate the % of block groups in each grade. I used the n() function to count the number of block groups in each grade, and then calculated the % of block groups in each grade by dividing the count number by the sum of blocks counted.\nClean up the data and show the results in a table (Table 1 below).\n\n\nQC #2: Are the EJ screen and redlining data in the same CRS?\n\n\nShow the code\n# make sure the CRS of the EJ screen data and HOLC redlining are the same\nif(st_crs(ej_screen_la) == st_crs(holc_redlining)) {\n  print(\"The EJ screen and HOLC redlining data are in the same CRS\")\n} else {\n  warning(\"The EJ screen and HOLC redlining data are NOT in the same CRS\")\n}\n\n\n[1] \"The EJ screen and HOLC redlining data are in the same CRS\"\n\n\n\n\nCalculate the % of block groups within each HOLC grade\n\n\nShow the code\n# join the redlining data to the EJ screen data; left join by default\nej_screen_redlining &lt;- st_join(ej_screen_la, holc_redlining)\n\n# calculate the % of each HOLC grade in the EJ screen data\nej_screen_redlining_pct &lt;- ej_screen_redlining %&gt;%\n  group_by(grade) %&gt;% # group by HOLC grade\n  summarize(n = n()) %&gt;% # count the number of block groups in each grade\n  mutate(pct_grade = n / sum(n) * 100) # add new column of % block groups in each grade\n\n\n\n##### cleaning up the data for the table\n\n# rename some columns and drop the geometry column\nej_screen_redlining_pct_clean &lt;- ej_screen_redlining_pct %&gt;% \n  rename(\"HOLC Grade\" = grade) %&gt;% # make the column names cleaner\n  rename(\"% of Block Groups\" = pct_grade) %&gt;%\n  select(\"HOLC Grade\", \"% of Block Groups\") %&gt;% \n  st_drop_geometry() # drop the geometry column\n\n# change the NA value in the HOLC Grade column to \"None\"\n# if not an NA value, leave the value as is\nej_screen_redlining_pct_clean$`HOLC Grade` &lt;- \n  ifelse(is.na(ej_screen_redlining_pct_clean$`HOLC Grade`), \"None\", \n         ej_screen_redlining_pct_clean$`HOLC Grade`)\n\n# change the # of decimals in the % column to 2\nej_screen_redlining_pct_clean$`% of Block Groups` &lt;- \n  round(ej_screen_redlining_pct_clean$`% of Block Groups`, 2)\n\n\n\n\nMake a table to show the results\n\n\nShow the code\n# make a table of the % of each HOLC grade in the EJ screen data\nej_screen_redlining_pct_clean %&gt;%\n  kable() %&gt;%\n  kable_styling(position = \"center\", \n                row_label_position = \"c\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\n\nTable 1: Percentage of current census block groups within each HOLC grade.\n\n\n\n\n\n\nHOLC Grade\n% of Block Groups\n\n\n\n\nA\n5.10\n\n\nB\n13.70\n\n\nC\n31.23\n\n\nD\n14.77\n\n\nNone\n35.19\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore conditions of different EJ Screen variables by HOLC grade\n\nQC #3: Are the EJ screen and redlining data still in the same CRS?\n\n\nShow the code\n# make sure the CRS of the EJ screen data and HOLC redlining are the same\nif(st_crs(ej_screen_la) == st_crs(holc_redlining)) {\n  print(\"The EJ screen and HOLC redlining data are in the same CRS\")\n} else {\n  warning(\"The EJ screen and HOLC redlining data are NOT in the same CRS\")\n}\n\n\n[1] \"The EJ screen and HOLC redlining data are in the same CRS\"\n\n\n\n\nOverview of analysis & variables\n\nAnalysis goal:\n\nThis analysis aims to explore the relationship between the Home Owners’ Loan Corporation (HOLC) grades and three EJ Screen variables in Los Angeles County. To examine how the current conditions of these variables in census block groups differ with the four HOLC grades, I calculated the mean of each variable within each HOLC grade. Figure 2 below shows the results of this analysis.\n\nVariables of interest:\n\n% low income (LOWINCPCT): Taken from the EPA EJ Screen documentation: “The percent of a block group’s population in households where the household income is less than or equal to twice the federal ‘poverty level.’”\nPercentile for PM 2.5 (P_PM25): Taken from the EPA EJ Screen documentation: “EJScreen presents PM2.5 concentrations using percentile rank, ranging from 0 (lowest) to 100 (highest).”\nPercentile for low life expectancy (P_LIFEEXPPCT): Taken from the EPA EJ Screen documentation: EJScreen presents life expectancy using percentile rank, ranging from 0 (lowest) to 100 (highest) low life expectancy. A higher percentile indicates a lower life expectancy.\n\n\n\n\nData wrangling\nSteps/Pseudocode outline:\n\nFilter the EJ Screen data to create three new data frames that contain the EJ Screen variables of interest in LA County: % low income (LOWINCPCT), percentile for PM 2.5 (P_PM25), and percentile for low life expectancy (P_LIFEEXPPCT)\nWith each of the three variables created, join the EJ Screen data to the HOLC redlining data. In this step, I again used a spatial join with st_join() because this joins based on geometries that intersect by default. st_join() also performs a left join by default, so this will keep all the EJ Screen data and add the HOLC grade to each observation.\nCalculate the mean of each variable for each HOLC grade. This is done by grouping the data by HOLC grade using group_by() and then calculating the mean of the variable of interest. I used the na.rm = TRUE argument in the mean() function to remove any NA values in the data.\nMake a set of figures to show the results, and then combine them into one with the patchwork package.\n\n\n\nShow the code\n## filter the EJ screen data to create three data frames, one for each variable\nej_screen_low_income &lt;- ej_screen_la %&gt;% \n  select(ID, LOWINCPCT) # % low income\n\nej_screen_pm25 &lt;- ej_screen_la %&gt;%\n  select(ID, P_PM25) # percentile for PM 2.5\n\nej_screen_life_exp &lt;- ej_screen_la %&gt;%\n  select(ID, P_LIFEEXPPCT) # percentile for low life expectancy\n\n\n## join the EJ screen data to the HOLC redlining data for each variable\nej_screen_redlining_low_income &lt;- st_join(ej_screen_low_income, holc_redlining)\n\nej_screen_redlining_pm25 &lt;- st_join(ej_screen_pm25, holc_redlining)\n\nej_screen_redlining_life_exp &lt;- st_join(ej_screen_life_exp, holc_redlining)\n\n\n#### calculate the mean of each variable for each HOLC grade ####\n\n# low income \nej_screen_redlining_low_income_mean &lt;- ej_screen_redlining_low_income %&gt;%\n  group_by(grade) %&gt;%\n  summarize(mean_low_income = mean(LOWINCPCT, na.rm = TRUE))\n# change NA value in the HOLC Grade column to \"None\"; same method as above\nej_screen_redlining_low_income_mean$grade &lt;- \n  ifelse(is.na(ej_screen_redlining_low_income_mean$grade), \"None\", \n         ej_screen_redlining_low_income_mean$grade)\n\n\n# PM 2.5\nej_screen_redlining_pm25_mean &lt;- ej_screen_redlining_pm25 %&gt;%\n  group_by(grade) %&gt;%\n  summarize(mean_pm25 = mean(P_PM25, na.rm = TRUE))\n# change NA value in the HOLC Grade column to \"None\"\nej_screen_redlining_pm25_mean$grade &lt;- \n  ifelse(is.na(ej_screen_redlining_pm25_mean$grade), \"None\", \n         ej_screen_redlining_pm25_mean$grade)\n\n\n# low life expectancy\nej_screen_redlining_life_exp_mean &lt;- ej_screen_redlining_life_exp %&gt;%\n  group_by(grade) %&gt;%\n  summarize(mean_life_exp = mean(P_LIFEEXPPCT, na.rm = TRUE))\n# change NA value in the HOLC Grade column to \"None\"\nej_screen_redlining_life_exp_mean$grade &lt;- \n  ifelse(is.na(ej_screen_redlining_life_exp_mean$grade), \"None\", \n         ej_screen_redlining_life_exp_mean$grade)\n\n\n\n\nMake a set of figures (one for each variable)\n\n\nShow the code\n# % low income figure \npct_low_income &lt;- ggplot(ej_screen_redlining_low_income_mean, \n                        aes(x = grade, y = mean_low_income, \n                            fill = grade)) +\n  geom_col(col = \"black\") +\n  labs(title = \" \",\n       x = \" \",\n       y = \"% low income\") +\n  scale_fill_manual(values = c(\"A\" = \"limegreen\", \n                               \"B\" = \"royalblue2\", \n                               \"C\" = \"gold\", \n                               \"D\" = \"firebrick3\", \n                               \"None\" = \"grey\")) +\n  theme_classic()\npct_low_income &lt;- pct_low_income + \n  theme(legend.position = \"none\")\n\n\n\n\nShow the code\n# PM 2.5 figure\npm25 &lt;- ggplot(ej_screen_redlining_pm25_mean, \n               aes(x = grade, y = mean_pm25, \n                   fill = grade)) +\n  geom_col(col = \"black\") +\n  labs(title = \" \",\n       x = \"HOLC Grade\",\n       y = \"percentile for PM 2.5\") +\n  scale_fill_manual(values = c(\"A\" = \"limegreen\", \n                               \"B\" = \"royalblue2\", \n                               \"C\" = \"gold\", \n                               \"D\" = \"firebrick3\", \n                               \"None\" = \"grey\")) +\n  theme_classic()\npm25 &lt;- pm25 +\n  theme(legend.position = \"none\")\n\n\n\n\nShow the code\n# low life expectancy figure\nlife_exp &lt;- ggplot(ej_screen_redlining_life_exp_mean, \n                   aes(x = grade, y = mean_life_exp, \n                       fill = grade)) +\n  geom_col(col = \"black\") +\n  labs(title = \" \",\n       x = \" \",\n       y = \"percentile for low life expectancy\") +\n  scale_fill_manual(values = c(\"A\" = \"limegreen\", \n                               \"B\" = \"royalblue2\", \n                               \"C\" = \"gold\", \n                               \"D\" = \"firebrick3\", \n                               \"None\" = \"grey\")) +\n  theme_classic()\nlife_exp &lt;- life_exp +\n  theme(legend.position = \"none\")\n\n\n\n\nCombine the three figures into one with patchwork\n\n\nShow the code\n# make a grid of the three figures\ncombined_figures &lt;- pct_low_income + pm25 + life_exp\ncombined_figures + plot_annotation(title = \"EJ Screen variables by HOLC grade\", \n                                   tag_levels = \"A\") # label figures A,B,C\n\n\n\n\n\n\n\n\nFigure 2: EJ Screen variables by HOLC grade. (A) % low income, (B) percentile for PM 2.5, (C) percentile for low life expectancy. All values represent the mean value of each EJ Screen variable by HOLC grade. Looking at the three figures, it is clear that areas with lower HOLC grades have higher % low income, higher PM 2.5 percentiles, and higher low life expectancy percentiles.\n\n\n\n\n\n\n\n\nInterpretation of Results\nLooking at the results of the analysis shown in Figure 2 above, it is clear that areas with lower HOLC grades (i.e., D) have higher % low income, higher percentiles for particulate matter 2.5, and higher percentiles for low life expectancy. This suggests that the legacy of redlining has had long lasting impacts on the socio-economic and environmental conditions of these neighborhoods, and that these present day environmental injustices reflect patterns of historical injustice. Given that that this analysis only explored 3 environmental and demographic variables, future research could expand on this analysis by examining additional information offered on EJ screen and exploring their conditions within different HOLC grades."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "href": "posts/2024-11-05-redlining-legacies/index.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "Part 2: Legacy of redlining in biodiversity observations",
    "text": "Part 2: Legacy of redlining in biodiversity observations\n\nBackground\nA recent study by Ellis-Soto et al. (2023) found that historical redlining has influenced our observations of biodiversity along with the socio-economic and environmental conditions of communities. Looking across 195 US cities, they found that redlined neighborhoods remain the most under sampled areas for biodiversity. One reason that makes this disparity concerning is that conservation decisions and management actions are often made based on available observation data. This can lead to biased selection of locations for conservation, inequitable distribution of the many benefits that result from being adjacent or close to conservation projects, and incomplete representations of biodiversity.\nTo explore this further, this analysis will examine the distribution of bird observations from 2022 within each HOLC grade in Los Angeles County using publicly accessible data from Nelson et al. (2023) and the Global Biodiversity Information Facility (GBIF).\n\n\nLoad data\n\n\nShow the code\n# GBIF birds data\nbirds &lt;- st_read(here(\"posts/2024-11-05-redlining-legacies/data/gbif-birds-LA/gbif-birds-LA.shp\"))\nst_crs(birds) # WGS 84; same as HOLC redlining, but we should still check\n\n# filter to only include data from 2022 bc the assignment\n# asks to analyze observations from 2022\nbirds &lt;- birds %&gt;% \n  filter(year == 2022)\n\n\n\n\nQC #4: Are the GBIF birds data and redlining data in the same CRS?\n\n\nShow the code\n# make sure the CRS of the GBIF birds data and HOLC redlining are the same\nif(st_crs(birds) == st_crs(holc_redlining)) {\n  print(\"The GBIF birds data and HOLC redlining data are in the same CRS\")\n} else {\n  warning(\"The GBIF birds data and HOLC redlining data are NOT in the same CRS\")\n}\n\n\n[1] \"The GBIF birds data and HOLC redlining data are in the same CRS\"\n\n\n\n\nQC #5: Does the birds data only include observations from 2022?\n\n\nShow the code\n# check if the birds data only includes observations from 2022\nif(all(birds$year == 2022)) {\n  print(\"The GBIF birds data only includes observations from 2022\")\n} else {\n  warning(\"Stop! The GBIF birds data includes observations from other years\")\n}\n\n\n[1] \"The GBIF birds data only includes observations from 2022\"\n\n\n\n\nData wrangling\nSteps/Pseudocode outline:\n\nJoin the GBIF birds data with observations from 2022 to the HOLC redlining data. In this step, I again used a spatial join with st_join() because this joins based on geometries that intersect by default. st_join() also performs a left join by default, so this will keep all the GBIF birds data and add the HOLC grade to each observation.\nCalculate the percent of observations within each HOLC grade. This is done by grouping the data by HOLC grade using group_by() and then calculating the percent of observations in each grade. I used the n() function to count the number of observations in each grade and then calculated the percent of observations in each grade by dividing this number by the sum of observations.\nMake a figure to show the results.\n\n\n\nShow the code\n# join the HOLC redlining data to the GBIF birds data\nbirds_redlining &lt;- st_join(birds, holc_redlining)\n\n# calculate the % of observations within each HOLC grade\nbirds_redlining_pct &lt;- birds_redlining %&gt;%\n  group_by(grade) %&gt;%\n  summarize(n = n()) %&gt;% # count the number of observations in each grade\n  mutate(pct_grade = n / sum(n) * 100) # add new column of % obsvs. in each grade\n\n# change NA value in the HOLC Grade column to \"None\"; same method as above\nbirds_redlining_pct$grade &lt;- \n  ifelse(is.na(birds_redlining_pct$grade), \"None\", \n         birds_redlining_pct$grade)\n\n# change the # of decimals in the % column to 2\nbirds_redlining_pct$pct_grade &lt;- \n  round(birds_redlining_pct$pct_grade, 2)\n\n# make a new object without the None observations \nbirds_redlining_pct_no_none &lt;- birds_redlining_pct %&gt;% \n  filter(grade != \"None\")\n\n\n\n\nMake a two figures & combine them with patchwork\n\n\nShow the code\n# make a bar plot of the % of observations in each HOLC grade\nbirds_plot_none &lt;- ggplot(birds_redlining_pct, \n                     aes(x = grade, y = pct_grade, \n                         fill = grade)) +\n  geom_col(col = \"black\") +\n  labs(title = \" \",\n       x = \"HOLC Grade\",\n       y = \"% of Observations\") +\n  scale_fill_manual(values = c(\"A\" = \"limegreen\", \n                               \"B\" = \"royalblue2\", \n                               \"C\" = \"gold\", \n                               \"D\" = \"firebrick3\", \n                               \"None\" = \"grey\")) +\n  theme_classic()\nbirds_plot_none &lt;- birds_plot_none + \n  theme(legend.position = \"none\")\n\n# make a plot without the None category at the end\nbirds_plot &lt;- ggplot(birds_redlining_pct_no_none, \n                     aes(x = grade, y = pct_grade, \n                         fill = grade)) +\n  geom_col(col = \"black\") +\n  geom_text(aes(label = paste0(pct_grade, \"%\")), \n            position = position_stack(vjust = 1.05), \n            size = 3) +\n  labs(title = \" \",\n       x = \"HOLC Grade\",\n       y = \"% of Observations\") +\n  scale_fill_manual(values = c(\"A\" = \"limegreen\", \n                               \"B\" = \"royalblue2\", \n                               \"C\" = \"gold\", \n                               \"D\" = \"firebrick3\")) +\n  theme_classic()\nbirds_plot &lt;- birds_plot + \n  theme(legend.position = \"none\")\n\n\n\n\nShow the code\n# combine the two figures with patchwork\nbirds_plot_none + birds_plot + \n  plot_annotation(title = \"Percentage of bird observations within each HOLC grade\", \n                  tag_levels = \"A\") # number the figures \"A\", \"B\", and \"C\"\n\n\n\n\n\n\n\n\nFigure 3: Percentage of bird observations from 2022 within each HOLC grade. (A) Including ‘None’ category with observations that were not within redlined districts, (B) Excluding ‘None’ category to only show observations within redlined districts. Other than non-redlined areas, districts with a grade of C had the highest % of bird observations. However, there doesn’t appear to be a clear relationship between HOLC grade and bird observations. Observation data was sourced from the Global Biodiversity Information Facility (GBIF), including data only from 2022.\n\n\n\n\n\n\n\nInterpretation of Results\nEllis-Soto et al. (2023) found that redlining has shaped our observations and distribution of biodiveristy as well as the socio-economic and environmental conditions of communities. They showed that “historically redlined neigborhoods remain the most under sampled urban areas for bird biodiversity today.” While there isn’t a clear difference in the % of bird observations between HOLC grades, Figure 1(A) makes it clear that redlined neighborhoods are still under sampled for bird biodiversity. This aligns with the findings of Ellis-Soto et al. (2023) and suggests that the legacy of redlining has had lasting impacts on the distribution of biodiversity observations in Los Angeles County."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#acknowledgements",
    "href": "posts/2024-11-05-redlining-legacies/index.html#acknowledgements",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized Ruth Oliver, an Assistant Professor at the Bren School and the instructor for EDS 223. EDS 223 (Geospatial Analysis & Remote Sensing) is offered in the Master of Environmental Data Science (MEDS) program at the Bren School."
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#references",
    "href": "posts/2024-11-05-redlining-legacies/index.html#references",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "References",
    "text": "References\nCity of Los Angeles GeoHub. (2020). County Boundary. https://geohub.lacity.org/datasets/county-boundary\nEllis-Soto, D., Chapman, M., & Locke, D. H. (2023). Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States. Nature Human Behaviour, 1-9\nGBIF. (2022). Global Biodiversity Information Facility. https://www.gbif.org/\nNelson, K. R., Winling, L., Marciano, R., Connolly, N., “Mapping Inequality,” American Panorama, ed. Robert K. Nelson and Edward L. Ayers, accessed October 17, 2023, https://dsl.richmond.edu/panorama/redlining/\nUnited States Environmental Protection Agency. 2024 version. EJScreen. https://www.epa.gov/ejscreen"
  },
  {
    "objectID": "posts/2024-11-05-redlining-legacies/index.html#github-repository",
    "href": "posts/2024-11-05-redlining-legacies/index.html#github-repository",
    "title": "Exploring patterns of environmental justice in Los Angeles County",
    "section": "GitHub repository",
    "text": "GitHub repository\nLink to the GitHub repository for this analysis: redlining-legacies-LA"
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html",
    "href": "posts/2024-09-22-maxent-coral/index.html",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "",
    "text": "This analysis, completed by Zoe Zhou and I, is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on modeling the current and future distributions of the crown-of-thorns starfish (Acanthaster planci) and grape coral (Euphyllia cristata) in the eastern Indo-Pacific region using Maxent. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to visualize changes in the occurrence probability and geographical distribution of these species under current and future climate scenarios to delineate regions where active restoration plans could be implemented."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#overview",
    "href": "posts/2024-09-22-maxent-coral/index.html#overview",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "",
    "text": "This analysis, completed by Zoe Zhou and I, is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on modeling the current and future distributions of the crown-of-thorns starfish (Acanthaster planci) and grape coral (Euphyllia cristata) in the eastern Indo-Pacific region using Maxent. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to visualize changes in the occurrence probability and geographical distribution of these species under current and future climate scenarios to delineate regions where active restoration plans could be implemented."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#background",
    "href": "posts/2024-09-22-maxent-coral/index.html#background",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "Background",
    "text": "Background\nEcological restoration is the process of enhancing the recovery of natural ecosystems that have been degraded, damaged, or destroyed (SER, 2004). Among other reasons, restoration efforts are commonly employed to mitigate and repair damage caused by introduced or invasive species. One example of an invasive species is the crown-of-thorn starfish (Acanthaster planci) which is known to eat and decimate coral populations such as the grape coral (Euphyllia cristata) (Brodie et al., 2005). Before initiating restoration efforts, it’s important to conduct data-driven analyses to advance restoration planning and employ the most effective management actions."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#approach",
    "href": "posts/2024-09-22-maxent-coral/index.html#approach",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "Approach",
    "text": "Approach\nA maximum entropy (Maxent) model was used to predict changes in the occurrence distribution for the crown-of-thorns starfish (Acanthaster planci) and grape coral (Euphyllia cristata) based on key marine data layers. Our analysis aimed to accomplish two primary goals: (1) Visualize changes in the current and future occurrence probability and geographical distribution of the crown-of-thorns starfish (COTS) and grape coral in the eastern Indo-Pacific region; (2) Utilize information from the species distribution models to propose an active restoration plan in areas where the COTS and grape coral are likely to occur. Maxent was applied using the R package Wallace (v2.1.1), an R-based GUI application that offers a simple, reproducible approach for ecological modeling and niche/distribution modeling analyses (Kass et al., 2022).\nOccurrence data for each species was acquired from the Global Biodiversity Information Facility (GBIF). Marine environmental data were sourced from Bio-ORACLE, which provides essential oceanographic layers such as ocean temperature (mean, max, min), salinity, nitrate and phosphate concentrations, and pH, all at a resolution of 0.05 degrees (Tyberghein et al., 2012; Assis et al., 2024). These variables were chosen based on their relevance to marine species distribution (Brodie et al., 2005; Yasuda, 2018; Wang & Tabeta, 2023). To model future distribution, we used data from a moderate emission scenario (SSP3-7.0) for the decade 2020-2030. All marine environmental data were downloaded in NetCDF format and converted to GeoTIFFs using the terra package in R to ensure compatibility with Maxent.\nFor model evaluation, we used spatial partitioning with a 75:25 (training:testing) split. The Maxent model leveraged presence and background points to account for potential biases that may arise from using presence-only points (Merow, 2013). A linear-quadratic (LQ) feature class was used to balance model complexity and account for non-linear species-environment relationships. After developing the distribution models, we analyzed the results in ArcGIS Pro v3.0 to identify key areas for restoration."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#results",
    "href": "posts/2024-09-22-maxent-coral/index.html#results",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "Results",
    "text": "Results\nUnder current conditions, the Maxent models indicated a high probability of occurrence for both the crown-of-thorns starfish (CoTS) and grape coral along the coastal areas of Northern Australia. Grape coral also showed a concentrated distribution around the Natuna Islands in the South China Sea, where CoTS is not projected to occur (Figure 1). Future projections suggest a shift in both species’ distributions to the northeastern coast of Australia, with a contraction across the broader Indo-Pacific. Notably, the contraction appears more pronounced for CoTS than grape coral. Despite these changes, the northeastern coast remains a favorable habitat for both species, making it a critical focus for future conservation and management strategies. The model’s performance was strong, with an AUC of 0.9187 for the training data and 0.9024 for the validation data.\n\n\n\nFigure 1. Current (A) and future (B) projected occurrence distribution for the crown-of-thorn starfish in the eastern Indo-Pacific region. Higher probabilities of occurrence are shown in darker shades of green, signifying areas of higher habitat suitability. A general shift to lower occurrence probabilities in the future is evident throughout the study area."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#conclusion",
    "href": "posts/2024-09-22-maxent-coral/index.html#conclusion",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "Conclusion",
    "text": "Conclusion\nFor restoration efforts, the predicted overlap between CoTS and grape coral distributions offers essential insights. Despite CoTS being native to the Great Barrier Reef, their periodic population outbreaks—exacerbated by human activities—continue to pose a significant threat to coral ecosystems. Active management strategies, such as the ongoing Crown-of-Thorns Starfish Control Program on the Great Barrier Reef, play a pivotal role in mitigating CoTS outbreaks. However, the study has several caveats, such as the reliance on presence-only data and environmental layers that may not capture local habitat variability, as well as uncertainty in future projections based on a single emission scenario. Moreover, the model does not account for species interactions or local adaptations, which could affect the predicted distributions of both species."
  },
  {
    "objectID": "posts/2024-09-22-maxent-coral/index.html#acknowledgements",
    "href": "posts/2024-09-22-maxent-coral/index.html#acknowledgements",
    "title": "Mapping the distribution of native and invasive marine species in the Indo-Pacific region",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Ashley Larsen, an Associate Professor at the Bren School and the instructor for ESM 270 (Conservation Planning). ESM 270 is offered in the Master of Environmental Science & Management (MESM) program at the Bren School."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html",
    "href": "posts/2024-09-17-connectivity/index.html",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "",
    "text": "This analysis is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on modeling movement patterns and connectivity for the jaguar (Panthera onca) across southern Costa Rica with Circuitscape. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to identify important movement corridors and barriers between core areas in the Talamanca-Osa region of southern Costa Rica to support jaguar connectivity and continued movement across the landscape."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#overview",
    "href": "posts/2024-09-17-connectivity/index.html#overview",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "",
    "text": "This analysis is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on modeling movement patterns and connectivity for the jaguar (Panthera onca) across southern Costa Rica with Circuitscape. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to identify important movement corridors and barriers between core areas in the Talamanca-Osa region of southern Costa Rica to support jaguar connectivity and continued movement across the landscape."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#background-problem",
    "href": "posts/2024-09-17-connectivity/index.html#background-problem",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "Background & Problem",
    "text": "Background & Problem\nThroughout the last 100 years, southern Costa Rica has undergone significant agricultural expansion and urban development. This has had adverse impacts on many species occurring throughout the region, especially jaguars (Panthera onca) which are forest-dependent with wide home ranges and require large areas of undisturbed habitat; therefore, making them particularly vulnerable to fragmentation. Increasing fragmentation in this region has resulted in two disconnected jaguar populations. To reconnect them and support their continued persistence throughout the southern Costa Rican landscape, land managers and conservation practitioners need to understand the species’ movement patterns and barriers hindering movement connectivity."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#approach",
    "href": "posts/2024-09-17-connectivity/index.html#approach",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "Approach",
    "text": "Approach\nCircuitscape and Linkage Mapper were used to model connectivity across the study area in ArcGIS Pro v3.0. Circuitscape is a tool that treats landscapes as conductive surfaces, utilizing connections between random walk theory and electrical circuit theory to predict aspects of movement and connectivity between important patches of a landscape. This analysis employed three Linkage Mapper modules to analyze jaguar movement and connectivity between identified core areas in the Talamanca-Osa region of southern Costa Rica: Linkage Mapper, Pinchpoint Mapper, and Barrier Mapper. The core areas are 3 national parks in the region–Corcovado, Piedras Blancas, and La Amistad which was clipped to the study area.\nThe first step of the analysis was to map the least-cost corridors between the three core areas with Linkage Mapper using a cost-weighted distance (CWD) threshold of 20,000 m. This served to create a distribution of CWDs of the best route passing through each cell across the study area, with lower values being closer to the LCP and higher values being more costly/resistive. Next, Circuitscape was run using Pinchpoint Mapper to inject ‘electrical current’ into the core areas and facilitate its flow across the landscape between cores. This step was conducted twice, once with a CWD cutoff distance of 1,000,000 m to run Circuitscape across the entire resistance surface, and once with a CWD cutoff distance of 20,000 m to constrain current flow to the least-cost corridor. As implied in the tool’s name, flowing current concentrates in ‘pinch-points’, representing areas where jaguar movement is likely to be funneled through a narrow area. Lastly, the Barrier Mapper tool was used to identify important barriers running along and adjacent to the LCP. A barrier detection radius of 1000 m was used, detecting barriers up to 2 km across."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#results",
    "href": "posts/2024-09-17-connectivity/index.html#results",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "Results",
    "text": "Results\nResults of the Circuitscape and Linkage mapper analysis are shown in Figure 1 below. Figure 1(A) portrays the Circuitscape and Pinchpoint Mapper analysis with a CWD cutoff distance of 20,000 m to constrain current flow to the least-cost corridor defined in the first step of the analysis. Areas in red represent higher values and regions where the corridor is more constricted (and jaguar movement is concentrated), with green regions as lower values and less constricted areas. Pinchpoint Mapper was designed to run Circuitscape within identified least-cost corridors to map pinch-points within these areas; however, running this tool across an entire landscape can also provide valuable information regarding alternative movement routes and stepping stones. Figure 1(B) shows the result of the Barrier Mapper analysis. Again, red areas represent higher values that indicate barriers with the most significant effect on CWDs between core areas.\n\n\n\nFigure 1. Least-cost paths (LCP) and corridors connecting core areas for jaguars in the Talamanca-Osa region of southern Costa Rica. The top map shows the three core areas (Corcovado National Park in the southwest, Piedras Blancas National Park in the center, and La Amistad National Park in the northeast) with LCPs (white lines) connecting each core area. Figure 1(A) portrays the results of the pinch-point mapper analysis with a CWD cutoff distance of 20,000 m, with red areas representing higher values and places where the corridor surrounding the LCP is more constricted. Figure 1(B) shows the results of the barrier mapper analysis with a barrier detection radius of 1000 m. Again, red areas have higher values representing the strongest CWD effect between core areas, and more likely barriers to movement."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#conclusion",
    "href": "posts/2024-09-17-connectivity/index.html#conclusion",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis provides valuable insight into jaguar movement between 3 core areas in the Talamanca-Osa region of southern Costa Rica. The Circuitscape analysis identified pinch-points within the least-cost corridors connecting core areas (Figure 1(A)). These regions are extremely important for connectivity and ensuring continued movement by jaguars across the landscape. They represent locations where jaguar movement is likely to be funneled through more narrow areas and vulnerable regions where a loss or degradation of habitat might significantly alter or sever important movement linkages. Barriers surrounding LCPs with the strongest effect on CWDs between core areas also represent more vulnerable areas within the landscape (Figure 1(B)), and conservation planning efforts should be focused in these locations to improve connectivity in high resistance areas within identified corridors.\nOne important consideration that isn’t captured in this analysis is the impact that increased jaguar connectivity or habitat might have on surrounding rural subsistence farmers in the region. Throughout the modeling process and selection of regions to focus connectivity enhancement on, land managers and conservation practitioners should engage local community members and ensure a diverse range of voices are included to incorporate important socioeconomic dynamics of conservation planning."
  },
  {
    "objectID": "posts/2024-09-17-connectivity/index.html#acknowledgements",
    "href": "posts/2024-09-17-connectivity/index.html#acknowledgements",
    "title": "Exploring habitat connectivity between core areas in a Costa Rican landscape",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Ashley Larsen, an Associate Professor at the Bren School and the instructor for ESM 270 (Conservation Planning). ESM 270 is offered in the Master of Environmental Science & Management (MESM) program at the Bren School."
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html",
    "href": "posts/2024-02-15-nonlinear-squares/index.html",
    "title": "Maximizing crop yields with data analysis",
    "section": "",
    "text": "Grain growing at a farm in Greece. Image credit: Enterprise Greece"
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#overview",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#overview",
    "title": "Maximizing crop yields with data analysis",
    "section": "Overview",
    "text": "Overview\n\nLoad Packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(nlraa)\nlibrary(janitor)\nlibrary(tibble)\nlibrary(kableExtra)\nlibrary(knitr)\nlibrary(Metrics)\nlibrary(nls2)\nlibrary(broom)\n\n\nData Description:\nThe data used in this analysis comes from the paper “Nonlinear Regression Models and Applications in Agricultural Research,” written by Sotirios Archontoulis and Fernando Miguez. It’s stored in an object called sm in the nlraa package, and contains the following 5 variables:\n\n‘doy’ - integer representing the day of the year, spanning 141-303\n‘block’ - integer that represents 1-4 blocks used in the experiment\n‘input’ - integer that represents low (1) or high (2) agronomic input\n‘crop’ - factor with three levels; fiber sorghum (F), sweet sorghum (S), and maize (M)\n‘yield’ - numeric variable representing biomass yield in Mg/ha\n\n\n\nCode\nvariable &lt;- c(\"doy\", \"block\", \"input\", \"crop\", \"yield\")\ndata_type &lt;- c(\"integer\", \"integer\", \"integer\", \"factor\", \"numeric\")\ndescription &lt;- c(\"day of the year, spanning 141-303\", \n                 \"1-4 blocks used in the experiment\", \n                 \"low (1) or high (2) agronomic input\", \n                 \"three levels: fiber sorghum (F), sweet sorghum (S), and maize (M)\", \n                 \"biomass yield in Mg/ha\")\n\nsm_variables &lt;- tibble(Variable = variable, \n                           Data.Type = data_type, \n                           Description = description)\n\nkable(sm_variables, col.names = c(\"Variable\", \"Data Type\", \n                                  \"Description\")) %&gt;% \n  kable_classic_2() \n\n\n\n\nTable 1: Summary of the data used in the report, stored in the sm object from the nlraa package.\n\n\n\n\n\n\nVariable\nData Type\nDescription\n\n\n\n\ndoy\ninteger\nday of the year, spanning 141-303\n\n\nblock\ninteger\n1-4 blocks used in the experiment\n\n\ninput\ninteger\nlow (1) or high (2) agronomic input\n\n\ncrop\nfactor\nthree levels: fiber sorghum (F), sweet sorghum (S), and maize (M)\n\n\nyield\nnumeric\nbiomass yield in Mg/ha\n\n\n\n\n\n\n\n\n\n\nObjective:\nTo maximize yield, it’s crucial for farmers to understand the biology of plants and their responses to fertilizers. This analysis aims to help farmers make predictions on their yields by running non-linear least squares on experimental growth data for three grains in Greece.\nData Citation:\nArchontoulis, S.V. and Miguez, F.E. (2015), Nonlinear Regression Models and Applications in Agricultural Research. Agronomy Journal, 107: 786-798. https://doi.org/10.2134/agronj2012.0506\nPsuedo-code:\n\nLoad the data stored as sm from the nlraa package.\nChoose candidate models; in this case, we’re using the Beta function from Table 1, Equation 2.5 of Archontoulis, S.V. and Miguez, F.E. (2015).\nWrite a function to model the Beta function.\nCreate a plot to find some potential starting parameter values, and set initial guesses.\nRun an NLS model to predict yield on any given day for sorghum fields with high inputs.\nRun NLS models for all 24 combinations of plot, input level, and crop type using purrr\nFind the ‘best’ model among competing models for each species.\nPlot the results of the anaysis, recreating Figure 7 of the paper."
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#setup",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#setup",
    "title": "Maximizing crop yields with data analysis",
    "section": "Setup",
    "text": "Setup\n\nLoad the Data\n\n\nCode\nsm_raw &lt;- nlraa::sm\n\nsm_df &lt;- sm_raw %&gt;% \n  clean_names()"
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#beta-function-from-archontoulis-miguez-2015",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#beta-function-from-archontoulis-miguez-2015",
    "title": "Maximizing crop yields with data analysis",
    "section": "Beta Function (from Archontoulis & Miguez, 2015)",
    "text": "Beta Function (from Archontoulis & Miguez, 2015)\n\\[\nY=Y_{max}(1 + \\frac{t_e-t}{t_e-t_m})(\\frac{t}{t_e})^\\frac{t_e}{t_e-t_m}\n\\]\n\nDescription of Beta Function Parameters\n\nY is the response variable (biomass)\nt is the explanatory variable (doy)\nYmax is the maximum Y value\ntm is the inflection point at which the growth rate is maximized\nte is the time when Y = Yasym\n\n\n\nWrite a Function to Model the Beta Function\n\n\nCode\nbeta &lt;- function(ymax,doy,tm,te){\n  out=ymax*(1 + ((te-doy)/(te-tm)))*((doy/te)^(te/(te-tm)))\nreturn(out)\n}\n\n\n\n\nChoosing Starting Values\n\nMake a Plot Similar to Figure 4\n\n\nCode\nguess_plot &lt;- ggplot(sm_df, aes(x = doy, y = yield, \n                                shape = crop, color = crop)) +\n  geom_point() + \n  facet_wrap(~input) +\n  theme_bw()\n\n#guess_plot\n\n\n\n\nDefine Guesses / Potential Starting Values\n\n\nCode\nymax_guess = max(sm_df$yield)\n\ntm_guess = 225\n\nte_guess = 260"
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#sorghum-fields-nls",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#sorghum-fields-nls",
    "title": "Maximizing crop yields with data analysis",
    "section": "Sorghum Fields NLS",
    "text": "Sorghum Fields NLS\n\n\nCode\nsorghum_df &lt;- sm_df %&gt;% \n  filter(input == 2 & crop == \"S\")\n\n\n\nRun NLS\n\n\nCode\nsorghum_nls=nls(formula=yield~beta(ymax,doy,tm,te),\n                  data=sorghum_df,\n                  start=list(ymax=ymax_guess, tm=tm_guess, \n                             te=te_guess),\n                  trace=TRUE)\n\n# sorghum_nls\n\n\n\n\nMake a Table\n\n\nCode\ntidy(sorghum_nls) %&gt;% \n  kable(col.names = c(\"Parameter\", \"Selected Value\", \n                      \"Standard Error\", \"Statistic\", \"p-value\")) %&gt;% \n  kable_classic_2()\n\n\n\n\nTable 2: Selected parameter values, standard errors, statistics, and p-values of the estimated parameters for the sorghum fields, high input NLS model. The p-values aren’t actually 0, but low enough that the function used to create the table considered them 0.\n\n\n\n\n\n\nParameter\nSelected Value\nStandard Error\nStatistic\np-value\n\n\n\n\nymax\n39.82035\n2.248860\n17.70690\n0\n\n\ntm\n244.77725\n3.511964\n69.69811\n0\n\n\nte\n281.58631\n2.080764\n135.32834\n0\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate the Model\n\n\nCode\n# find the sorghum_nls model predictions\nsorghum_predict &lt;- sorghum_df %&gt;% \n  mutate(predict = predict(sorghum_nls, newdata = sorghum_df))\n\n# plot the sorghum_nls model on top of the sweet sorghum data\nggplot(data = sorghum_predict) +\n  geom_point(aes(x = doy, y = yield)) +\n  geom_path(aes(x = doy, y = predict), color = 'red') +\n  theme_bw() +\n  labs(x = \"Day of the Year\", \n       y = \"Biomass (Mg/ha)\") +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 1: Fitted NLS model for high input, sweet sorghum fields. The fitted model (red line) is graphed on top of the raw sweet sorghum data."
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#run-nls-models-for-all-combinations",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#run-nls-models-for-all-combinations",
    "title": "Maximizing crop yields with data analysis",
    "section": "Run NLS Models for All Combinations",
    "text": "Run NLS Models for All Combinations\n\nCreate a New Function\n\n\nCode\n#Define a new function to pass along all nls calls\n\nall_nls_fcn&lt;-function(sm_df){ \n  \n  ymax_guess=max(sm_df$yield)\n  tm_guess=225\n  te_guess=260\n  \n  nls(yield~beta(ymax,doy,tm,te),\n    data=sm_df,\n    start=list(ymax=ymax_guess, tm=tm_guess, \n                             te=te_guess)) \n}\n\n\n\n\nUse purrr to Run All NLS\n\n\nCode\n# run nls models for all 24 combinations of plot, input level, crop type\n# calculate RMSE values \nbeta_all &lt;- sm_df %&gt;% \n  group_by(input, crop, block) %&gt;% \n  nest() %&gt;% \n  mutate(nls_model = map(data, ~all_nls_fcn(.x))) %&gt;% \n  mutate(predictions = map2(nls_model, data, \n                            ~predict(.x, newdata = .y))) %&gt;% \n  mutate(RMSE = map2_dbl(predictions, data, \n                         ~Metrics::rmse(.x, .y$yield))) %&gt;%\n  mutate(smooth = map(nls_model, \n                      ~predict(.x, newdata = list(doy = seq(147, 306)))))\n\n#'smooth' breakdown:\n### runs the model between every single 'doy' point \n### creates a smoothed line of the best fitting NLS model \n\n\n\n\nLowest RMSE for Each Species\n\n\nCode\n### Fiber Sorghum (F) ###\nbeta_fiber &lt;- beta_all %&gt;% \n  filter(crop == \"F\") \nbest_fiber = min(beta_fiber$RMSE)\n\n#pull out the model to make tables\nbest_fiber1 &lt;- beta_all$nls_model[22]\n\n\n### Sweet Sorghum (S) ###\nbeta_sweet &lt;- beta_all %&gt;% \n  filter(crop == \"S\") \nbest_sweet = min(beta_sweet$RMSE)\n\n#pull out the model to make tables\nbest_sweet1 &lt;- beta_all$nls_model[14]\n\n\n### Maize (M) ###\nbeta_maize &lt;- beta_all %&gt;% \n  filter(crop == \"M\") \nbest_maize = min(beta_maize$RMSE)\n\n#pull out the model to make tables\nbest_maize1 &lt;- beta_all$nls_model[4]\n\n\n\n\nMake Tables\n\n\nCode\n# add RMSE values into the table caption\n\n### Fiber ###\ntidy(best_fiber1[[1]]) %&gt;% \n  kable(col.names = c(\"Variable\", \"Parameter Values\", \n                      \"Standard Error\", \"Statistic\", \n                      \"p-value\")) %&gt;% \n  kable_classic_2()\n### Sweet ###\ntidy(best_sweet1[[1]]) %&gt;% \n  kable(col.names = c(\"Variable\", \"Parameter Values\", \n                      \"Standard Error\", \"Statistic\", \n                      \"p-value\")) %&gt;% \n  kable_classic_2()\n### Maize ###\ntidy(best_maize1[[1]]) %&gt;% \n  kable(col.names = c(\"Variable\", \"Parameter Values\", \n                      \"Standard Error\", \"Statistic\", \n                      \"p-value\")) %&gt;% \n  kable_classic_2()\n\n\n\n\nTable 3: Best fitted NLS models for each species: (a) fiber sorghum; (b) sweet sorghum; (c) maize. The RMSE scores for each model are reported in the subcaptions.\n\n\n\n\n\n\n\n(a) Fiber sorghum (model RMSE: 1.82)\n\n\n\n\n\nVariable\nParameter Values\nStandard Error\nStatistic\np-value\n\n\n\n\nymax\n29.05639\n1.657636\n17.52881\n5e-07\n\n\ntm\n244.99880\n3.538539\n69.23727\n0e+00\n\n\nte\n280.42964\n1.838951\n152.49433\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Sweet sorghum (model RMSE: 2.22)\n\n\n\n\n\nVariable\nParameter Values\nStandard Error\nStatistic\np-value\n\n\n\n\nymax\n31.34741\n2.047998\n15.30637\n1.2e-06\n\n\ntm\n245.77425\n3.815015\n64.42288\n0.0e+00\n\n\nte\n278.34654\n1.817123\n153.17984\n0.0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Maize (model RMSE: 0.81)\n\n\n\n\n\nVariable\nParameter Values\nStandard Error\nStatistic\np-value\n\n\n\n\nymax\n18.93109\n0.8396967\n22.54516\n5e-07\n\n\ntm\n225.22836\n1.9262857\n116.92365\n0e+00\n\n\nte\n252.57495\n1.8691829\n135.12586\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecreate Figure 7\n\nData Prep\n\n\nCode\n# filtered the data and unnest the smoothed values for plotting\n# add a doy column\nunnest_df &lt;- beta_all %&gt;% \n  filter(block == 1) %&gt;% \n  tidyr::unnest(smooth) %&gt;% \n  mutate(doy = seq(147, 306)) %&gt;% \n  filter(!(doy &gt; 263 & crop == \"M\")) \n\n# filter the original data for plotting on top of model results\nsm_filter &lt;- sm_df %&gt;% \n  filter(block == 1) %&gt;% \n  select(doy, yield, crop)\n\n# # join the unnest_df to the sm_filter to get yield data for fun\n# unnest_sm_join &lt;- left_join(unnest_df, sm_filter, \n#                             by = join_by(doy))\n\n# # unnest the model predictions\n# predict_df &lt;- beta_all %&gt;% \n#   tidyr::unnest(predictions)\n\n\n\n\nMake a Plot\n\n\nCode\n# change the facet labels from \"1\" and \"2\" to \"low\" and \"high\"\nunnest_df$input &lt;- factor(unnest_df$input, levels = c(1, 2), \n                          labels = c(\"Low\", \"High\"))\n\n# make the final plot\nfinal_plot &lt;- ggplot() +\n  geom_path(data = unnest_df, aes(x = doy, y = smooth, color = crop)) +\n  geom_point(data = sm_filter, aes(x = doy, y = yield, shape = crop), \n             size = 1.5, color = \"darkslategrey\") +\n  facet_wrap(~input) +\n  theme_bw() +\n  labs(x = \"Day of the Year\", \n       y = \"Biomass (Mg/ha)\", \n       color = \"Model Fit\", \n       shape = \"sm Data\") +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\nFigure 2: Best fitted NLS models for each crop species in block 1 from the experiment (this is a recreation of Figure 7 from Archontoulis, S.V. & Miguez, F.E. (2015)). The sm data for each species in block 1 is shown. It’s evident that higher fertilizer inputs resulted in increased yields (Mg/ha) for all three crop species."
  },
  {
    "objectID": "posts/2024-02-15-nonlinear-squares/index.html#acknowledgements",
    "href": "posts/2024-02-15-nonlinear-squares/index.html#acknowledgements",
    "title": "Maximizing crop yields with data analysis",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Nathan Grimes at the Bren School for ESM 244 (Advanced Data Analysis for Environmental Science & Management). ESM 244 is offered in the Master of Environmental Science & Management (MESM) program."
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html",
    "href": "posts/2024-02-14-blr-plant-classification/index.html",
    "title": "Plant classification using logistic regression",
    "section": "",
    "text": "Cluster of saw palmetto (Serenoa Repens) plants. Image credit: iNaturalist"
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#overview",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#overview",
    "title": "Plant classification using logistic regression",
    "section": "Overview",
    "text": "Overview\nData Description:\nData used in this analysis comes from a study conducted by Warren Abrahamson which looked at survival, growth, and biomass estimated of two dominant palmetto species of South Florida: Serenoa repens and Sabal etonia. The study includes three data sets, and the one used in this analysis, palmetto_data, contains survival and growth data from 1981 through 1997, and then again in 2001 and 2017; data collection is ongoing at 5-year intervals.\nObjective:\nThis analysis aims to test the feasibility of using variables plant height, canopy length, canopy width, and number of green leaves to classify whether a palmetto species is Serenoa repens or Sabal etonia. It will examine two different models with combinations of these predictor variables, determine which model is the ‘best’, and then examine the success of that model in classifying plant species correctly.\nData Citation:\nAbrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\nPseudo-code:\n\nLoad the palmetto data, clean it up, and wrangle it to prep for exploratory data visualizations.\nCreate basic visualizations that explore differences in canopy height, length, width, and number of green leaves between the two species.\nSet up the two models we’re comparing based on several predictor variables, and perform a binary logistic regression (BLR) for each.\nSplit the data into training (80%) and testing (20%).\nSet up each BLR model, and fit them to the training data.\nCompare how well each model predicts species correctly based on their respective predictor variables.\nCreate ROC curves and calculate the AUC to further explore both models’ performance.\nUse a ten-fold cross validation to examine which model performs better at classification.\nTrain the ‘better’ model using the entire dataset.\nEvaluate how successfully this chosen model would classify a plant as the correct species, using a 50% cutoff.\nCreate a finalized table showing, for each species, how many plants in the original dataset would be correctly/incorrectly classified."
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#setup",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#setup",
    "title": "Plant classification using logistic regression",
    "section": "Setup",
    "text": "Setup\n\nLoad Packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(Metrics)\nlibrary(cowplot)\nlibrary(here)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(kableExtra)\n\n\n\n\nLoad Data & Select Variables\n\n\nCode\npalmetto_df &lt;- read_csv(here(\"posts/2024-02-14-blr-plant-classification/data/palmetto.csv\"))\n\npalmetto_mod &lt;- palmetto_df %&gt;% \n  select(year, plant, species, survival, height, \n         length, width, green_lvs) %&gt;% \n  mutate(spec_fac = as_factor(species))"
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#data-visualizations-for-canopy-height-length-width-green-leaves",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#data-visualizations-for-canopy-height-length-width-green-leaves",
    "title": "Plant classification using logistic regression",
    "section": "Data Visualizations for Canopy Height, Length, Width & Green Leaves",
    "text": "Data Visualizations for Canopy Height, Length, Width & Green Leaves\n\nData Wrangling\n\n\nCode\n#create a df to visualize canopy height for each species\npalmetto_height &lt;- palmetto_mod %&gt;% \n  group_by(spec_fac) %&gt;% \n  summarize(height_avg = mean(height, na.rm = TRUE))\n\npalmetto_height$height_avg &lt;- round(palmetto_height$height_avg, digits = 1)\n\n\n#create a df to visualize canopy length \npalmetto_length &lt;- palmetto_mod %&gt;% \n  group_by(spec_fac) %&gt;% \n  summarize(length_avg = mean(length, na.rm = TRUE))\n\npalmetto_length$length_avg &lt;- round(palmetto_length$length_avg, digits = 1)\n\n\n#create a df to visualize canopy width\npalmetto_width &lt;- palmetto_mod %&gt;% \n  group_by(spec_fac) %&gt;% \n  summarize(width_avg = mean(width, na.rm = TRUE))\n\npalmetto_width$width_avg &lt;- round(palmetto_width$width_avg, digits = 1)\n\n\n#Create a df to visualize # of green leaves \npalmetto_leaves &lt;- palmetto_mod %&gt;% \n  group_by(spec_fac, year) %&gt;% \n  summarize(leaves_avg = mean(green_lvs, na.rm = TRUE))\n\n\n\n\nMake Plots\n\nHeight Plot\n\n\nCode\nheight_plot &lt;- ggplot(palmetto_height, \n                      aes(x = spec_fac, y = height_avg, fill = spec_fac)) +\n  geom_col() +\n  geom_text(aes(label = height_avg), vjust = -0.5, size = 3.5) +\n  theme_bw() +\n  labs(x = \" \", \n       y = \"Average measurement (cm)\", \n       title = \"Height\") +\n  scale_fill_manual(values = c(\"1\" = \"chartreuse3\", \n                               \"2\" = \"darkolivegreen\")) + \n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = c(\"Serenoa repens\", \"Sabal etonia\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nheight_plot &lt;- height_plot + \n  coord_cartesian(ylim = c(0, 125)) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n#height_plot\n\n\n\n\nLength Plot\n\n\nCode\nlength_plot &lt;- ggplot(palmetto_length, \n                      aes(x = spec_fac, y = length_avg, fill = spec_fac)) +\n  geom_col() + \n  geom_text(aes(label = length_avg), vjust = -0.5, size = 3.5) +\n  theme_bw() +\n  labs(x = \" \", \n       y = \" \", \n       title = \"Length\") +\n  scale_fill_manual(values = c(\"1\" = \"chartreuse3\", \n                               \"2\" = \"darkolivegreen\")) + \n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = c(\"Serenoa repens\", \"Sabal etonia\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nlength_plot &lt;- length_plot + \n  coord_cartesian(ylim = c(0, 175)) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n#length_plot\n\n\n\n\nWidth Plot\n\n\nCode\nwidth_plot &lt;- ggplot(palmetto_width, \n                     aes(x = spec_fac, y = width_avg, fill = spec_fac)) +\n  geom_col() + \n  geom_text(aes(label = width_avg), vjust = -0.5, size = 3.5) +\n  theme_bw() +\n  labs(x = \" \", \n       y = \" \", \n       title = \"Width\") +\n  scale_fill_manual(values = c(\"1\" = \"chartreuse3\", \n                               \"2\" = \"darkolivegreen\")) + \n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = c(\"Serenoa repens\", \"Sabal etonia\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nwidth_plot &lt;- width_plot +\n  coord_cartesian(ylim = c(0, 125)) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n#width_plot\n\n\n\n\nCombined Plots\n\n\nCode\nh_w_l_plot &lt;- height_plot + length_plot + width_plot\n\nh_w_l_plot\n\n\n\n\n\n\n\n\nFigure 1: Comparison of average canopy height, length, and width in Serenoa repens and Sabal etonia during the study period. Average values are shown on top of each bar, and they represent the mean values for each species across the entire study period.\n\n\n\n\n\n\n\nGreen Leaves Plot\n\n\nCode\nggplot(palmetto_leaves, \n       aes(x = spec_fac, y = leaves_avg, fill = spec_fac)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_bw() +\n  scale_x_discrete(labels = c(\"Serenoa repens\", \"Sabal etonia\")) +\n  labs(y = \"Average count of leaves\", \n       x = \" \") +\n  theme(legend.position = \"none\") + \n  scale_fill_manual(values = c(\"1\" = \"chartreuse3\", \n                               \"2\" = \"darkolivegreen\")) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 2: Average count of leaves in Serenoa repens and Sabal etonia. The mean number of leaves for both species in each year over the study period was calculated, and the distribution of the mean value of leaves for each year is shown.\n\n\n\n\n\nKey Takeaways:\n\nCanopy height doesn’t appear to differ significantly between the two species.\nBased on Figure 1, canopy length is likely to be the best predictor, and canopy width may also be a fairly good predictor.\nBased on Figure 2, it appears that the average count of leaves could likely help classify species correctly as well."
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#binary-logistic-regression",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#binary-logistic-regression",
    "title": "Plant classification using logistic regression",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\n\nSet up the Two Models\n\n\nCode\n#define the two models\nm1 &lt;- spec_fac ~ height + length + width + green_lvs\nm2 &lt;- spec_fac ~ height + width + green_lvs\n\n\n#create each of the BLR's with the two models\nblr1 &lt;- glm(formula = m1, data = palmetto_mod, family = binomial)\nsummary(blr1)\n\nblr2 &lt;- glm(formula = m2, data = palmetto_mod, family = binomial)\nsummary(blr2)\n\n\n# AIC & BIC scores for each model\nAIC(blr1) #5195\nAIC(blr2) #5987\n\nBIC(blr1) #5231\nBIC(blr2) #6017\n\n\n\n\nSplit the Data\n\n\nCode\n#check balance of the spec_fac column\npalmetto_mod %&gt;%\n  group_by(spec_fac) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(prop = n / sum(n))\n#species counts are pretty much split 50/50\n#no need to choose a stratified split\n\nspec_split &lt;- initial_split(palmetto_mod, prop = 0.80)\n\nspec_train_df &lt;- training(spec_split)\nspec_test_df &lt;- testing(spec_split)\n\n\n\n\nSet up the Basic BLR Models\n\n\nCode\n### Set up our model\nblr_mdl &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\n\n### Model 1 ###\nblr1_fit &lt;- blr_mdl %&gt;%\n  fit(formula = m1, data = spec_train_df)\n\n\n### Model 2 ###\nblr2_fit &lt;- blr_mdl %&gt;%\n  fit(formula = m2, data = spec_train_df)\n\n\n#Examine the model coefficients\nblr1_fit\nblr2_fit\n\n\n\n\nCompare How Well Each Model Predicts Species\n\n\nCode\n## Model 1 ##\nspec_test_predict1 &lt;- spec_test_df %&gt;%\n  mutate(predict(blr1_fit, new_data = spec_test_df)) %&gt;%\n  mutate(predict(blr1_fit, new_data = ., type = 'prob'))\n\n\n## Model 2 ##\nspec_test_predict2 &lt;- spec_test_df %&gt;%\n  mutate(predict(blr2_fit, new_data = spec_test_df)) %&gt;%\n  mutate(predict(blr2_fit, new_data = ., type = 'prob'))\n\n\n# Examine relationship b/w `.pred_class`, `.pred_1`, and `.pred_2`.\ntable(spec_test_predict1 %&gt;%\n        select(spec_fac, .pred_class))\n\ntable(spec_test_predict2 %&gt;%\n        select(spec_fac, .pred_class))\n\n\n# Examine the accuracy of each model\naccuracy(spec_test_predict1, truth = spec_fac, estimate = .pred_class)\naccuracy(spec_test_predict2, truth = spec_fac, estimate = .pred_class)\n\n\n\n\nCreate ROC Curves and Calculate AUC\n\n\nCode\n## Model 1 ##\nblr1_roc_df &lt;- roc_curve(spec_test_predict1, \n                         truth = spec_fac, .pred_1)\n#autoplot(blr1_roc_df)\n\n\n## Model 2 ##\nblr2_roc_df &lt;- roc_curve(spec_test_predict2, \n                         truth = spec_fac, .pred_1)\n#autoplot(blr2_roc_df)\n\n\n# Calculate area under curve\n# 50% is random guessing, 100% is perfect classifier\nyardstick::roc_auc(spec_test_predict1, truth = spec_fac, .pred_1)\nyardstick::roc_auc(spec_test_predict2, truth = spec_fac, .pred_1)\n\n\n\n\nTen-Fold Cross Validation\n\n\nCode\n# Define the number of folds, and number of repeats\nspec_train_folds &lt;- vfold_cv(spec_train_df, v = 10, repeats = 10)\nspec_train_folds\n\n##### Model 1 #####\n\n# Create a workflow that combines our model & a formula \nblr1_wf &lt;- workflow() %&gt;% \n  add_model(blr_mdl) %&gt;%\n  add_formula(spec_fac ~ height + length + width + green_lvs)\n\n# Apply the workflow to the train dataset & see how it performs\nblr1_fit_folds &lt;- blr1_wf %&gt;%\n  fit_resamples(spec_train_folds)\n\nblr1_fit_folds\n\n### Average the predictive performance of the ten models:\ncollect_metrics(blr1_fit_folds)\n\n\n##### Model 2 #####\n\n# Create a workflow that combines our model & a formula \nblr2_wf &lt;- workflow() %&gt;%\n  add_model(blr_mdl) %&gt;%\n  add_formula(spec_fac ~ height + width + green_lvs)\n\n# Apply the workflow to the train dataset & see how it performs\nblr2_fit_folds &lt;- blr2_wf %&gt;%\n  fit_resamples(spec_train_folds)\n\nblr2_fit_folds\n\n### Average the predictive performance of the ten models:\ncollect_metrics(blr2_fit_folds)\n\n\n\n\nSummary: ‘Best’ Model\n\nBased on the results of the BLR and cross validation, model 1 (species ~ height + length + width + green_lvs) performs better at classifying Serenoa repens or Sabal etonia correctly.\nModel 1 had a lower AIC score of 5195 compared to model 2’s AIC score of 5987. Model 1 also had a lower BIC score of 5231 compared to model 2’s BIC score of 6017.\nAfter testing each model’s predictive capabilities on the test df, model 1 predicted species classification accurately ~ 92% of the time, and model 2 did ~ 90% of the time.\nBoth models had high area under ROC curve scores, although model 1 had a slightly higher AUC at ~ 0.97 compared to model 2’s AUC score of ~ 0.96."
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#train-model-1-using-entire-dataset",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#train-model-1-using-entire-dataset",
    "title": "Plant classification using logistic regression",
    "section": "Train Model 1 Using Entire Dataset",
    "text": "Train Model 1 Using Entire Dataset\n\n\nCode\n# Fit the first model to the entire data set\nlast_mdl_fit &lt;- blr_mdl %&gt;%\n  fit(formula = m1, data = palmetto_mod)\n\n\n\nFinalized Table for Model 1\n\n\nCode\ntidy(last_mdl_fit) %&gt;% \n  kableExtra::kable(col.names = c(\"Variable\", \"Coefficient Estimate\", \n                                  \"Standard Error\", \"Statistic\", \n                                  \"p-value\")) %&gt;% \n  kableExtra::kable_classic_2()\n# tidy(blr1_fit) %&gt;% \n#   kableExtra::kable(col.names = c(\"Variable\", \"Coefficient Estimate\", \n#                                   \"Standard Error\", \"Statistic\", \n#                                   \"p-value\"), \n#                     caption = \"Table 1. Fill in later\") %&gt;% \n#   kableExtra::kable_classic()\n\n\n\n\nTable 1: Binary logistic regression results for model 1. The p-values aren’t truly 0; all were so far below the significance threshold (0.05) that the broom::tidy() function regarded them as 0.\n\n\n\n\n\n\nVariable\nCoefficient Estimate\nStandard Error\nStatistic\np-value\n\n\n\n\n(Intercept)\n3.2266851\n0.1420708\n22.71180\n0\n\n\nheight\n-0.0292173\n0.0023061\n-12.66984\n0\n\n\nlength\n0.0458233\n0.0018661\n24.55600\n0\n\n\nwidth\n0.0394434\n0.0021000\n18.78227\n0\n\n\ngreen_lvs\n-1.9084747\n0.0388634\n-49.10728\n0"
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#evaluation-of-model-1s-classification-success",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#evaluation-of-model-1s-classification-success",
    "title": "Plant classification using logistic regression",
    "section": "Evaluation of Model 1’s Classification Success",
    "text": "Evaluation of Model 1’s Classification Success\n\n\nCode\n### Last Fit ###\n# Apply our workflow to the train dataset & see how it performs\nlast_mdl_eval &lt;- blr1_wf %&gt;%\n  last_fit(spec_split)\n\n### Average the predictive performance of the ten models:\ncollect_metrics(last_mdl_eval)\n\n\n### Look at how well the trained model 1 predicts species ###\nspec_pred_final &lt;- palmetto_mod %&gt;%\n  mutate(predict(last_mdl_fit, new_data = palmetto_mod)) %&gt;%\n  mutate(predict(last_mdl_fit, new_data = ., type = 'prob'))\n\ntable(spec_pred_final %&gt;%\n        select(spec_fac, .pred_class))\n\n# Examine the accuracy\naccuracy(spec_pred_final, truth = spec_fac, estimate = .pred_class)\n\n\n\nMake a Table of Model 1’s Success\n\n\nCode\nSpecies &lt;- c(\"Serenoa repens\", \"Sabal etonia\")\ncorrectly_classified &lt;- c(5548, 5701)\nincorrectly_classified &lt;- c(564, 454)\npercent_correct &lt;- c(90.77, 92.62)\npercent_incorrect &lt;- c(9.23, 7.38)\n\nmdl_1_sucess &lt;- tibble(Species = Species, \n                       Correct = correctly_classified, \n                       Incorrect = incorrectly_classified, \n                       Percent.Correct = percent_correct, \n                       Perfect.Incorrect = percent_incorrect)\n\nkable(mdl_1_sucess, col.names = c(\"Species\", \"Correct\", \"Incorrect\", \n                    \"% Correct\", \"% Incorrect\")) %&gt;% \n  kable_classic_2() %&gt;% \n  kable_styling(full_width = FALSE, \n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n                font_size = 14)\n\n\n\n\nTable 2: Number of each plant species correctly and incorrectly classified by the model.\n\n\n\n\n\n\nSpecies\nCorrect\nIncorrect\n% Correct\n% Incorrect\n\n\n\n\nSerenoa repens\n5548\n564\n90.77\n9.23\n\n\nSabal etonia\n5701\n454\n92.62\n7.38\n\n\n\n\n\n\n\n\n\n\nConclusion:\nAfter generating and comparing two models to classify whether a palmetto species is Serenoa repens or Sabal etonia, we found that model 1 (species ~ height + length + width + green_lvs) performed best. We then trained this model on the entire dataset and evaluated its classification success, finding that it correctly identified Serenoa repens about 90.77% of the time, and Sabal etonia about 92.62% of the time (see Table 2 above)."
  },
  {
    "objectID": "posts/2024-02-14-blr-plant-classification/index.html#acknowledgements",
    "href": "posts/2024-02-14-blr-plant-classification/index.html#acknowledgements",
    "title": "Plant classification using logistic regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Casey O’Hara at the Bren School for ESM 244 (Advanced Data Analysis for Environmental Science & Management). ESM 244 is offered in the Master of Environmental Science & Management (MESM) program."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maxwell Pepperdine",
    "section": "",
    "text": "My name is Maxwell, and I’m a recent graduate with a Master of Environmental Science and Management at the Bren School of Environmental Science & Management, where I specialized in Conservation Planning with an emphasis in GIS and geospatial analysis. I built this website using Quarto to share a little bit about myself and showcase my academic and professional projects, particularly those rooted in data-driven approaches to environmental problem-solving. Feel free to explore my work and reach out if you would like to connect!"
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Maxwell Pepperdine",
    "section": "",
    "text": "My name is Maxwell, and I’m a recent graduate with a Master of Environmental Science and Management at the Bren School of Environmental Science & Management, where I specialized in Conservation Planning with an emphasis in GIS and geospatial analysis. I built this website using Quarto to share a little bit about myself and showcase my academic and professional projects, particularly those rooted in data-driven approaches to environmental problem-solving. Feel free to explore my work and reach out if you would like to connect!"
  },
  {
    "objectID": "index.html#professional-interests",
    "href": "index.html#professional-interests",
    "title": "Maxwell Pepperdine",
    "section": "Professional Interests",
    "text": "Professional Interests\nConservation Planning | GIS | Geospatial Analysis | Data Analysis | Water Resources Management | Nature-based Solutions | Remote Sensing | Climate Adaptation"
  },
  {
    "objectID": "index.html#recent-ongoing-projects",
    "href": "index.html#recent-ongoing-projects",
    "title": "Maxwell Pepperdine",
    "section": "Recent & Ongoing Projects",
    "text": "Recent & Ongoing Projects\nIntegrating climate adaptation strategies into forest management\nMapping suitable nesting habitat for marbled murrelets in the SCM"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Maxwell Pepperdine",
    "section": "",
    "text": "Hi there! I’m Maxwell Pepperdine (no affiliation to the university) and I’m a recent graduate with a Master of Environmental Science and Management (MESM) from the Bren School at UC Santa Barbara. Specializing in Conservation Planning with an emphasis in GIS and geospatial analysis, I’m committed to using data-driven, interdisciplinary approaches to address today’s pressing environmental challenges.\nMy work is grounded in a passion for protecting biodiversity, advancing nature-based solutions, and supporting human well-being through effective conservation planning and resource management. I bring strong technical skills in data analysis and management, GIS, and geospatial analysis, along with strengths in science communication and cross-disciplinary collaboration. I’m eager to apply these tools to contribute to projects supporting climate adaptation, resilient ecosystems, and spatial planning efforts that bridge science, policy, and practice to create meaningful environmental impact.\nI graduated summa cum laude from California Polytechnic State University (Cal Poly), San Luis Obispo in 2023 with a B.S. in Environmental Management and Protection, along with minors in Biology and Sustainable Environments. Growing up in Santa Cruz, CA, I was fortunate to be surrounded by incredible ocean and redwood ecosystems. This fostered my appreciation for nature at an early age, and instilled in me a lasting admiration for the natural world and a drive to protect it.\nOutside of work, you can find me hiking, backpacking, playing disc golf, practicing ocean photography (some of my favorite photos are shared below) or spending time with friends and family.\n\nExperienceEducationSkillsWork SamplesHonors & AwardsOcean PhotographySome of My Favorite Places\n\n\nGIS Volunteer - Felidae Conservation Fund, Remote (8/25 – Present)\nArnhold Environmental Graduate Fellow – Environmental Markets Lab (emLab), Santa Barbara, CA (6/24 – Present)\nGraduate Teaching Assistant – University of California, Santa Barbara (1/24 – 6/25)\nRemote Sensing Research Assistant – Edge Hill University, Remote (6/24 – 9/24)\nSpatial Conservation Planning Intern – California Central Coast Joint Venture (C3JV), Remote (11/23 – 4/24)\nFrost Summer Undergraduate Research Program - California Polytechnic State University (Cal Poly), San Luis Obispo, Remote (5/22 – 9/22)\n\n\nMaster of Environmental Science and Management | Bren School of Environmental Science & Management, UCSB (June 2025)\nB.S. in Environmental Management and Protection | California Polytechnic State University (Cal Poly), San Luis Obispo (June 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science and Management: R/RStudio, Python, Quarto, GitHub, SQL, DuckDB, Google Drive\nGIS and Geospatial Analysis: R/RStudio, ArcGIS Pro, QGIS, ArcPy, Field Maps, Python, Google Earth Engine\nOther Technical: Microsoft Office Suite (Word, Excel, PowerPoint), Quarto, NVivo, OpenLCA\nWriting: Technical writing of scientific research manuscripts, policy memos, and CEQA and NEPA reports/assessments\n\n\nCartography\nWriting\n\n\n2023 Environmental Engineering and Science Foundation (EESF) Scholarship Recipient\nPresidents Honor List (2020, 2021, 2022, 2023) | Cal Poly, San Luis Obispo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\n\n\nColorado Springs, Colorado\n\n\n\n\n\n\n\nWhite Sands National Park, New Mexico\n\n\n\n\n\n\n\n\n\nBentonite Hills, Utah\n\n\n\n\n\n\n\nWhite Sands National Park, New Mexico\n\n\n\n\n\n\n\n\n\nBentonite Hills, Utah\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\n\n\nGreat Sand Dunes National Park, Colorado\n\n\n\n\n\n\n\nBryce Canyon National Park, Utah\n\n\n\n\n\n\n\n\n\nShiprock, New Mexico\n\n\n\n\n\n\n\nArches National Park, Utah\n\n\n\n\n\n\n\n\n\nGoblin Valley State Park, Utah\n\n\n\n\n\n\n\nPlatforms Beach, Aptos, CA\n\n\n\n\n\n\nall photos were taken by my partner, Indra Lyons, or myself"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Maxwell Pepperdine",
    "section": "",
    "text": "Hi there! I’m Maxwell Pepperdine (no affiliation to the university) and I’m a recent graduate with a Master of Environmental Science and Management (MESM) from the Bren School at UC Santa Barbara. Specializing in Conservation Planning with an emphasis in GIS and geospatial analysis, I’m committed to using data-driven, interdisciplinary approaches to address today’s pressing environmental challenges.\nMy work is grounded in a passion for protecting biodiversity, advancing nature-based solutions, and supporting human well-being through effective conservation planning and resource management. I bring strong technical skills in data analysis and management, GIS, and geospatial analysis, along with strengths in science communication and cross-disciplinary collaboration. I’m eager to apply these tools to contribute to projects supporting climate adaptation, resilient ecosystems, and spatial planning efforts that bridge science, policy, and practice to create meaningful environmental impact.\nI graduated summa cum laude from California Polytechnic State University (Cal Poly), San Luis Obispo in 2023 with a B.S. in Environmental Management and Protection, along with minors in Biology and Sustainable Environments. Growing up in Santa Cruz, CA, I was fortunate to be surrounded by incredible ocean and redwood ecosystems. This fostered my appreciation for nature at an early age, and instilled in me a lasting admiration for the natural world and a drive to protect it.\nOutside of work, you can find me hiking, backpacking, playing disc golf, practicing ocean photography (some of my favorite photos are shared below) or spending time with friends and family.\n\nExperienceEducationSkillsWork SamplesHonors & AwardsOcean PhotographySome of My Favorite Places\n\n\nGIS Volunteer - Felidae Conservation Fund, Remote (8/25 – Present)\nArnhold Environmental Graduate Fellow – Environmental Markets Lab (emLab), Santa Barbara, CA (6/24 – Present)\nGraduate Teaching Assistant – University of California, Santa Barbara (1/24 – 6/25)\nRemote Sensing Research Assistant – Edge Hill University, Remote (6/24 – 9/24)\nSpatial Conservation Planning Intern – California Central Coast Joint Venture (C3JV), Remote (11/23 – 4/24)\nFrost Summer Undergraduate Research Program - California Polytechnic State University (Cal Poly), San Luis Obispo, Remote (5/22 – 9/22)\n\n\nMaster of Environmental Science and Management | Bren School of Environmental Science & Management, UCSB (June 2025)\nB.S. in Environmental Management and Protection | California Polytechnic State University (Cal Poly), San Luis Obispo (June 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science and Management: R/RStudio, Python, Quarto, GitHub, SQL, DuckDB, Google Drive\nGIS and Geospatial Analysis: R/RStudio, ArcGIS Pro, QGIS, ArcPy, Field Maps, Python, Google Earth Engine\nOther Technical: Microsoft Office Suite (Word, Excel, PowerPoint), Quarto, NVivo, OpenLCA\nWriting: Technical writing of scientific research manuscripts, policy memos, and CEQA and NEPA reports/assessments\n\n\nCartography\nWriting\n\n\n2023 Environmental Engineering and Science Foundation (EESF) Scholarship Recipient\nPresidents Honor List (2020, 2021, 2022, 2023) | Cal Poly, San Luis Obispo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\nYosemite National Park, California\n\n\n\n\n\n\n\n\n\nColorado Springs, Colorado\n\n\n\n\n\n\n\nWhite Sands National Park, New Mexico\n\n\n\n\n\n\n\n\n\nBentonite Hills, Utah\n\n\n\n\n\n\n\nWhite Sands National Park, New Mexico\n\n\n\n\n\n\n\n\n\nBentonite Hills, Utah\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\nBisti/De-Na-Zin Wilderness, New Mexico\n\n\n\n\n\n\n\n\n\nGreat Sand Dunes National Park, Colorado\n\n\n\n\n\n\n\nBryce Canyon National Park, Utah\n\n\n\n\n\n\n\n\n\nShiprock, New Mexico\n\n\n\n\n\n\n\nArches National Park, Utah\n\n\n\n\n\n\n\n\n\nGoblin Valley State Park, Utah\n\n\n\n\n\n\n\nPlatforms Beach, Aptos, CA\n\n\n\n\n\n\nall photos were taken by my partner, Indra Lyons, or myself"
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html",
    "href": "posts/2024-01-20-salmon-time-series/index.html",
    "title": "Time series analysis",
    "section": "",
    "text": "Willamette Falls Fish Ladder on the Willamette River (Oregon). Image credit: PGE.\n\n\nDataset & report summary: Data used in this analysis was accessed from Columbia River Data Access in Real Time (DART). Columbia River DART provides interactive data resources to support research and management practices relating to the Columbia River Basin salmon populations and overall river ecosystem. This dataset includes information on adult fish passage recorded from 2001-01-01 to 2010-12-31 at the Willamette Falls fish ladder on the Willamette River (Oregon). The following report includes a time series analysis for the data and various figures in three different parts: part 1 (original time series analysis), part 2 (season plots exploring patterns in seasonality), and part 3 (annual counts of fish passage by each species).\nData citation: Columbia Basin Research. (n.d.). Columbia River DART. Retrieved January 25, 2023, from https://www.cbr.washington.edu/dart/query/adult_graph_text\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tsibble) #for time series analysis\nlibrary(feasts) #feature extraction and statistics for time series\nlibrary(fable)\nlibrary(patchwork)\nlibrary(janitor)\n\n\n\n\nCode\nfish_df &lt;- read_csv(here(\"posts/2024-01-20-salmon-time-series/data/willamette_fish_passage.csv\")) %&gt;% \n  clean_names()"
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html#overview",
    "href": "posts/2024-01-20-salmon-time-series/index.html#overview",
    "title": "Time series analysis",
    "section": "",
    "text": "Willamette Falls Fish Ladder on the Willamette River (Oregon). Image credit: PGE.\n\n\nDataset & report summary: Data used in this analysis was accessed from Columbia River Data Access in Real Time (DART). Columbia River DART provides interactive data resources to support research and management practices relating to the Columbia River Basin salmon populations and overall river ecosystem. This dataset includes information on adult fish passage recorded from 2001-01-01 to 2010-12-31 at the Willamette Falls fish ladder on the Willamette River (Oregon). The following report includes a time series analysis for the data and various figures in three different parts: part 1 (original time series analysis), part 2 (season plots exploring patterns in seasonality), and part 3 (annual counts of fish passage by each species).\nData citation: Columbia Basin Research. (n.d.). Columbia River DART. Retrieved January 25, 2023, from https://www.cbr.washington.edu/dart/query/adult_graph_text\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tsibble) #for time series analysis\nlibrary(feasts) #feature extraction and statistics for time series\nlibrary(fable)\nlibrary(patchwork)\nlibrary(janitor)\n\n\n\n\nCode\nfish_df &lt;- read_csv(here(\"posts/2024-01-20-salmon-time-series/data/willamette_fish_passage.csv\")) %&gt;% \n  clean_names()"
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html#part-1-original-time-series",
    "href": "posts/2024-01-20-salmon-time-series/index.html#part-1-original-time-series",
    "title": "Time series analysis",
    "section": "Part 1: Original Time Series",
    "text": "Part 1: Original Time Series\n\n\nCode\n#Select the appropriate columns and pivot the data\n#Convert the data frame into time series format \nfish_ts &lt;- fish_df %&gt;% \n  select(project, date, coho, jack_coho, steelhead) %&gt;% \n  pivot_longer(cols = 3:5, \n               names_to = \"species\", values_to = \"count\") %&gt;% \n  mutate(date = lubridate::mdy(date)) %&gt;% \n  as_tsibble(key = species,\n             index = date)\n\nfish_ts_0 &lt;- replace(fish_ts, is.na(fish_ts), 0) #replace NAs with 0\n\n\n\n\nCode\npassage_plot &lt;- ggplot(fish_ts_0, \n                       aes(x = date, y = count)) +\n  geom_line() +\n  theme_bw() +\n  facet_wrap(~species, labeller = labeller(species = c(\n    \"coho\" = \"Coho\", \n    \"jack_coho\" = \"Jack coho\", \n    \"steelhead\" = \"Steelhead\"))) +\n  labs(x = \"Year\", y = \"Fish passage counts\") \n\n  # scale_color_manual(values = c(\"coho\" = \"lightblue\",\n  #                               \"jack_coho\" = \"skyblue2\",\n  #                               \"steelhead\" = \"skyblue4\"))\npassage_plot\n\n# passage_plot2 &lt;- ggplot(fish_ts_0, \n#                        aes(x = date, y = count, fill = species)) +\n#   geom_col() +\n#   theme_bw() +\n#   scale_fill_brewer(palette = \"Dark2\")\n# passage_plot2\n\n\n\n\n\n\n\n\nFigure 1: Static plot of adult fish passage for coho, jack coho, and steelhead salmon at the Willamette Falls fish ladder on the Willamette River (Oregon) between 2001-2010.\n\n\n\n\n\nMajor Trends:\n\nCoho salmon have an increasing trend of fish passage counts, jack coho have a fairly consistent trend, and steelhead have a slightly decreasing trend.\nAll three fish species show consistent patterns of seasonality. Coho and jack coho fish passage counts increase significantly in what appears to be later months of each year. This is expected as, according to NOAA, Pacific coho salmon usually return to spawning grounds between September and December. It’s interesting to note that some years have significantly smaller counts during spawning season. The seasonality of increased steelhead counts are more consistent than the coho species, and spread out over a larger spawning season which, according to NOAA, is between the months of April-June."
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html#part-2-season-plots",
    "href": "posts/2024-01-20-salmon-time-series/index.html#part-2-season-plots",
    "title": "Time series analysis",
    "section": "Part 2: Season Plots",
    "text": "Part 2: Season Plots\n\n\nCode\nfish_ts_0 %&gt;% \n  gg_season(y = count, pal = hcl.colors(n = 10)) +\n  facet_wrap(~species, scales = \"free_y\", nrow = 3, \n             labeller = labeller(species = c(\"coho\" = \"Coho\", \n                                             \"jack_coho\" = \"Jack coho\", \n                                             \"steelhead\" = \"Steelhead\"))) +\n  labs(x = \"Month\", \n       y = \"Fish passage counts\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 2: Season plots for coho, jack coho, and steelhead salmon species at the Willamette Falls fish ladder on the Willamette River (Oregon) between 2001-2010. Months are shown on the x-axis and number of fish passage counts on the y-axis, and each year in which data was collected are shown by different colors.\n\n\n\n\n\nMajor Trends:\n\nThe seasonality patterns identified in Figure 1 are much more evident in the season plots above. Fish passage counts for both coho species increase significantly during their spawning season (September-December), which is a much more concentrated time than the steelhead. We also see that coho fish passage counts (shown in the top panel) increased a lot from 2007-2010.\nSeasonality for steelhead fish passage counts is much more spread out than the two coho species, reflective of their longer spawning season (April-June). There is no noticeable trend in changes in fish passage counts between each year data was collected."
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html#part-3-annual-counts-by-species",
    "href": "posts/2024-01-20-salmon-time-series/index.html#part-3-annual-counts-by-species",
    "title": "Time series analysis",
    "section": "Part 3: Annual Counts by Species",
    "text": "Part 3: Annual Counts by Species\n\n\nCode\nfish_ts_annual &lt;- fish_df %&gt;% \n  select(project, date, coho, jack_coho, steelhead) %&gt;% \n  pivot_longer(cols = 3:5, \n               names_to = \"species\", values_to = \"count\")\n\nfish_ts_annual_0 &lt;- replace(fish_ts_annual, is.na(fish_ts_annual), 0)\n\nfish_ts_annual_0 &lt;- fish_ts_annual_0 %&gt;% \n  mutate(date = lubridate::mdy(date)) %&gt;% \n  mutate(year = lubridate::year(date)) %&gt;% \n  mutate(Year = as.factor(year)) %&gt;% \n  select(Year, species, count)\n\nfish_count &lt;- aggregate(count ~ Year+species, data = fish_ts_annual_0, FUN = sum)\n  \n  #group_by(Year, species) %&gt;% \n  #summarize(annual_count = n(), .groups = \"drop\")\n\n\n\n\nCode\nannual_count &lt;- ggplot(fish_count, aes(x = Year, y = count, fill = species)) +\n  geom_col(color = \"black\", size = 0.1) +\n  labs(x = \"Year\", \n       y = \"Fish passage counts\") +\n  theme_bw() +\n  facet_wrap(~species, scales = \"free_y\", nrow = 3, \n             labeller = labeller(species = c(\"coho\" = \"Coho\", \n                                             \"jack_coho\" = \"Jack coho\", \n                                             \"steelhead\" = \"Steelhead\"))) +\n  scale_fill_brewer() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(legend.position = \"none\") #get rid of the legend\n  # geom_text(aes(label = count), vjust = -0.5, size = 2) #show the column values\n\nannual_count\n\n\n\n\n\n\n\n\nFigure 3: Annual totals for fish passage for coho (gray), jack coho (light blue), and steelhead (dark blue) salmon. Note the different y-axis values for each species to recognize the larger number of counts for steelhead and coho compared to jack coho.\n\n\n\n\n\nMajor Trends:\n\nI don’t see any clear trends in the annual totals by species from 2001-2010. Fish passage counts for coho salmon appear to be increasing overall throughout the decade, with a significant increase in 2009 and 2010. Jack coho counts follow a similar patter to the coho with recent increases, but with far less total fish passage counts overall.\nSteelhead salmon have the most overall and consistent counts year to year. There seems to be a slight decrease in their annual fish passage counts starting in 2005, but rebounded in 2010."
  },
  {
    "objectID": "posts/2024-01-20-salmon-time-series/index.html#acknowledgements",
    "href": "posts/2024-01-20-salmon-time-series/index.html#acknowledgements",
    "title": "Time series analysis",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Casey O’Hara at the Bren School for ESM 244 (Advanced Data Analysis for Environmental Science & Management). ESM 244 is offered in the Master of Environmental Science & Management (MESM) program."
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "",
    "text": "SBC LTER Logo. Image credit: SBC LTER"
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html#overview",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html#overview",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "Overview",
    "text": "Overview\nData Description:\nThe data used in this analysis is a data package from Santa Barbara Coastal (SBC) Long Term Ecological Research (LTER). It contains stream water chemistry measurements taken in various watersheds throughout the Santa Barbara area, with data collection beginning in 2000. As described by the data source, this “dataset is ongoing, and data will be added approximately annually. Stream water samples are collected weekly during non-storm flows in winter, and bi-weekly during summer. During winter storms, samples are collected hourly (rising limb) or at 2-4 hour intervals (falling limb). Analytes sampled in the SBC LTER watersheds include dissolved nitrogen (nitrate, ammonium, total dissolved nitrogen); soluble reactive phosphorus (SRP); particulate organic carbon, nitrogen and phosphorus; total suspended sediments; and conductivity.”\nData Citation:\nSanta Barbara Coastal LTER and J. Melack. 2019. SBC LTER: Land: Stream chemistry in the Santa Barbara Coastal drainage area, ongoing since 2000 ver 16. Environmental Data Initiative. https://doi.org/10.6073/pasta/67a558a24ceed9a0a5bf5e46ab841174\nObjective:\nThis analysis aims to use complete linkage agglomerative hierarchical clustering to compare water chemistry by specific sites in the SBC LTER dataset. This “bottom-up” approach to clustering will allow us to group sites whose water quality are more similar to each other and different from other sites in the dataset.\nPseudo-code:\n\nLoad the data.\nExamine which columns have a lot of NA’s and drop them from the analysis. Use the summary() function to identify all columns that have greater than 50% NA values and remove them.\nMake a data frame that has a single summary row per site based on the average of all observations from that site. Use na.rm = TRUE when summarizing to remove any rows with NA’s.\nScale the average measurements at each site.\nPerform the complete linkage agglomerative hierarchical clustering:\n\nCalculate the Euclidean distance.\nUse the stats::hclust() function to perform the complete linkage agglomerative hierarchical clustering.\n\nMake a clean dendrogram to show the results of the multivariate clustering."
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html#setup",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html#setup",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "Setup",
    "text": "Setup\n\nLoad Packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(NbClust) #cluster package\nlibrary(cluster)\nlibrary(dendextend)\nlibrary(ggdendro)\nlibrary(factoextra)\n\n\n\n\nLoad Data\n\n\nCode\n# load the data and assign all values of -999.0 as NA \nstream_chem_raw &lt;- read_csv(here(\"posts/2024-02-14-hierarchical-cluster/data/sbc_lter_registered_stream_chemistry.csv\"), \n                            na = '-999')"
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html#hierarchical-cluster-analysis-by-complete-linkage",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html#hierarchical-cluster-analysis-by-complete-linkage",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "Hierarchical Cluster Analysis (by Complete Linkage)",
    "text": "Hierarchical Cluster Analysis (by Complete Linkage)\n\nData Wrangling\nProcess to Deal w/ NAs:\n\nIdentify columns with lots of NA values (&gt;50%) using the summary() function, and drop them from the analysis.\nUse na.rm = TRUE when summarizing water quality measurements at each site to remove all other NA values remaining in the data. I chose this method instead of using drop_na() for the entire data frame because this removed NAs for each respective measurement when calculating their mean values rather than the entire row of data wherever an NA value was present.\n\n\n\nCode\n# examine which columns have more than 50% NA values \nsummary(stream_chem_raw) ## 'tpc_uM' ; 'tpn_uM' ; 'tpp_uM ; 'tss_mgperLiter' \n\n\n# drop columns with &gt; 50% NA's from the df\nstream_chem_df &lt;- stream_chem_raw %&gt;% \n  select(site_code, nh4_uM, no3_uM, po4_uM, tdn_uM, tdp_uM, spec_cond_uSpercm)\n\n\n# make a data frame with a single summary row per site \n# take the mean of all observations at each site\n# drop NAs using `na.rm = TRUE` when summarizing\n### use this df when calculating the Euclidian distance ###\nstream_chem_means &lt;- stream_chem_df %&gt;% \n  group_by(site_code) %&gt;% \n  summarize(nh4_avg = mean(nh4_uM, na.rm = TRUE),\n            no3_avg = mean(no3_uM, na.rm = TRUE), \n            po4_avg = mean(po4_uM, na.rm = TRUE), \n            tdn_avg = mean(tdn_uM, na.rm = TRUE), \n            tdp_avg = mean(tdp_uM, na.rm = TRUE), \n            spec_avg = mean(spec_cond_uSpercm, na.rm = TRUE))\n\n\n# scale the average measurements at each site\nstream_mean_scale &lt;- stream_chem_means %&gt;% \n  select(-site_code) %&gt;% \n  scale()\n\n\n### If I want to drop all NA's instread of using na.rm in summarize() ###\n\n# # drop all rows with NA's\n# stream_complete &lt;- stream_chem_df %&gt;% \n#   drop_na()\n# \n# # scale the measurements at each site\n# stream_scale &lt;- stream_complete %&gt;% \n#   scale() # this function centers and/or scales the columns of a numeric matrix\n# \n# # compare the scaled and complete df's\n# summary(stream_complete)\n# summary(stream_scale) # the mean measurement at each site is 0 \n\n\n\n\nComplete Linkage\n\n\nCode\n### complete ###\n\n# calculate Euclidean distance; should this be using the mean stream df???\nstream_dist &lt;- dist(stream_mean_scale, method = \"euclidean\")\n\n# Hierarchical clustering (complete linkage) \nstream_hc_complete &lt;- hclust(stream_dist, method = \"complete\")\n\n# # Plot it (base plot):\n# plot(stream_hc_complete, cex = 0.6, hang = -1)\n\n\n\n\nMake Dendrograms with ggplot\n\n\nCode\n# prep vectors for labelling \nstream_sites &lt;- c(\"AB00\", \"AT07\", \"BC02\", \"DV01\", \"GV01\", \"HO00\", \"MC00\", \n                  \"MC06\", \"ON02\", \"RG01\", \"RS02\", \"SP02\", \"TO02\")\n\nint_placeholders &lt;- c(4, 3, 2, 1, 7, 12, 5, 8, 13, 10, 9, 6, 11)\n\n\n# make the dendrogram\nggdendrogram(stream_hc_complete) + \n  theme_classic() + \n  scale_x_continuous(labels = stream_sites, breaks = int_placeholders) +\n  labs(x = \"Measurement site\", \n       y = \"Distance between clusters\")\n\n\n\n\n\n\n\n\nFigure 1: Dendrogram showing the results of the complete linkage agglomerative hierarchical clustering to compare water chemistry by specific sites in the SBC LTER dataset. Each site is shown on the x-axis, and the distance between clusters on the y-axis."
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html#summary",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html#summary",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "Summary",
    "text": "Summary\n\nFor this analysis, we performed complete linkage agglomerative hierarchical clustering to compare water chemistry by specific sites in the SBC LTER dataset. In this method of clustering, all sites start as their own cluster, and are grouped into larger clusters according to similarities between them.\nThe height at which branches merge and objects (different sites) exist on the dendrogram indicates the relative dissimilarity between the sites (clusters) being merged. The taller the branch or height at which two clusters merge, the less similar the sites are.\nLooking at the dendrogram in Figure 1 above, we can see that the water chemistry at sites ‘DV01’ and ‘BC02’ are most different from the rest of the sites.\nThe smaller groupings of branches and branches closer to eachother might indicate sites in the same watershed or those in a closer spatial proximity with more similar water chemistry measurements."
  },
  {
    "objectID": "posts/2024-02-14-hierarchical-cluster/index.html#acknowledgements",
    "href": "posts/2024-02-14-hierarchical-cluster/index.html#acknowledgements",
    "title": "Comparing water chemistry in Santa Barbara streams with a cluster analysis",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Casey O’Hara at the Bren School for ESM 244 (Advanced Data Analysis for Environmental Science & Management). ESM 244 is offered in the Master of Environmental Science & Management (MESM) program."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html",
    "href": "posts/2024-09-20-morro-prioritizr/index.html",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "",
    "text": "This analysis is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on informing reserve design in the Morro Bay watershed prioritizing special status species. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to identify important planning units in the Morro Bay watershed to include in a conservation reserve under two different settings/problems: (1) considering species with any status in the watershed; (2) considering only endemic, endangered, or threatened species in the watershed."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#overview",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#overview",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "",
    "text": "This analysis is from an assignment in ESM 270 (Conservation Planning) at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. The assignment focused on informing reserve design in the Morro Bay watershed prioritizing special status species. It was required to be limited to two pages, with a figure included, hence the brevity of the write-up. The analysis aimed to identify important planning units in the Morro Bay watershed to include in a conservation reserve under two different settings/problems: (1) considering species with any status in the watershed; (2) considering only endemic, endangered, or threatened species in the watershed."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#background-problem",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#background-problem",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "Background & Problem",
    "text": "Background & Problem\nLandscapes and reserves are complex socio-ecological systems with various interactions between socioeconomic dynamics, political and governance systems, and natural processes across spatial and temporal scales (Meyfroidt et al., 2022). Because of this, successful planning measures at large scales and the selection of the most important planning units are difficult. To design effective reserve networks that address systematic conservation planning problems holistically, it’s crucial to consider both ecological and social factors. One situation where this kind of analysis is required is the identification of priority parcels in a region to include in a conservation reserve."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#approach",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#approach",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "Approach",
    "text": "Approach\nPrioritizr is an R package that guides systematic reserve design and solves various conservation planning problems. Similar to Marxan, but with enhanced flexibility, speed, and reproducibility, it can be used to build a conservation problem, generate a prioritization setting, solve and evaluate it. It’s flexible in design with many different functions that can build and customize conservation planning problems and solutions in a variety of settings. This analysis employs prioritizr to solve a conservation planning problem in the Morro Bay watershed with the goal of identifying priority parcels in the region to include in a conservation reserve under two different settings/problems: (1) considering species with any status in the watershed; (2) considering only endemic, endangered, or threatened species in the watershed.\nFor both settings, the problem was set up in prioritizr with the same exact inputs other than the filtering of irreplaceable species. All data was obtained from a previous MESM group project focused on the Morro Bay estuary. Planning units were parcel data containing information on the unit id, cost, and status. It’s important to note that some units’ statuses were ‘locked in’ or ‘locked out’, meaning they either had to be included or could not be included in the reserve. Conservation features were defined by the three different settings of species mentioned above and the number of species in each planning unit. For each setting above, two problems were created: one that found the optimal solution, and another that created a portfolio of solutions that were within the top 15% of the optimal solution. After creating the portfolio of solutions, they were summed to examine the selection frequency of each parcel across all 100 runs, essentially identifying which planning units were most important to address the planning problem."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#results",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#results",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "Results",
    "text": "Results\nThe resulting planning units determined to be included/not included in the best solution under the setting with all species and the setting with only endemic, endangered, and threatened species are shown in Figure 1(A) and Figure 1(C) below, respectively. Under each setting, prioritized planning units appear to be the same for the most part; however, there are a couple of parcels throughout the watershed that were prioritized in Figure 1(A), but not in Figure 1(C). Figure 1(B) and Figure 1(D) portray the results of the summed portfolio solutions within the top 15% of the optimal solution in each setting across 100 runs, representing the parcels most important to address the planning problem. Similarly to Figure 1(A) and Figure 1(C), most of these planning units appear to be the same with all species and the selected species.\n\n\n\nFigure 1. Reserve design in the Morro Bay watershed under various scenarios. (A) Parcels either included (green) or not included (white dots) in the optimal reserve design solution including all species in the study. (B) Sum solutions showing the selection frequency of each planning unit across all model runs for all species in the study. (C) Parcels either included (green) or not included (white dots) in the optimal reserve design solution when considering only endemic, endangered, or threatened species occurring in the region. (D) Sum solutions showing the selection frequency of each planning unit across all model runs when considering only endemic, endangered, or threatened species occurring in the region."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#conclusion",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#conclusion",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "Conclusion",
    "text": "Conclusion\nUnder each setting/problem explored in this analysis, many of the same planning units were selected to include in the optimal solution and reserve design in the Morro Bay watershed. For the few parcels that were included in the setting with all species but not in the setting with endemic, endangered, or threatened species, it’s likely that these species are not known to occur across these units. However, due to reserve dynamics, spillover effects, and adverse, interconnected impacts across landscapes, that doesn’t necessarily mean that these parcels are not important to the conservation of these imperiled species. In an analysis like this striving to solve a specific conservation problem, it’s crucial to consider other aspects of equitable, inclusive conservation planning that are difficult to capture in maximization models. After prioritizing planning units for biodiversity and socioeconomic factors (or before this kind of analysis is done), it’s imperative to engage local communities and a diverse range of stakeholders and voices in the reserve design process."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#acknowledgements",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#acknowledgements",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized by Ashley Larsen, an Associate Professor at the Bren School and the instructor for ESM 270 (Conservation Planning). ESM 270 is offered in the Master of Environmental Science & Management (MESM) program at the Bren School."
  },
  {
    "objectID": "posts/2024-09-20-morro-prioritizr/index.html#references",
    "href": "posts/2024-09-20-morro-prioritizr/index.html#references",
    "title": "Informing reserve design in the Morro Bay Watershed",
    "section": "References",
    "text": "References\nMeyfroidt, P., de Bremond, A., Ryan, C. M., Archer, E., Aspinall, R., Chhabra, A., Camara, G., Corbera, E., DeFries, R., Díaz, S., Dong, J., Ellis, E. C., Erb, K.-H., Fisher, J. A., Garrett, R. D., Golubiewski, N. E., Grau, H. R., Grove, J. M., Haberl, H., & Heinimann, A. (2022). Ten facts about land systems for sustainability. Proceedings of the National Academy of Sciences, 119(7), e2109217118. https://doi.org/10.1073/pnas.2109217118"
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html",
    "href": "posts/2024-11-10-houston-blackout/index.html",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "",
    "text": "The frequency and intensity of extreme weather events are increasing due to climate change, bringing devastating impacts. “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20” (Wikipedia, 2021).\nThis analysis will identify the impacts of these extreme winter storms by estimating the number of homes that lost power throughout the Houston metropolitan area, and examine whether or not these impacts were distributed equitably across census tracts based on their median income levels. Remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite, serve as the basis for this analysis. Specifically, we used the VNP46A1 to detect differences in nighttime lights before and after the winter storms, allowing the identification of areas that lost electric power.\nTo determine the number of homes that lost power, these areas identified with the VIIRS data were linked to OpenStreetMap (OSM) data on buildings and road. These analyses were then linked with data from the US Census Bureau to investigate the correlation between socioeconomic factors and recovery."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#backgroundoverview",
    "href": "posts/2024-11-10-houston-blackout/index.html#backgroundoverview",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "",
    "text": "The frequency and intensity of extreme weather events are increasing due to climate change, bringing devastating impacts. “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20” (Wikipedia, 2021).\nThis analysis will identify the impacts of these extreme winter storms by estimating the number of homes that lost power throughout the Houston metropolitan area, and examine whether or not these impacts were distributed equitably across census tracts based on their median income levels. Remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite, serve as the basis for this analysis. Specifically, we used the VNP46A1 to detect differences in nighttime lights before and after the winter storms, allowing the identification of areas that lost electric power.\nTo determine the number of homes that lost power, these areas identified with the VIIRS data were linked to OpenStreetMap (OSM) data on buildings and road. These analyses were then linked with data from the US Census Bureau to investigate the correlation between socioeconomic factors and recovery."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#data-description",
    "href": "posts/2024-11-10-houston-blackout/index.html#data-description",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Data description",
    "text": "Data description\n\nNight lights\nAs mentioned above, this analysis uses night light data acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS). VIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection, and tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06, so we need to download two files per date to cover the entire Houston area, as described below.\nData files:\n\nVNP46A1.A2021038.h08v05.001.2021039064328.tif: tile h08v05, collected on 2021-02-07\nVNP46A1.A2021038.h08v06.001.2021039064329.tif: tile h08v06, collected on 2021-02-07\nVNP46A1.A2021047.h08v05.001.2021048091106.tif: tile h08v05, collected on 2021-02-16\nVNP46A1.A2021047.h08v06.001.2021048091105.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoads\nBecause highways typically account for a large portion of night lights observable from space, we need to exclude them from the analysis to avoid falsely identifying areas with reduced traffic as areas without power. OpenStreetMap (OSM) is a collaborative project that creates publicy avialable geographic data; however, because it covers global extents, organizing this data into a database where it can be subsetted and processed is a large undertaking. Thankfully, third party companies such as Geofabrik’s redistribute OSM. We used this site to retrieve a shapefile of all highways in Texas and then subset this data to only include roads intersecting the Houston metropolitan area.\nData file\n\ngis_osm_roads_free_1.gpkg: OpenStreetMap data for roads in Texas\n\n\n\nHouses\nOSM also provides building data, which again was downloaded from Geofabrik and subset to only include houses in the Houston metropolitan area.\nData file\n\ngis_osm_buildings_a_free_1.gpkg: OpenStreetMap data for buildings in Texas\n\n\n\nSocioeconomic\nWe obtained data from the U.S. Census Bureau’s American Community Survey (ACS) for 2019. This data is distributed in a file geodatabase format, which contains both the geometry of census tracts and layers that contain a subset of the fields documents in the ACS metadata.\nData file\n\nACS_2019_5YR_TRACT_48_TEXAS.gdb: ACS data for Texas"
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#load-packages",
    "href": "posts/2024-11-10-houston-blackout/index.html#load-packages",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Load packages",
    "text": "Load packages\n\n\nShow packages used in this analysis\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#part-1-find-locations-that-experienced-a-blackout-by-creating-a-mask",
    "href": "posts/2024-11-10-houston-blackout/index.html#part-1-find-locations-that-experienced-a-blackout-by-creating-a-mask",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Part 1: Find locations that experienced a blackout by creating a mask",
    "text": "Part 1: Find locations that experienced a blackout by creating a mask\nPsuedo code:\n\nLoad VIIRS night light data for two days to explore the data around the day of the storm. 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nCreate a raster object for each day with st_mosaic(). Houston lies on the border of two tiles (h08v05 and h08v06) produced by the NASA data products; therefore, we need to download these two files per date and mosaic them into one raster.\nFind the change in night light intensity between the two days by subtracting the Feb 07 mosaic raster (pre-blackout data) by the Feb 16 mosaic raster (post-blackout data). The difference in night light intensity will help identify the areas that experienced a blackout.\nReclassify the difference raster to create a “blackout mask”, assuming that any location that experienced a drop of more than 200 nW cm^-2 sr^-1 experienced a blackout. Assign NA values to the areas that did not experience a blackout (all locations that experienced a drop of less than 200 nW cm^-2 sr^-1 change).\nVectorize the blackout mask to create a polygon layer that can be used to identify the census tracts that experienced a blackout.\nCrop (spatially subset) the blackout mask to the Houston area as defined by the following coordinates: (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29). This was completed by the following steps: (1) define the bounding box for Houston using the coordinates provided; (2) create a polygon from the bounding box and make the polygon an sf object; (3) use a spatial subset to crop the blackout mask to the Houston area.\nRe-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\nLoad data\n\n\nShow the code\n# h08v05 on 2021-02-07\nNL_0207_v05 &lt;- stars::read_stars(here(\"posts/2024-11-10-houston-blackout/data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\n# h08v06 on 2021-02-07\nNL_0207_v06 &lt;- stars::read_stars(here(\"posts/2024-11-10-houston-blackout/data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\n# h08v05 on 2021-02-16\nNL_0216_v05 &lt;- stars::read_stars(here(\"posts/2024-11-10-houston-blackout/data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\n# h08v06 on 2021-02-16\nNL_0216_v06 &lt;- stars::read_stars(here(\"posts/2024-11-10-houston-blackout/data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n\n\n\nCreate a raster object for each day with terra::mosaic()\n\n\nShow the code\n# mosaic the raster objects for 2021-02-07\nNL_0207_mosaic &lt;- stars::st_mosaic(NL_0207_v05, NL_0207_v06)\n\n# mosaic the raster objects for 2021-02-16\nNL_0216_mosaic &lt;- stars::st_mosaic(NL_0216_v05, NL_0216_v06)\n\n\n\n\nMake a map showing the night light intensity for each day\n\n\nShow the code\ntmap_mode(\"plot\")\n\nmap1 &lt;- tm_graticules(lines = FALSE) +\ntm_shape(NL_0207_mosaic) + \n  tm_raster(breaks = c(0, 0.2, 1, 3, 5, 10, 100, 200, 10000, 100000), \n            palette = viridisLite::viridis(8), \n            title = \"Night light intensity (nW cm^-2sr^-1)\") + \n  tm_layout(legend.outside = TRUE,\n            main.title = \"2021-02-07\",\n            main.title.size = 1,\n            legend.outside.position = \"right\",\n            legend.text.size = 0.5, \n            legend.title.size = 1)\n\nmap2 &lt;- tm_graticules(lines = FALSE) +\ntm_shape(NL_0216_mosaic) +\n  tm_raster(breaks = c(0, 0.2, 1, 3, 5, 10, 100, 200, 10000, 100000), \n            palette = viridisLite::viridis(8), \n            title = \"Night light intensity (nW cm^-2sr^-1)\") + \n  tm_layout(legend.outside = TRUE,\n            main.title = \"2021-02-16\",\n            main.title.size = 1,\n            legend.outside.position = \"right\",\n            legend.text.size = 0.5, \n            legend.title.size = 1)\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\nFigure 1: Night light intensity in Houston for 2021-02-07 and 2021-02-16. The color scale represents the night light intensity in nW cm^-2 sr^-1.\n\n\n\n\n\n\n\nFind the change in night light intensity between the two days\n\n\nShow the code\n# find the difference in night light intensity\nNL_diff &lt;- NL_0207_mosaic - NL_0216_mosaic\n\n\n\n\nReclassify the difference raster\n\n\nShow the code\n# assign NA values to areas that did not experience a blackout (&lt; 200 nW)\n# assume any location w/ a drop of more than 200 nW experienced a blackout\nNL_diff[NL_diff &lt; 200] &lt;- NA\n# plot(NL_diff)\n\n\n\n\nVectorize the blackout mask\n\n\nShow the code\n# vectorize the blackout mask\nblackout_mask_sf &lt;- st_as_sf(NL_diff) %&gt;% # convert from raster to vector\n  st_make_valid() # fix any invalid geometries \n\n\n\n\nQC1: Check for invalid geometries in the blackout mask\n\n\nShow the code\n# check for invalid geometries in the redlining data with testthat\nif(testthat::expect_true(all(st_is_valid(blackout_mask_sf)))) {\n  print(\"No invalid geometries in the blackout mask\")\n} else {\n  warning(\"There are invalid geometries in the blackout mask\")\n}\n\n\n[1] \"No invalid geometries in the blackout mask\"\n\n\n\n\nCrop the blackout mask to the Houston area\n\n\nShow the code\n# define the bounding box for Houston\nhouston_bbox &lt;- matrix(c(-96.5, 29, \n                         -96.5, 30.5, \n                         -94.5, 30.5, \n                         -94.5, 29, \n                         -96.5, 29), # last coordinate to close the polygon\n                       ncol = 2, byrow = TRUE)\n\n# create a polygon with the bbox list of coordinates\nhouston_polygon &lt;- st_polygon(list(houston_bbox))\n\n# make the polygon an sf object\nhouston_sf &lt;- st_sfc(houston_polygon, \n                     crs = crs(blackout_mask_sf)) # assign same CRS as bo mask \n\n# use a spatial subset to crop the blackout mask to the Houston area\nblackout_mask_houston &lt;- blackout_mask_sf[houston_sf, ]\n# plot(blackout_mask_houston)\n\n# re-project cropped blackout sf to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\nblackout_mask_houston_3083 &lt;- st_transform(blackout_mask_houston, \n                                           crs = 3083)\ncrs(blackout_mask_houston_3083) #QC\n\n\n\n\nQC2: Make sure the spatial subset worked; are all the data points within the Houston area?\n\n\nShow the code\n# change the houston_sf to the same CRS as the blackout mask for the QC\nhouston_sf_QC &lt;- st_transform(houston_sf, \n                              crs(blackout_mask_houston_3083), \n                              quiet = TRUE)\n\n# check if all points are within the Houston area\nif(testthat::expect_true(all(st_intersects(blackout_mask_houston_3083, \n                                           houston_sf_QC)))) {\n  print(\"All points are within the Houston area!\")\n} else {\n  warning(\"STOP! Not all points are within the Houston area\")\n}\n\n\n[1] \"All points are within the Houston area!\""
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#part-2-exclude-highways-from-the-analysis",
    "href": "posts/2024-11-10-houston-blackout/index.html#part-2-exclude-highways-from-the-analysis",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Part 2: Exclude highways from the analysis",
    "text": "Part 2: Exclude highways from the analysis\nBecause highways may have experienced changes in their night light intensities that are unrelated to the storm, we need to exlude any locations within 200m of all highways in the Houston areas.\nPseudocode:\n\nLoad the OpenStreetMap data for Houston and filter the OpenStreetMap data to only include highways. A cool query shared by Ruth was used in the st_read() function to only load the highways from the roads GPKG!\n\n2, Identify all areas within 200m of all highways with the st_buffer() function. Before using st_buffer(), I transformed the highways to the same CRS as the blackout mask (EPSG:3083) and checked the units of the CRS to make sure the buffer was correct.\n\nUse st_union() to combine all the highway buffers into one polygon.\nUse the st_difference() function to exclude the highways from the blackout mask. st_difference(x, y) creates a polygon of the area of x that is not in y (x being the blackout mask, and y being the highway buffer.\n\n\nLoad highways data\n\n\nShow the code\n# load the OpenStreetMap data for Houston\nhighways &lt;- st_read(here(\"posts/2024-11-10-houston-blackout/data/gis_osm_roads_free_1.gpkg/gis_osm_roads_free_1.gpkg\"), \n                    query = \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\")\n\n\n\n\nIdentify all areas within 200m of all highways\n\n\nShow the code\n# put the highways into the same crs as the blackout mask\nhighways_3083 &lt;- st_transform(highways, \n                              crs(blackout_mask_houston_3083))\n\n# check the units of the crs\nst_crs(highways_3083)$units # meters!\n\n# buffer the highways by 200m\nhighways_buffer &lt;- st_buffer(highways_3083, \n                             dist = 200)\n\n# make sure the highways buffer is valid\nhighways_buffer &lt;- st_make_valid(highways_buffer) \n\n# combine all the highway buffers into one polygon\nhighways_buffer_union &lt;- st_union(highways_buffer)\n# plot(highways_buffer_union)\n\n# make sure the highways buffer is in the same CRS as the blackout mask\nhighways_buffer_union &lt;- st_transform(highways_buffer_union, \n                                      crs(blackout_mask_houston_3083))\n\n\n\n\nQC3: Make sure the blackout mask & highways buffer are in the same CRS\n\n\nShow the code\n# check if the blackout mask and highways buffer are in the same CRS\nif(st_crs(blackout_mask_houston_3083) == st_crs(highways_buffer_union)) {\n  print(\"Carry on! The blackout mask and highways buffer are in the same CRS\")\n} else {\n  warning(\"STOP! The blackout mask and highways buffer are not in the same CRS\")\n}\n\n\n[1] \"Carry on! The blackout mask and highways buffer are in the same CRS\"\n\n\n\n\nExclude the highways from the blackout mask using st_difference()\n\n\nShow the code\n# exclude the highways from the blackout mask\nblackout_mask_houston_no_highways &lt;- st_difference(blackout_mask_houston_3083, \n                                                   highways_buffer_union)"
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#part-3-identify-homes-that-experienced-blackouts-by-combining-the-locations-of-homes-and-blackouts",
    "href": "posts/2024-11-10-houston-blackout/index.html#part-3-identify-homes-that-experienced-blackouts-by-combining-the-locations-of-homes-and-blackouts",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Part 3: Identify homes that experienced blackouts by combining the locations of homes and blackouts",
    "text": "Part 3: Identify homes that experienced blackouts by combining the locations of homes and blackouts\n\nLoad the OpenStreetMap data for homes\n\n\nShow the code\nhomes_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nhomes &lt;- st_read(here(\"posts/2024-11-10-houston-blackout/data/gis_osm_buildings_a_free_1.gpkg/gis_osm_buildings_a_free_1.gpkg\"), \n                 query = homes_query)\n\n# transform the crs to the same as the blackout mask\nhomes_3083 &lt;- st_transform(homes, \n                           crs(blackout_mask_houston_no_highways))\n# plot(homes_3083)\n\n\n\n\nQC4: Make sure the homes are in the same CRS as the blackout mask\n\n\nShow the code\n# check if the homes are in the same CRS as the blackout mask\nif(st_crs(homes_3083) == st_crs(blackout_mask_houston_no_highways)) {\n  print(\"Carry on! The homes are in the same CRS as the blackout mask\")\n} else {\n  warning(\"STOP! The homes are not in the same CRS as the blackout mask\")\n}\n\n\n[1] \"Carry on! The homes are in the same CRS as the blackout mask\"\n\n\n\n\nIdentify homes that overlap with areas that experienced blackouts\n\n\nShow the code\n# filter the homes that overlap with the blackout mask\nhomes_blackout &lt;- homes_3083 %&gt;% \n  st_filter(y = blackout_mask_houston_no_highways, .predicate = st_intersects)\n\n\nAccording to this analysis, there were an estimated 157,970 homes in Houston that lost power during the 2021 winter storm (see Figure 2 below). Homes were defined as any building that was classified as residential, apartments, house, static caravan, or detached in the OpenStreetMap buildings data.\n\n\nMake a map of homes in Houston that experienced blackouts\n\n\nShow the code\n# load the Houston county boundary for context \nhouston_counties &lt;- st_read(here(\"posts/2024-11-10-houston-blackout/data/houston_county_boundaries/houston-county-boundaries.shp\")) %&gt;% \n  st_transform(crs(blackout_mask_houston_no_highways))\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\ntm_graticules(lines = FALSE) +\ntm_shape(houston_counties) + \n  tm_polygons(col = \"NAME\", border.col = \"black\", \n              title = \"Surrounding County\", \n              palette = tmaptools::get_brewer_pal(\"Accent\", n = 13, \n                                                  plot = FALSE)) +\ntm_shape(homes_blackout, name = \"Homes\") +\n  tm_dots(col = \"black\", size = 0.1) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_compass(position = c(0.85, 0.85), \n             size = 2) + \n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\nFigure 2: Homes in the Houston metropolitan area that experienced blackouts during the 2021 winter storm. Homes, represented by the black dots, were defined as any building that was classified as residential, apartments, house, static caravan, or detached in the OpenStreetMap buildings data."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#part-4-identify-the-census-tracts-likely-impacted-by-blackouts",
    "href": "posts/2024-11-10-houston-blackout/index.html#part-4-identify-the-census-tracts-likely-impacted-by-blackouts",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Part 4: Identify the census tracts likely impacted by blackouts",
    "text": "Part 4: Identify the census tracts likely impacted by blackouts\nPseudocode:\n\nThe folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase” that contains data from the US Census Bureaus American Community Survet for census tracts in 2019. We need to use the st_layers() function to list the layers in the file geodatabase and identify the layer that contains the census tract data.\nIncome data is stored in the X19_INCOME layer. Looking at the ACS metadata, we can see that the B19013e1 variable represents the median household income.\nThe geodatabase contains a layer holding the geometry information (ACS_2019_5YR_TRACT_48_TEXAS), separate from the layers holding the ACS attributes. We have to combine the geometry with the attributes to get a feature layer that sf can use. We used a left_join() to combine the geometry and income data by the GEOID_Data field.\nIdentify the census tracts that contain homes that experienced blackouts by using the st_filter() function with .predicate = st_intersects to filter the census tract geometries that intersect with the homes that experienced blackouts.\nMake a new column in the sf object containing all census tracts that identified each tract as either experiencing a blackout or not. Use this column to create a boxplot comparing the distribution of median household income in census tracts that experienced blackouts to those that did not.\n\n\nLoad the socio-economic data for Houston\n\n\nShow the code\n# load the ACS gbd layers\nst_layers(here(\"posts/2024-11-10-houston-blackout/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n# load in the geometries layer & assign the same CRS as the blackout mask\nacs_geometry &lt;- st_read(here(\"posts/2024-11-10-houston-blackout/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                        layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") %&gt;% \n  st_transform(crs(blackout_mask_houston_no_highways))\n\n# load in the income layer\nacs_income &lt;- st_read(here(\"posts/2024-11-10-houston-blackout/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                      layer = \"X19_INCOME\")\n\n# select the median household income variable & GEOID\nacs_median_income &lt;- acs_income %&gt;% \n  select(med_income = B19013e1, GEOID_Data = GEOID)\n\n\n\n\nJoin the median household income to the census tract geometries by GEOID_Data\n\n\nShow the code\n# join the median household income to the census tract geometries\nacs_census_income &lt;- left_join(acs_geometry, acs_median_income, \n                               by = \"GEOID_Data\")\n\n\n\n\nIdentify census tracts that contained homes that experienced blackouts\n\n\nShow the code\n# filter the census tracts that contain homes that experienced blackouts\ncensus_tracts_blackout &lt;- acs_census_income %&gt;% \n  st_filter(y = homes_blackout, .predicate = st_intersects)\n\n# list the names of the census tracts that contain homes that experienced blackouts\ncencus_tract_list &lt;- unique(census_tracts_blackout$NAME)\n# cencus_tract_list\n\n# count the number of census tracts that contain homes that experienced blackouts\nnum_census_tracts &lt;- length(cencus_tract_list)\n\n\nAccording to this analysis, there were an estimated 756 census tracts in Houston that contained homes that lost power during the 2021 winter storm.\n\n\nMake a map of the census tracts that contained homes that experienced blackouts\n\n\nShow the code\n# make sure the census tract layer is in the same CRS as the blackout mask\nacs_census_income &lt;- st_transform(acs_census_income, \n                                  crs(blackout_mask_houston_no_highways))\n\n# mask the tract layer to Houston for map context\nfull_tract_layer &lt;- acs_census_income[houston_sf_QC, ]\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\ntm_graticules(lines = FALSE) +\ntm_shape(full_tract_layer) + \n  tm_fill(col = \"grey\", alpha = 0.5, \n          showNA = FALSE) +\ntm_shape(census_tracts_blackout) + \n  tm_fill(col = \"med_income\", \n          style = \"cont\", \n          title = \"Median Household Income ($)\", \n          palette = viridisLite::plasma(5), \n          showNA = FALSE, \n          legend.reverse = TRUE) +\n  tm_compass(size = 2, \n             position = c(0.85, 0.1)) + \n  tm_scale_bar(position = c(0.6, 0.01)) +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\nFigure 3: Census tracts in Houston containing homes that experienced a blackout during the 2021 winter storm. All grey areas represesent census tracts that did not experience blackouts. The color scale represents the median household income ($) in census tracts that experienced a blackout.\n\n\n\n\n\n\n\nMake a plot comparing the distribution of median household income in census tracts that experienced blackouts to those that did not experience blackouts\nThe census_tract_list object contains the census tracts that experienced blackouts. We can use this to create a new column in the full_tract_layer object that indicates whether or not a census tract experienced a blackout.\n\n\nShow the code\n# make a new layer of census tracts that did not experience blackouts\nfull_tract_layer &lt;- full_tract_layer %&gt;% \n  mutate(blackout = ifelse(NAME %in% cencus_tract_list, \"Yes\", \"No\"))\n\n\n\n\nShow the code\n# calculate the median income for census tracts that experienced blackouts and those that did not\nmedian_income_blackout &lt;- full_tract_layer %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(blackout) %&gt;%\n  summarise(med_income = median(med_income, na.rm = TRUE))\n\n# make a boxplot \nggplot(full_tract_layer, aes(x = blackout, y = med_income, fill = blackout)) + \n  geom_boxplot() + \n  scale_fill_manual(values = c(\"No\" = \"grey\", \"Yes\" = \"salmon\")) +\n  theme_bw() + \n  labs(x = \"Blackout\", \n       y = \"Median Household Income ($)\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 4: Distribution of median household income in census tracts that experienced blackouts compared to those that did not experience blackouts. The median income for census tracts that experienced blackouts was $60,415, while the median income for census tracts that did not experience blackouts was approximately $57,220."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#reflection",
    "href": "posts/2024-11-10-houston-blackout/index.html#reflection",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Reflection",
    "text": "Reflection\nThis analysis examined the impacts of the 2021 winter storms in the Houston metropolitan area. We estimated the number of homes that lost power between February 7th and 16th, and examined the relationship between median income levels of census tracts that were likely impacted by the blackout versus those that were not. The analysis found that an estimated 157,970 homes in the Houston metropolitan area lost power during the winter storm. These homes were distributed across 756 census tracts in the region. The median household income in census tracts that experienced blackouts was $60,415, compared to $57,220 in census tracts that did not experience blackouts.\nOne of the limitations of this analysis is that the VIIRS night lights data used to identify areas that experienced blackouts is not a perfect measure of power outages, and while its resolution is relatively granular, it’s still coarse enough that some areas might be “brighter” or “darker” than they truly were. Additionally, the analysis assumes that any location that experienced a drop in night light intensity of more than 200 nW cm^-2 sr^-1 experienced a blackout. This threshold was chosen somewhat arbitrarily and may not accurately capture all areas that lost power. Therefore, taking both of these limitations into account, there’s a chance we didn’t truly capture all homes/locations that experienced blackouts. However, this analysis provides a good estimate of the impacts of the 2021 winter storms in Houston."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#acknowledgements",
    "href": "posts/2024-11-10-houston-blackout/index.html#acknowledgements",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized Ruth Oliver, an Assistant Professor at the Bren School and the instructor for EDS 223. EDS 223 (Geospatial Analysis & Remote Sensing) is offered in the Master of Environmental Data Science (MEDS) program at the Bren School."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#links-to-data-sources",
    "href": "posts/2024-11-10-houston-blackout/index.html#links-to-data-sources",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "Links to data sources",
    "text": "Links to data sources\n\nGeofabrik OSM Data Extracts\nHouston County Boundaries\nOpenStreetMap\nU.S. Census Bureau’s ACS\nVisible Infrared Imaging Radiometer Suite (VIIRS)"
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#references",
    "href": "posts/2024-11-10-houston-blackout/index.html#references",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "References",
    "text": "References\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis."
  },
  {
    "objectID": "posts/2024-11-10-houston-blackout/index.html#github-repository",
    "href": "posts/2024-11-10-houston-blackout/index.html#github-repository",
    "title": "Investigating EJ implications of the Houston blackout",
    "section": "GitHub repository",
    "text": "GitHub repository\nLink to the GitHub repository for this analysis: houston-blackout-analysis"
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html",
    "href": "posts/2024-11-20-aquaculture/index.html",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "",
    "text": "Show packages used in this analysis\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#load-packages",
    "href": "posts/2024-11-20-aquaculture/index.html#load-packages",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "",
    "text": "Show packages used in this analysis\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#background",
    "href": "posts/2024-11-20-aquaculture/index.html#background",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Background",
    "text": "Background\nWith an ever growing human population, marine aquaculture has the potential to play a significant role global food supply, and is a more sustainable option than land-based meat production (Hall et al., 2011). A study that mapped the global potential for marine aquaculture based on multiple constraints (e.g., ship traffic, dissolved oxygen, bottom depth) found that global seafood demand could be met using less than 0.015% of the global ocean area (Gentry et al., 2017).\nThis analysis aims to determine which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to develop marine aquaculture for several species of oysters and Pacific littleneck clams (Leukoma staminea),two of the most common species farmed in marine aquaculture on the West Coast. Suitable growing locations were determined based on range of suitable sea surface temperature (SST) and depth values for each species listed below. Oyster conditions were provided in the assignment description, and suitable temperatures and depths for the littleneck clam were selected based on Shaw (1986) and Harbo (1997). Suitable growing areas for oysters were found first, and then the workflow was made generalizable by transforming into a function that could be used to find suitable growing areas for any marine aquaculture species on the West Coast based on ocean depth and SST.\nSuitable growing conditions for oysters:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nSuitable growing conditions for the Pacific littleneck clam:\n\nsea surface temperature: 0-25°C\ndepth: 0-46 meters below sea level"
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#data-description",
    "href": "posts/2024-11-20-aquaculture/index.html#data-description",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Data description",
    "text": "Data description\n\nSea surface temperature (SST)\nAnnual sea surface temperature (SST) from the years 2008 to 2012 were used to characterize the average SST within the region. This data was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\nData files:\n\naverage_annual_sst_2008.tif\naverage_annual_sst_2009.tif\naverage_annual_sst_2010.tif\naverage_annual_sst_2011.tif\naverage_annual_sst_2012.tif\n\n\n\nBathyemtry\nBathymetric data was sourced from General Bathymetric Chart of the Oceans (GEBCO) to characterize the depth of the ocean (m below sea level).\nData file: depth.tif\n\n\nExclusive Economic Zones (EEZ)\nMaritime boundaries were designated using Exclusive Economic Zones (EEZs) off of the west coast of US from Marine Regions.\nData file: wc_regions_clean.shp\n\n\nU.S. states & territories\nBoundaries for all U.S. states and territories, included for context in the maps, were sourced from NOAA’s national GIS program.\nData file: s_05mr24.shp"
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#analysis",
    "href": "posts/2024-11-20-aquaculture/index.html#analysis",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Analysis",
    "text": "Analysis\nPseudo code/outline:\n\nPrepare data. Load all necessary data, combine the SST data into a raster stack, and make sure they’re all in the same CRS. If not, transform everything to WGS 84 (EPSG:4326).\nProcess data. Find the mean SST from 2008-2012, convert it from Kelvin to Celsius, resample the bathymetry raster to match the resolution, extent, and position of the SST raster, and crop the depth raster to match the extent of the SST raster.\nFind suitable locations. Determine which locations meet the suitable growing conditions for oysters only before expanding the workflow to other species. To do this, we first need to reclassify the bathymetry and SST rasters to either NA (unsuitable), or 1 (suitable). These rasters were then multiplied by each other to isolate areas that meet suitable conditions for both depth and SST.\nDetermine the most suitable EEZ. Calculate the amount of suitable area for each EEZ region to rank zones by priority. To do this, rasterize the EEZ shapefile, mask it to the suitable locations, calculate the area of each cell in the raster, and sum the area of suitable locations in each EEZ. Lastly, join the area data back to the EEZ shapefile and make some visualizations.\nCreate a reproducible workflow for other species. After finding locations with suitable growing conditions for oysters, this workflow was transformed into a function that could isolate suitable growing conditions for any marine aquaculture species on the West Coast based on ocean depth and SST. The function has the following characteristics:\n\n\narguments:\n\nminimum and maximum sea surface temperature\nminimum and maximum depth\nspecies name\n\noutputs:\n\nmap of EEZ regions colored by amount of suitable area\n\n\n\nTest the function for the Pacific littleneck clam! The function was tested for the Pacific littleneck clam using the species-specific growing conditions described above.\n\n\nLoad data\nWe want to load the SST data as a raster stack to make it easier to generate a mean SST raster for the years 2008-2012.\n\n\nShow the code\n# shapefile for West Coast EEZ\nwc_eez &lt;- st_read(here(\"posts/2024-11-20-aquaculture/data/wc_regions_clean.shp\"))\n\n# bathymetry raster\nbathy &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/depth.tif\"))\n\n# SST rasters\nsst_2008 &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/average_annual_sst_2008.tif\"))\nsst_2009 &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/average_annual_sst_2009.tif\"))\nsst_2010 &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/average_annual_sst_2010.tif\"))\nsst_2011 &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/average_annual_sst_2011.tif\"))\nsst_2012 &lt;- rast(here(\"posts/2024-11-20-aquaculture/data/average_annual_sst_2012.tif\"))\n\n# SST raster stack \nsst_stack &lt;- c(sst_2008, sst_2009, sst_2010, sst_2011, sst_2012)\n#plot(sst_stack)\n\n\n\n\nCheck the CRS of each dataset & transform to WGS 84 if they don’t match\n\n\nShow the code\n# # check if each dataset has the same CRS\n# crs(wc_eez) == crs(bathy)\n# crs(wc_eez) == crs(sst_stack)\n# crs(bathy) == crs(sst_stack)\n\n# none are in the same CRS; transform all to WGS 84\nbathy &lt;- project(bathy, \n                 \"EPSG:4326\")\nsst_stack &lt;- project(sst_stack, \n                     \"EPSG:4326\")\nwc_eez &lt;- st_transform(wc_eez, \n                       crs = st_crs(bathy))\n\n# check if they are now in the same CRS\nif(crs(wc_eez) == crs(bathy) & crs(wc_eez) == crs(sst_stack)) {\n  print(\"Carry on! All datasets are now in the same CRS\") \n} else {\n  warning(\"STOP! Datasets are not in the same CRS\")\n}\n\n\n[1] \"Carry on! All datasets are now in the same CRS\"\n\n\n\n\nProcess data\nTo prepare the data for the growing suitability analysis, we found the mean SST from 2008-2012, converted it from Kelvin to Celsius, resampled the bathymetry raster to match the resolution, extent, and position of the SST raster using the nearest neighbor method, and cropped the depth raster to match the extent of the SST raster.\n\n\nShow the code\n# find the mean SST from 2008-2012\n# create a single raster of average SST\nsst_mean &lt;- mean(sst_stack)\n# plot(sst_mean) # QC to make sure this created one raster\n\n\n# convert average SST from Kelvin to Celsius\n# subtract 273.15 from each grid cell in the raster\nsst_mean_celsius &lt;- sst_mean - 273.15\n# plot(sst_mean_celsius) # QC to make sure this worked\n\n\n# resample the bathy raster \n# match the resolution, extent, and position of the SST raster\nbathy_resample &lt;- resample(bathy, \n                           sst_mean_celsius, \n                           method = \"near\")\n\n\n# crop & mask the depth raster to match the extent of the SST raster \nbathy_crop &lt;- crop(bathy_resample, \n                   sst_mean_celsius, \n                   mask = TRUE)\n\n\n\n\nQC: Check that the depth & SST data match in resolution, extent, and CRS\n\n\nShow the code\n# resolution \nif(all(res(bathy_crop) == res(sst_mean_celsius))) {\n  print(\"The bathy and SST data have the same resolution.\")\n} else {\n  warning(\"STOP! Resolution does not match.\")\n}\n\n\n[1] \"The bathy and SST data have the same resolution.\"\n\n\nShow the code\n# extent\nif(ext(bathy_crop) == ext(sst_mean_celsius)) {\n  print(\"The bathy and SST data have the same extent.\")\n} else {\n  warning(\"STOP! The bathy and SST raster do not have the same extent.\")\n}\n\n\n[1] \"The bathy and SST data have the same extent.\"\n\n\nShow the code\n# CRS\nif(crs(bathy_crop) == crs(sst_mean_celsius)) {\n  print(\"The bathy and SST data have the same CRS.\")\n} else {\n  warning(\"STOP! The bathy and SST data do not have the same CRS.\")\n}\n\n\n[1] \"The bathy and SST data have the same CRS.\"\n\n\n\n\nPlot the bathymetric and SST data\n\n\nShow the code\n# load US states for context \nus_states &lt;- st_read(here(\"posts/2024-11-20-aquaculture/data/us_states/s_05mr24.shp\"), \n                     quiet = TRUE) %&gt;% \n  filter(STATE %in% c(\"WA\", \"CA\", \"OR\", \"NV\", \"ID\"))\nus_states &lt;- st_make_valid(us_states) %&gt;% \n  st_transform(crs = st_crs(wc_eez))\n\nbathy_map &lt;- tm_grid(lines = FALSE) + \ntm_shape(us_states) +\n  tm_polygons(col = \"grey\",\n              alpha = 0.2,\n              border.col = \"black\") +\ntm_shape(bathy_crop, \n         is.master = TRUE) + \n  tm_raster(palette = \"Blues\", \n            breaks = c(-4000, -2000, -1000, -500, -100, -50, 0), \n            title = \"Bathymetry (m)\") + \ntm_shape(wc_eez) + \n  tm_borders(col = \"black\", \n              lwd = 1.5) + \n  tm_text(\"rgn_key\", size = 0.7, col = \"black\") +\n  tm_layout(legend.outside = TRUE, \n            frame = TRUE)\nbathy_map\n\n\n\n\n\n\n\n\nFigure 1: Bathymetric (m below sea level) data for the West Coast of the US used in the analysis. West Coast Exclusive Economic Zones (EEZ) are outlined in black. WA = Washington, OR = Oregon, CA-N = northern California, CA-C = central California, and CA-S = southern California.\n\n\n\n\n\n\n\nShow the code\nsst_map &lt;- tm_grid(lines = FALSE) +\ntm_shape(us_states) +\n  tm_polygons(col = \"grey\",\n              alpha = 0.2,\n              border.col = \"black\") +\ntm_shape(sst_mean_celsius, \n         is.master = TRUE) + \n  tm_raster(palette = \"viridis\", \n            style = \"cont\", \n            title = \"Sea Surface Temperature (°C)\") +\ntm_shape(wc_eez) +\n  tm_borders(col = \"black\", \n              lwd = 1.5) + \n  tm_text(\"rgn_key\", size = 0.7, col = \"white\") +\n  tm_layout(legend.outside = TRUE, \n            frame = TRUE)\nsst_map\n\n\n\n\n\n\n\n\nFigure 2: Sea surface temperature (SST) (°C) data for the West Coast of the US used in the analysis. West Coast Exclusive Economic Zones (EEZ) are outlined in black. WA = Washington, OR = Oregon, CA-N = northern California, CA-C = central California, and CA-S = southern California.\n\n\n\n\n\n\n\nFind suitable locations for oysters\nSuitable growing conditions for oysters are defined as follows:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nTo find suitable locations for oysters, we reclassified the SST and depth data into locations that meet the suitable growing conditions for oysters. Suitable areas we defined as 1, and unsuitable areas were assigned NA values. We then used map algebra to multiply the reclassified rasters together to find locations that meet the suitable growing conditions for both depth and SST. If either of these environmental variables’ suitable conditions were not met, the location was assigned a value of NA and considered unsuitable.\n\n\nShow the code\n# define SST reclassification matrix\nrcl_sst &lt;- matrix(c(-Inf, 11, NA, \n                    11, 30, 1, \n                    30, Inf, NA), \n                  ncol = 3, \n                  byrow = TRUE)\n# define depth reclassification matrix\nrcl_depth &lt;- matrix(c(-Inf, -70, NA, \n                      -70, 0, 1, \n                      0, Inf, NA), \n                    ncol = 3, \n                    byrow = TRUE)\n\n# reclassify SST and depth data into locations that are suitable for oysters\nsst_reclass &lt;- classify(sst_mean_celsius, \n                        rcl_sst)\nbathy_reclass &lt;- classify(bathy_crop, \n                          rcl_depth)\n\n\n\n# find locations that meet the suitable growing conditions for oysters\n# values of 1 indicate suitable locations for both depth and SST!\noyster_suitable &lt;-  sst_reclass * bathy_reclass\n# plot(oyster_suitable)\n\n\n\n\nDetermine the most suitable EEZ\nTo determine the most suitable EEZ for oysters, we calculated the total area of suitable growing locations in each EEZ region. We first rasterized the EEZ shapefile to mask it to the suitable locations for oysters. Then, we calculated the area of each cell in the raster using the terra::cellSize() function and summed the area of suitable locations in each EEZ with terra::zonal(). Lastly, we joined the area data back to the EEZ shapefile by rgn and calculated the percent of each EEZs total area that has suitable growing areas for oysters.\n\n\nShow the code\n# rasterize the EEZ shapefile\nwc_eez_rast &lt;- rasterize(wc_eez, \n                         oyster_suitable, \n                         field = \"rgn\")\n# plot(wc_eez_rast) # QC\n\n# crop & mask it to the suitable locations for oysters\nwc_eez_suitable &lt;- crop(wc_eez_rast, \n                        oyster_suitable, \n                        mask = TRUE)\n\n# find the area covered by each cell in the masked EEZ raster\ncell_area &lt;- cellSize(wc_eez_suitable, \n                      unit = \"km\")\n\n# calculate the area of suitable locations for oysters in each EEZ\nsuitable_area &lt;- zonal(cell_area, \n                       wc_eez_suitable, # define the zones to calc area in \n                       fun = \"sum\", \n                       na.rm = TRUE)\n\n# join the area data back to the EEZ shapefile\nwc_eez_joined &lt;- left_join(wc_eez, suitable_area, \n                           by = \"rgn\") %&gt;% \n  rename(suitable_area_km2 = area) %&gt;% \n  select(rgn, rgn_key, area_km2, suitable_area_km2) %&gt;% \n  mutate(percent_suitable = suitable_area_km2 / area_km2 * 100)\n\n\n\n\nVisualize the results\nTo visualize the results, we made two figures: (1) a map of EEZ regions colored by the amount of suitable area (km^2) for oysters; (2) two bar plots to show the total suitable area (km^2) and the percent of each EEZs total area that has suitable growing areas for oysters.\n\n\nShow the code\ntm_grid(lines = FALSE) +\ntm_shape(us_states) +\n  tm_polygons(col = \"grey\",\n              alpha = 0.2,\n              border.col = \"black\") + \ntm_shape(wc_eez_joined, \n         is.master = TRUE) + \n  tm_polygons(col = \"suitable_area_km2\", \n              palette = \"Greens\", \n              title = \"Amount of suitable area (km^2)\", \n              style = \"cat\") + \n  tm_text(\"rgn_key\", size = 0.7, col = \"black\") + \ntm_shape(wc_eez_suitable) + \n    tm_raster(palette = \"black\", \n              legend.show = FALSE) +\n  tm_add_legend(type = \"fill\", \n                labels = \" \", \n                col = \"black\", \n                title = \"Suitable growing areas\") +\n  tm_layout(legend.outside = TRUE, \n            frame = TRUE, \n            main.title = \"Suitable growing area for oysters in West Coast EEZs\", \n            main.title.size = 1.3)\n\n\n\n\n\n\n\n\nFigure 3: Exclusive Economic Zones (EEZ) on the West Coast of the US colored by the amount of suitable area for oysters in each zone (km^2). WA = Washington, OR = Oregon, CA-N = northern California, CA-C = central California, and CA-S = southern California. The black areas show the delineated suitable growing areas for oysters that were summed.\n\n\n\n\n\n\n\nShow the code\nplot1 &lt;- ggplot(wc_eez_joined, \n                aes(x = rgn_key, \n                    y = suitable_area_km2)) + \n  geom_bar(stat = \"identity\", \n           fill = \"seagreen3\", \n           col = \"black\") + \n  labs(title = \" \", \n       x = \"EEZ\", \n       y = \"Suitable area (km^2)\") + \n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, \n                                   hjust = 1))\n#plot1\n\nplot2 &lt;- ggplot(wc_eez_joined, \n                aes(x = rgn_key, \n                    y = percent_suitable)) + \n  geom_bar(stat = \"identity\", \n           fill = \"seagreen4\", \n           col = \"black\") + \n  labs(title = \" \", \n       x = \"EEZ\", \n       y = \"% suitable growing areas\") + \n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, \n                                   hjust = 1))\n#plot2\n\ncombined_plots &lt;- plot1 + plot2\ncombined_plots &lt;- combined_plots + plot_annotation(tag_levels = \"A\")\ncombined_plots\n\n\n\n\n\n\n\n\nFigure 4: Amount of suitable area for oysters in each Exclusive Economic Zone (EEZ) on the West Coast of the US. (A) Shows the total suitable area in km^2, and (B) shows the percent of each EEZs total area that has suitable growing areas for oysters. WA = Washington, OR = Oregon, CA-N = northern California, CA-C = central California, and CA-S = southern California."
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#make-workflow-generalizable",
    "href": "posts/2024-11-20-aquaculture/index.html#make-workflow-generalizable",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Make workflow generalizable",
    "text": "Make workflow generalizable\nNow that we have a workflow to find suitable growing areas for oysters, we can transform this workflow into a function that can be used to find suitable growing areas for any marine aquaculture species on the West Coast based on ocean depth and SST!\n\n\nShow the code\n# function to find suitable locations for any marine aquaculture species\nideal_growing_conditions &lt;- function(species, depth_min, depth_max, \n                                     sst_min, sst_max){\n  \n  ################ reclassification to find suitable habitat ################\n  \n  # define SST reclassification matrix\n  rcl_sst &lt;- matrix(c(-Inf, sst_min, NA, \n                      sst_min, sst_max, 1, \n                      sst_max, Inf, NA), \n                    ncol = 3, \n                    byrow = TRUE) \n  # define depth reclassification matrix\n  rcl_depth &lt;- matrix(c(-Inf, depth_max, NA, \n                        depth_max, depth_min, 1, \n                        depth_min, Inf, NA), \n                      ncol = 3, \n                      byrow = TRUE)\n  \n  \n  # reclassify SST data into suitable locations \n  sst_reclass &lt;- classify(sst_mean_celsius, \n                          rcl_sst) \n  # reclassify depth data into suitable locations\n  bathy_reclass &lt;- classify(bathy_crop, \n                            rcl_depth)\n  \n  \n  # find locations that meet the suitable growing conditions\n  suitable_locations &lt;-  sst_reclass * bathy_reclass\n  \n  \n  ##################### finding the most suitable EEZ #####################\n  \n  # rasterize the EEZ shapefile\n  wc_eez_rast &lt;- rasterize(wc_eez, \n                           suitable_locations, \n                           field = \"rgn\")\n  \n  # crop & mask it to the suitable locations\n  wc_eez_suitable &lt;- crop(wc_eez_rast, \n                          suitable_locations, \n                          mask = TRUE)\n  \n  # find the area of each cell in the masked EEZ raster\n  cell_area &lt;- cellSize(wc_eez_suitable, \n                        unit = \"km\")\n  \n  # calculate the area of suitable locations in each EEZ\n  suitable_area &lt;- zonal(cell_area, \n                         wc_eez_suitable, \n                         fun = \"sum\", \n                         na.rm = TRUE)\n  \n  # join the area data back to the EEZ shapefile\n  wc_eez_joined &lt;- left_join(wc_eez, \n                             suitable_area, \n                             by = \"rgn\") %&gt;% \n    rename(suitable_area_km2 = area) %&gt;% \n    select(rgn, rgn_key, area_km2, suitable_area_km2) %&gt;% \n    mutate(percent_suitable = suitable_area_km2 / area_km2 * 100)\n  \n  \n  ##################### mapping the results #####################\n  \n  # map of EEZ regions colored by the amount of suitable area\n  \n  tm_grid(lines = FALSE) + \n  tm_shape(us_states) +\n  tm_polygons(col = \"grey\",\n              alpha = 0.2,\n              border.col = \"black\") +\n  tm_shape(wc_eez_joined, \n           is.master = TRUE) + \n    tm_polygons(col = \"suitable_area_km2\", \n                palette = \"Greens\", \n                title = \"Amount of suitable area (km^2)\", \n                style = \"cat\") + \n    tm_text(\"rgn_key\", size = 0.7, col = \"black\") +\n  tm_shape(wc_eez_suitable) + \n    tm_raster(palette = \"black\", \n              legend.show = FALSE) +\n  tm_add_legend(type = \"fill\", \n                labels = \" \", \n                col = \"black\", \n                title = \"Suitable growing areas\") +\n    tm_layout(legend.outside = TRUE, \n              frame = TRUE, \n              main.title = paste(\"Suitable growing area for\", species, \n                            \"in West Coast EEZs\"), \n              main.title.size = 1.2)\n  \n}\n\n\n\nTest the function for the Pacific littleneck clam\n\n\nShow the code\n# test the function with the Pacific littleneck clam's growing conditions\nideal_growing_conditions(species = \"Pacific littleneck clams\", \n                         depth_min = 0, \n                         depth_max = -46, \n                         sst_min = 0, \n                         sst_max = 25)\n\n\n\n\n\n\n\n\nFigure 5: Amount of suitable area for the Pacific littleneck clam in each Exclusive Economic Zone (EEZ) on the West Coast of the US. (A) Shows the total suitable area in km^2, and (B) shows the percent of each EEZ’s area that has suitable growing conditions."
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#reflection",
    "href": "posts/2024-11-20-aquaculture/index.html#reflection",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Reflection",
    "text": "Reflection\nLooking at Figure 4, we can see that the central California EEZ (CA-C) has the greatest area of suitable growing locations for oysters based on SST and depth, and the Washington EEZ has the greatest percentage of suitable growing locations compared to the EEZs total area. Looking at Figure 5, we can see that the Washington EEZ has both the greatest area of suitable growing locations and the greatest percentage of suitable growing locations for the Pacific littleneck clam.\nAs the human population continues to grow, the demand for seafood is expected to increase. Marine aquaculture has the potential to play a significant role in meeting this demand, and is a more sustainable option than land-based meat production. This analysis aimed to determine which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to develop marine aquaculture for several species of oysters and the Pacific littleneck clam. We only considered two species and two variables driving suitable growing conditions in this analysis, but the workflow can be easily adapted to other species and additional predictor variables to create more robust suitability modeling."
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#acknowledgements",
    "href": "posts/2024-11-20-aquaculture/index.html#acknowledgements",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was created and organized Ruth Oliver, an Assistant Professor at the Bren School and the instructor for EDS 223. EDS 223 (Geospatial Analysis & Remote Sensing) is offered in the Master of Environmental Data Science (MEDS) program at the Bren School."
  },
  {
    "objectID": "posts/2024-11-20-aquaculture/index.html#references",
    "href": "posts/2024-11-20-aquaculture/index.html#references",
    "title": "Marine aquacultire suitability on the West Coast",
    "section": "References",
    "text": "References\nFlanders Marine Institute (2023). Maritime Boundaries Geodatabase: Maritime Boundaries and Exclusive Economic Zones (200NM), version 12. Available online at https://www.marineregions.org/. https://doi.org/10.14284/632\nGEBCO Compilation Group (2024) GEBCO 2024 Grid (doi:10.5285/1c44ce99-0a0d-5f4f-e063-7086abc0ea0f)\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).\nHarbo, R. M. (1997). Shells & shellfish of the Pacific northwest: a field guide. Madiera Park, BC: Harbour Publishing.\nNOAA Coral Reef Watch. 2019, updated daily. NOAA Coral Reef Watch Version 3.1 Daily 5km Satellite Regional Virtual Station Time Series Data for Southeast Florida, Mar. 12, 2013-Mar. 11, 2014. College Park, Maryland, USA: NOAA Coral Reef Watch. Data set accessed at https://coralreefwatch.noaa.gov/product/vs/data.php.\nShaw, W N. (1986) Species profiles: life histories and environmental requirements of coastal fishes and invertebrates (Pacific Southwest): Common littleneck clam. Protothaca staminea. United States."
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "A workflow adapted from the Climate DataLab as part of EDS 296-1S (Advanced Special Topics in Environmental Data Science: A Climate Modeling Perspective on Big Data Techniques), a course offered at the Bren School of Environmental Science & Management.\n\n\nThis post walks through the process of accessing, processing, and visualizing large-scale climate data from Amazon Web Services (AWS) using Python in a Jupyter Lab environment. Focusing on Southern Africa, the analysis draws on two global climate models — GFDL-ESM4 and IPSL-CM6A-LR — to download and map historical simulations and future projections of air temperature. The workflow highlights geospatial data manipulation with xarray, rioxarray, and geopandas, including time averaging, model comparison, and visualization using matplotlib and cartopy. This post provides a practical introduction to open-source tools for regional climate analysis with cloud-hosted datasets.\n\n\nShow code\n# load packages\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport intake\nimport s3fs\nimport numpy as np\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport cftime\nfrom cftime import DatetimeNoLeap\n\n\n\n\n\n\n\n\nUsing the CMIP6 database hosted on Amazon Web Services, we will be downloading/using both historical simulations and future projections from two climate models (GFDL-ESM4 and IPSL-CM6A-LR) for ssp370.\nClimate Models:  GFDL-ESM4 is the Geophysical Fluid Dynamics Laboratory (GFDL) Earth System Model version 4 (ESM4) climate model. It’s a coupled chemistry-carbon-climate Earth system model that was developed at the GFDL. As described by NOAA, GFDL-ESM4 consists of:\n\natmosphere at approximately 1 degree resolution with 49 levels of comprehensive, interactive chemistry and aerosols (including aerosol indirect effect) from precursor emissions\nOM4 MOM6-based ocean at ½ degree resolution with 75 levels using hybrid pressure/isopycnal vertical coordinate\nSIS2 sea ice with radiative transfer and C-grid dynamics for compatibility with MOM6\nLM4.1 land model with a new vegetation dynamics model with explicit treatment of plant age and height structure and soil microbes, with daily fire, crops, pasture, and grazing tiles\nCOBALTv2 ocean biogeochemical component representing ocean ecological and biogeochemical interactions\nFully Interactive dust and iron cycling between land-atmosphere and ocean\n\nIPSL-CM6A-LR is the latest version of the Institut Pierre Simon Laplace (IPSL) climate model. It’s a physical atmosphere-land-ocean-sea ice model that also includes a representation of the carbon cycle, and it was developed at the IPSL to study natural climate variability and climate response to both anthropogenic and natural changes as part of CMIP6.\nSSP Scenario:  ssp370 (also written as SSP3-RCP7.0) refers to the Shared Socioeconomic Pathway 3 combined with a radiative forcing level of 7.0 W/m² by the year 2100. It is considered a medium-to-high emissions scenario and commonly known as the “Regional Rivalry” pathway, characterized by high challenges to both mitigation and adaptation. A Carbon Brief article written by Zeke Hausfather describes SSP3 as the following:\n\n“A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.”\n\n\n\n\nWe’ll be downloading surface air temperature (tas) data from the following two climate models:\n\nGFDL-ESM4\nIPSL-CM6A-LR\n\nFor each climate model, we need:\n\nHistorical simulations\nFuture projections for ssp370\n\nCharacteristics of these files (corresponding fields in the CMIP6 catalog are in parentheses):\n\nVariable (“variable_id”): This is a surface air temperature, or “tas”, variable.\nRealm (“table_id”): Surface air temperature is generated by the atmosphere component of a climate model (“A”), and the information in this particular file is averaged monthly (“mon”).\nModel (“source_id”): The name of the climate model\nExperiment (“experiment_id”): The name of the model experiment being run. The file above is a historical simulation: since we’re also interested in the future projection information, we’ll further specify that we’d also like the associated SSP below.\nEnsemble member (“member_id”): The name of this ensemble member is “r1i1p1f1”.\n\nThe workflow used to download the data was guided by the Accessing CMIP6 Data via AWS tutorial.\n\n\nShow code\n# Open the CMIP6 data catalog, store as a variable\nCMIP6_catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')\n\n\n\n\n\n\nShow code\n# Specify search terms to query catalog for GFDL-ESM4 data\n# activity_id: which project do you want? CMIP = historical data, ScenarioMIP = future projections\nactivity_ids = ['ScenarioMIP', 'CMIP']\n\n# source_id: which model do you want? \nsource_id = ['GFDL-ESM4']\n\n# experiment_id: what experimental configuration do you want? Here we want historical and ssp370\nexperiment_ids = ['historical', 'ssp370']\n\n# member_id: which ensemble member do you want? Here we want r1i1p1f1\nmember_id = 'r1i1p1f1'\n\n# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\ntable_id = 'Amon'\n\n# variable_id: which climate variable do you want? Here we want surface air temperature\nvariable_id = 'tas'\n\n\n\n\nShow code\n# Search through catalog, store results in \"res\" variable\nres_gfdl = CMIP6_catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n                     member_id=member_id, table_id=table_id, variable_id=variable_id)\n\n# Display data frame associated with results\ndisplay(res_gfdl.df)\n\n# Extract data for the historical period, store as a separate xarray Dataset\nhist_data_gfdl = xr.open_zarr(res_gfdl.df['zstore'][1], storage_options={'anon': True})\n\n# Extract data for each SSP \nfuture_data_ssp370_gfdl = xr.open_zarr(res_gfdl.df['zstore'][0], storage_options={'anon': True})\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n0\nScenarioMIP\nNOAA-GFDL\nGFDL-ESM4\nssp370\nr1i1p1f1\nAmon\ntas\ngr1\ns3://cmip6-pds/CMIP6/ScenarioMIP/NOAA-GFDL/GFD...\nNaN\n20180701\n\n\n1\nCMIP\nNOAA-GFDL\nGFDL-ESM4\nhistorical\nr1i1p1f1\nAmon\ntas\ngr1\ns3://cmip6-pds/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/...\nNaN\n20190726\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Specify search terms to query catalog for IPSL-CM6A data\n# activity_id: which project do you want? CMIP = historical data, ScenarioMIP = future projections\nactivity_ids = ['ScenarioMIP', 'CMIP']\n\n# source_id: which model do you want? \nsource_id = ['IPSL-CM6A-LR']\n\n# experiment_id: what experimental configuration do you want? Here we want historical and ssp370\nexperiment_ids = ['historical', 'ssp370']\n\n# member_id: which ensemble member do you want? Here we want r1i1p1f1\nmember_id = 'r1i1p1f1'\n\n# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\ntable_id = 'Amon'\n\n# variable_id: which climate variable do you want? Here we want surface air temperature\nvariable_id = 'tas'\n\n\n\n\nShow code\n# Search through catalog, store results in \"res\" variable\nres_ipsl = CMIP6_catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n                     member_id=member_id, table_id=table_id, variable_id=variable_id)\n\n# Display data frame associated with results\ndisplay(res_ipsl.df)\n\n# Extract data for the historical period, store as a separate xarray Dataset\nhist_data_ipsl = xr.open_zarr(res_ipsl.df['zstore'][0], storage_options={'anon': True})\n\n# Extract data for each SSP \nfuture_data_ssp370_ipsl = xr.open_zarr(res_ipsl.df['zstore'][1], storage_options={'anon': True})\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n0\nCMIP\nIPSL\nIPSL-CM6A-LR\nhistorical\nr1i1p1f1\nAmon\ntas\ngr\ns3://cmip6-pds/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/hi...\nNaN\n20180803\n\n\n1\nScenarioMIP\nIPSL\nIPSL-CM6A-LR\nssp370\nr1i1p1f1\nAmon\ntas\ngr\ns3://cmip6-pds/CMIP6/ScenarioMIP/IPSL/IPSL-CM6...\nNaN\n20190119\n\n\n\n\n\n\n\n\n\n\n\nThe focus region for this analysis is Southern Africa. As it sounds, it’s the southernmost region of Africa. There are various definitions for the exact extent of this region, but we’ll be using the delineation of the United Nations geoscheme that includes Botswana, Eswatini, Lesotho, Namibia, and South Africa.\nSouthern Africa Climate:  Due to its size and varied geography, the greater Southern Africa region exhibits a wide range of climates. In general, this area is characterized by a subtropical to temperate climate, with significant variations in temperature and rainfall throughout the year. The northwest reaches of Southern Africa (namely western Namibia) exhibit desert conditions, the eastern coast can be classified as more subtropical, and the southwest (coastal reaches of South Africa) experience a Mediterranean climate. As mentioned above, average mean surface air temperature and precipitation vary significanly both throughout the region and annually. Southern areas throughout South Africa are cooler than the northern locations in Namibia and Botswana, with average mean surface air temperatures around 17 - 19 deg C and 22 - 24 deg C, respectively. Annually, temperatures fluctuate significantly with Nov - Mar being the hottest, and Apr - Oct the coldest. Southern areas of Southern Africa recieve less mean annual rainfall than northern areas of this region, and similarly to temperature trends, annual precipitation varies significantly with the vast majority of rainfall occuring in the wet season months (Nov - March), and little to no rainfall during the dry season (Apr - Oct).\nClimate Change Impacts:  In a region that already experiences drastic fluctuations in temperature and precipitation throughout each year, I might expect these patterns to become even more pronounced under a changing climate. I would expect overall rainfall to decrease, with an increase in extreme precipitation events like atmospheric rivers. This would likely result in more frequent drying of river and stream networks, with and more frequent flooding events. Following global trends, I would expect average temperatures to also increase throughout the greater Southern Africa Region.\nA reasonable lat/lon bounds for the Southern Africa Region is:\n\nLatitude range of 38°S to 15°S\nLongitude range of 10°E to 38°E\n\nInformation on Southern Africa’s climate was sourced from the Climate Change Knowledge Portal.\n\n\n\nThis analysis focuses on the following two time periods:\n\n2000-2050\n2050-2100\n\nI chose these time periods to get a sense of climatic trends in a recent to mid century window, and mid to late century window to acquire a full picture of temperature trends in the Southern Africa region throughout this century.\n\n\nFirst, we’ll concatenate the historical and SSP information into a single xarray object to make the plotting simpler:\n\n\nShow code\n# GFDL-ESM4 data\ngfdl_data_ssp370 = xr.concat([hist_data_gfdl, future_data_ssp370_gfdl], dim=\"time\")\n\n# IPSL-CM6A-LR data \nipsl_data_ssp370 = xr.concat([hist_data_ipsl, future_data_ssp370_ipsl], dim=\"time\")\n\n\n\n\nShow code\n# examine the time format for both datasets \n#print(gfdl_data_ssp370.time)\n#print(ipsl_data_ssp370.time)\n\n\n\n\n\nIt looks like the IPSL model data is already in a dtype of datetime64; however, we’ll need to change the format of the GFDL data to datetime64 to make plotting easier with Matplotlib\n\n\nShow code\n# convert the time variable to datetime format in the GFDL data\ntime = gfdl_data_ssp370.time.astype('datetime64[ns]')\n#print(time)\n\n\n\n\n\n\n\nFirst, we need to examine the time formats of each climate model.\n\n\nShow code\nprint(type(gfdl_data_ssp370.time.values[0]))\n\n\n&lt;class 'cftime._cftime.DatetimeNoLeap'&gt;\n\n\n\n\nShow code\nprint(type(ipsl_data_ssp370.time.values[0]))\n\n\n&lt;class 'numpy.datetime64'&gt;\n\n\ngfdl_data_ssp370 uses cftime.DatetimeNoLeap, which requires using that same calendar class. So, we need to use a different start and end date format that’s compatible for selecting specific time periods out of this object. The ipsl_data_ssp370 time coordinate uses standard numpy.datetime64, so we can slice it with normal strings like “2000-01-01” and don’t need cftime.\n\n\nShow code\n# Extract an xarray DataArray containing the tas variable\ngfdl_data_ssp370 = gfdl_data_ssp370.tas\nipsl_data_ssp370 = ipsl_data_ssp370.tas\n\n# convert units from Kelvin to Celsius\ngfdl_data_ssp370 = gfdl_data_ssp370 - 273.15\nipsl_data_ssp370 = ipsl_data_ssp370 - 273.15\n\n\n\n\n\n\n\nShow code\n# define the first time period\nstart_date1 = DatetimeNoLeap(2000, 1, 1)\nend_date1 = DatetimeNoLeap(2050, 12, 31)\n\n# define the second time period \nstart_date2 = DatetimeNoLeap(2050, 1, 1)\nend_date2 = DatetimeNoLeap(2100, 12, 31)\n\n\n# extract data for 2000-2050\ntemp_data_slice_gfdl_2000 = gfdl_data_ssp370.sel(\n    time=(gfdl_data_ssp370.time &gt;= start_date1) & \n         (gfdl_data_ssp370.time &lt;= end_date1)\n)\n\n# extract data for 2050-2100\ntemp_data_slice_gfdl_2050 = gfdl_data_ssp370.sel(\n    time=(gfdl_data_ssp370.time &gt;= start_date2) & \n         (gfdl_data_ssp370.time &lt;= end_date2)\n)\n\n\n\n\n\n\n\nShow code\n# extract data for 2000-2050\ntemp_data_slice_ipsl_2000 = ipsl_data_ssp370.sel(\n    time=slice(\"2000-01-01\", \"2050-12-31\")\n)\n\n# extract data for 2050-2100\ntemp_data_slice_ipsl_2050 = ipsl_data_ssp370.sel(\n    time=slice(\"2050-01-01\", \"2100-12-31\")\n)\n\n\n\n\n\n\nTake the time average of our filtered data for each climate model, and plot it for our specific region of interest (Southern Africa).\n\n\nShow code\n# Time average GFDL\ntemp_data_mn_gfdl_2000 = temp_data_slice_gfdl_2000.mean(dim=[\"time\"])\ntemp_data_mn_gfdl_2050 = temp_data_slice_gfdl_2050.mean(dim=[\"time\"])\n\n# Time average IPSL\ntemp_data_mn_ipsl_2000 = temp_data_slice_ipsl_2000.mean(dim=[\"time\"])\ntemp_data_mn_ipsl_2050 = temp_data_slice_ipsl_2050.mean(dim=[\"time\"])\n\n\n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create figure with two subplots side by side\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10),\n                         subplot_kw={\"projection\": map_proj})\n\n# Shared settings\nextent = [10, 38, -38, -15]\n\n# Plot for 2000-2050\nplot1 = axes[0].pcolormesh(\n    temp_data_mn_gfdl_2000.lon, temp_data_mn_gfdl_2000.lat,\n    temp_data_mn_gfdl_2000, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\naxes[0].set_title(\"Surface Air Temperature: 2000–2050\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl1.xlines = False\ngl1.ylines = False\n\n# Plot for 2050-2100\nplot2 = axes[1].pcolormesh(\n    temp_data_mn_gfdl_2050.lon, temp_data_mn_gfdl_2050.lat,\n    temp_data_mn_gfdl_2050, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\naxes[1].set_title(\"Surface Air Temperature: 2050–2100\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl2.xlines = False\ngl2.ylines = False\n\n# Add shared colorbar\ncbar = fig.colorbar(plot2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Surface Air Temperature for Southern Africa (GFDL-ESM4)\", fontsize=20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create figure with two subplots side by side\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10),\n                         subplot_kw={\"projection\": map_proj})\n\n# Shared settings\nextent = [10, 38, -38, -15]\n\n# Plot for 2000-2050\nplot1 = axes[0].pcolormesh(\n    temp_data_mn_ipsl_2000.lon, temp_data_mn_ipsl_2000.lat,\n    temp_data_mn_ipsl_2000, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\naxes[0].set_title(\"Surface Air Temperature: 2000–2050\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl1.xlines = False\ngl1.ylines = False\n\n# Plot for 2050-2100\nplot2 = axes[1].pcolormesh(\n    temp_data_mn_ipsl_2050.lon, temp_data_mn_ipsl_2050.lat,\n    temp_data_mn_ipsl_2050, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\naxes[1].set_title(\"Surface Air Temperature: 2050–2100\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl2.xlines = False\ngl2.ylines = False\n\n# Add shared colorbar\ncbar = fig.colorbar(plot2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Surface Air Temperature for Southern Africa (IPSL-CM6A-LR)\", fontsize=20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake the difference between each time period.\n\n\nShow code\n# subtract the mid century tas data from the late century tas data\ngfdl_tas_diff = temp_data_mn_gfdl_2050 - temp_data_mn_gfdl_2000 \nipsl_tas_diff = temp_data_mn_ipsl_2050 - temp_data_mn_ipsl_2000 \n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(20, 10), \n                         subplot_kw={'projection': map_proj})\n\n# Set common extent for both subplots (Southern Africa)\nextent = [10, 38, -38, -15]\n\n# --- Plot 1: GFDL-ESM4 ---\np1 = axes[0].pcolormesh(\n    gfdl_tas_diff.lon, gfdl_tas_diff.lat, gfdl_tas_diff,\n    transform=map_proj, cmap=\"coolwarm\", vmin=-0.5, vmax=3.5\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\ngl1.xlines = False\ngl1.ylines = False\naxes[0].set_title(\"GFDL-ESM4: Δ Temp [(2050–2100) - (2000–2050)]\", fontsize=14)\n\n# --- Plot 2: IPSL-CM6A-LR ---\np2 = axes[1].pcolormesh(\n    ipsl_tas_diff.lon, ipsl_tas_diff.lat, ipsl_tas_diff,\n    transform=map_proj, cmap=\"coolwarm\", vmin=-0.5, vmax=3.5\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\ngl2.xlines = False\ngl2.ylines = False\naxes[1].set_title(\"IPSL-CM6A-LR: Δ Temp [(2050–2100) - (2000–2050)]\", fontsize=14)\n\n# Add shared colorbar below the plots\ncbar = fig.colorbar(p2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature Change (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Change in Surface Air Temperature: Southern Africa\", fontsize=18)\n\n# Add a caption\nfig.text(0.5, 0.02, \n         \"Temperature difference is taken by subracting the 50 year average between 2000-2050 from 50 year average between 2050-2100. Therefore, positive values represent an increase in average surface air temperatures.\", \n         ha='center', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults:  Looking at the first sets of plots that show the projected average surface air temperature in Southern Africa between time periods of 2000-2050 and 2050-2100, it’s evident that temperatures are projected to increase moving into the late century. This trend becomes more apparent when taking the difference between the average temperatures of each time period. We subtracted the earlier period from the later period; therefore, positive values represent an increase in projected average temperatures, and negative values represent a decrease. Looking at our difference plots above for each climate model, there’s a clear trend of increasing projected air temperatures. The IPSL-CM6A-LR climate model projects a more significant increase than the GFDL-ESM4 model.\nImplications:  Rising average surface air temperatures across the greater Southern Africa region are likely to have widespread and compounding effects on both human populations and ecosystems. Higher temperatures can lead to increased evaporation rates and soil moisture loss, further stressing agricultural systems already vulnerable to climate variability. Crop failures may become more frequent, and livestock health may deteriorate as heat stress intensifies. Ecosystems throughout the region will also face mounting pressure, with species being forced to adapt, migrate, or face possible decline as thermal thresholds are exceeded. For human populations, prolonged exposure to extreme heat may worsen health outcomes, particularly for those with limited access to cooling infrastructure or healthcare. In urban areas, the intensification of the urban heat island effect could exacerbate social inequalities, while in rural areas, higher temperatures may further strain already limited water and food resources.\n\n\n\nThis assignment was created and organized by Sam Stevenson-Karl, an Associate Professor at the Bren School and the instructor for EDS 296-1S. EDS 296-1S (Advanced Special Topics in Environmental Data Science: A Climate Modeling Perspective on Big Data Techniques) is offered in the Master of Environmental Data Science (MEDS) program at the Bren School.\nThe workflow used to complete this analysis was adapted from the Climate DataLab, an open source resource for climate model education that’s supported by a growing community of educators, scientists, and developers working together to make climate data science more inclusive and effective."
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#overview",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#overview",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "This post walks through the process of accessing, processing, and visualizing large-scale climate data from Amazon Web Services (AWS) using Python in a Jupyter Lab environment. Focusing on Southern Africa, the analysis draws on two global climate models — GFDL-ESM4 and IPSL-CM6A-LR — to download and map historical simulations and future projections of air temperature. The workflow highlights geospatial data manipulation with xarray, rioxarray, and geopandas, including time averaging, model comparison, and visualization using matplotlib and cartopy. This post provides a practical introduction to open-source tools for regional climate analysis with cloud-hosted datasets.\n\n\nShow code\n# load packages\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport intake\nimport s3fs\nimport numpy as np\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport cftime\nfrom cftime import DatetimeNoLeap"
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#climate-data-description",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#climate-data-description",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "Using the CMIP6 database hosted on Amazon Web Services, we will be downloading/using both historical simulations and future projections from two climate models (GFDL-ESM4 and IPSL-CM6A-LR) for ssp370.\nClimate Models:  GFDL-ESM4 is the Geophysical Fluid Dynamics Laboratory (GFDL) Earth System Model version 4 (ESM4) climate model. It’s a coupled chemistry-carbon-climate Earth system model that was developed at the GFDL. As described by NOAA, GFDL-ESM4 consists of:\n\natmosphere at approximately 1 degree resolution with 49 levels of comprehensive, interactive chemistry and aerosols (including aerosol indirect effect) from precursor emissions\nOM4 MOM6-based ocean at ½ degree resolution with 75 levels using hybrid pressure/isopycnal vertical coordinate\nSIS2 sea ice with radiative transfer and C-grid dynamics for compatibility with MOM6\nLM4.1 land model with a new vegetation dynamics model with explicit treatment of plant age and height structure and soil microbes, with daily fire, crops, pasture, and grazing tiles\nCOBALTv2 ocean biogeochemical component representing ocean ecological and biogeochemical interactions\nFully Interactive dust and iron cycling between land-atmosphere and ocean\n\nIPSL-CM6A-LR is the latest version of the Institut Pierre Simon Laplace (IPSL) climate model. It’s a physical atmosphere-land-ocean-sea ice model that also includes a representation of the carbon cycle, and it was developed at the IPSL to study natural climate variability and climate response to both anthropogenic and natural changes as part of CMIP6.\nSSP Scenario:  ssp370 (also written as SSP3-RCP7.0) refers to the Shared Socioeconomic Pathway 3 combined with a radiative forcing level of 7.0 W/m² by the year 2100. It is considered a medium-to-high emissions scenario and commonly known as the “Regional Rivalry” pathway, characterized by high challenges to both mitigation and adaptation. A Carbon Brief article written by Zeke Hausfather describes SSP3 as the following:\n\n“A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.”"
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#climate-data-download-acquisition",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#climate-data-download-acquisition",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "We’ll be downloading surface air temperature (tas) data from the following two climate models:\n\nGFDL-ESM4\nIPSL-CM6A-LR\n\nFor each climate model, we need:\n\nHistorical simulations\nFuture projections for ssp370\n\nCharacteristics of these files (corresponding fields in the CMIP6 catalog are in parentheses):\n\nVariable (“variable_id”): This is a surface air temperature, or “tas”, variable.\nRealm (“table_id”): Surface air temperature is generated by the atmosphere component of a climate model (“A”), and the information in this particular file is averaged monthly (“mon”).\nModel (“source_id”): The name of the climate model\nExperiment (“experiment_id”): The name of the model experiment being run. The file above is a historical simulation: since we’re also interested in the future projection information, we’ll further specify that we’d also like the associated SSP below.\nEnsemble member (“member_id”): The name of this ensemble member is “r1i1p1f1”.\n\nThe workflow used to download the data was guided by the Accessing CMIP6 Data via AWS tutorial.\n\n\nShow code\n# Open the CMIP6 data catalog, store as a variable\nCMIP6_catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')\n\n\n\n\n\n\nShow code\n# Specify search terms to query catalog for GFDL-ESM4 data\n# activity_id: which project do you want? CMIP = historical data, ScenarioMIP = future projections\nactivity_ids = ['ScenarioMIP', 'CMIP']\n\n# source_id: which model do you want? \nsource_id = ['GFDL-ESM4']\n\n# experiment_id: what experimental configuration do you want? Here we want historical and ssp370\nexperiment_ids = ['historical', 'ssp370']\n\n# member_id: which ensemble member do you want? Here we want r1i1p1f1\nmember_id = 'r1i1p1f1'\n\n# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\ntable_id = 'Amon'\n\n# variable_id: which climate variable do you want? Here we want surface air temperature\nvariable_id = 'tas'\n\n\n\n\nShow code\n# Search through catalog, store results in \"res\" variable\nres_gfdl = CMIP6_catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n                     member_id=member_id, table_id=table_id, variable_id=variable_id)\n\n# Display data frame associated with results\ndisplay(res_gfdl.df)\n\n# Extract data for the historical period, store as a separate xarray Dataset\nhist_data_gfdl = xr.open_zarr(res_gfdl.df['zstore'][1], storage_options={'anon': True})\n\n# Extract data for each SSP \nfuture_data_ssp370_gfdl = xr.open_zarr(res_gfdl.df['zstore'][0], storage_options={'anon': True})\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n0\nScenarioMIP\nNOAA-GFDL\nGFDL-ESM4\nssp370\nr1i1p1f1\nAmon\ntas\ngr1\ns3://cmip6-pds/CMIP6/ScenarioMIP/NOAA-GFDL/GFD...\nNaN\n20180701\n\n\n1\nCMIP\nNOAA-GFDL\nGFDL-ESM4\nhistorical\nr1i1p1f1\nAmon\ntas\ngr1\ns3://cmip6-pds/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/...\nNaN\n20190726\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Specify search terms to query catalog for IPSL-CM6A data\n# activity_id: which project do you want? CMIP = historical data, ScenarioMIP = future projections\nactivity_ids = ['ScenarioMIP', 'CMIP']\n\n# source_id: which model do you want? \nsource_id = ['IPSL-CM6A-LR']\n\n# experiment_id: what experimental configuration do you want? Here we want historical and ssp370\nexperiment_ids = ['historical', 'ssp370']\n\n# member_id: which ensemble member do you want? Here we want r1i1p1f1\nmember_id = 'r1i1p1f1'\n\n# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\ntable_id = 'Amon'\n\n# variable_id: which climate variable do you want? Here we want surface air temperature\nvariable_id = 'tas'\n\n\n\n\nShow code\n# Search through catalog, store results in \"res\" variable\nres_ipsl = CMIP6_catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n                     member_id=member_id, table_id=table_id, variable_id=variable_id)\n\n# Display data frame associated with results\ndisplay(res_ipsl.df)\n\n# Extract data for the historical period, store as a separate xarray Dataset\nhist_data_ipsl = xr.open_zarr(res_ipsl.df['zstore'][0], storage_options={'anon': True})\n\n# Extract data for each SSP \nfuture_data_ssp370_ipsl = xr.open_zarr(res_ipsl.df['zstore'][1], storage_options={'anon': True})\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n0\nCMIP\nIPSL\nIPSL-CM6A-LR\nhistorical\nr1i1p1f1\nAmon\ntas\ngr\ns3://cmip6-pds/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/hi...\nNaN\n20180803\n\n\n1\nScenarioMIP\nIPSL\nIPSL-CM6A-LR\nssp370\nr1i1p1f1\nAmon\ntas\ngr\ns3://cmip6-pds/CMIP6/ScenarioMIP/IPSL/IPSL-CM6...\nNaN\n20190119"
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#focus-region-southern-africa",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#focus-region-southern-africa",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "The focus region for this analysis is Southern Africa. As it sounds, it’s the southernmost region of Africa. There are various definitions for the exact extent of this region, but we’ll be using the delineation of the United Nations geoscheme that includes Botswana, Eswatini, Lesotho, Namibia, and South Africa.\nSouthern Africa Climate:  Due to its size and varied geography, the greater Southern Africa region exhibits a wide range of climates. In general, this area is characterized by a subtropical to temperate climate, with significant variations in temperature and rainfall throughout the year. The northwest reaches of Southern Africa (namely western Namibia) exhibit desert conditions, the eastern coast can be classified as more subtropical, and the southwest (coastal reaches of South Africa) experience a Mediterranean climate. As mentioned above, average mean surface air temperature and precipitation vary significanly both throughout the region and annually. Southern areas throughout South Africa are cooler than the northern locations in Namibia and Botswana, with average mean surface air temperatures around 17 - 19 deg C and 22 - 24 deg C, respectively. Annually, temperatures fluctuate significantly with Nov - Mar being the hottest, and Apr - Oct the coldest. Southern areas of Southern Africa recieve less mean annual rainfall than northern areas of this region, and similarly to temperature trends, annual precipitation varies significantly with the vast majority of rainfall occuring in the wet season months (Nov - March), and little to no rainfall during the dry season (Apr - Oct).\nClimate Change Impacts:  In a region that already experiences drastic fluctuations in temperature and precipitation throughout each year, I might expect these patterns to become even more pronounced under a changing climate. I would expect overall rainfall to decrease, with an increase in extreme precipitation events like atmospheric rivers. This would likely result in more frequent drying of river and stream networks, with and more frequent flooding events. Following global trends, I would expect average temperatures to also increase throughout the greater Southern Africa Region.\nA reasonable lat/lon bounds for the Southern Africa Region is:\n\nLatitude range of 38°S to 15°S\nLongitude range of 10°E to 38°E\n\nInformation on Southern Africa’s climate was sourced from the Climate Change Knowledge Portal."
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#focus-on-specific-time-periods-make-maps",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#focus-on-specific-time-periods-make-maps",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "This analysis focuses on the following two time periods:\n\n2000-2050\n2050-2100\n\nI chose these time periods to get a sense of climatic trends in a recent to mid century window, and mid to late century window to acquire a full picture of temperature trends in the Southern Africa region throughout this century.\n\n\nFirst, we’ll concatenate the historical and SSP information into a single xarray object to make the plotting simpler:\n\n\nShow code\n# GFDL-ESM4 data\ngfdl_data_ssp370 = xr.concat([hist_data_gfdl, future_data_ssp370_gfdl], dim=\"time\")\n\n# IPSL-CM6A-LR data \nipsl_data_ssp370 = xr.concat([hist_data_ipsl, future_data_ssp370_ipsl], dim=\"time\")\n\n\n\n\nShow code\n# examine the time format for both datasets \n#print(gfdl_data_ssp370.time)\n#print(ipsl_data_ssp370.time)\n\n\n\n\n\nIt looks like the IPSL model data is already in a dtype of datetime64; however, we’ll need to change the format of the GFDL data to datetime64 to make plotting easier with Matplotlib\n\n\nShow code\n# convert the time variable to datetime format in the GFDL data\ntime = gfdl_data_ssp370.time.astype('datetime64[ns]')\n#print(time)\n\n\n\n\n\n\n\nFirst, we need to examine the time formats of each climate model.\n\n\nShow code\nprint(type(gfdl_data_ssp370.time.values[0]))\n\n\n&lt;class 'cftime._cftime.DatetimeNoLeap'&gt;\n\n\n\n\nShow code\nprint(type(ipsl_data_ssp370.time.values[0]))\n\n\n&lt;class 'numpy.datetime64'&gt;\n\n\ngfdl_data_ssp370 uses cftime.DatetimeNoLeap, which requires using that same calendar class. So, we need to use a different start and end date format that’s compatible for selecting specific time periods out of this object. The ipsl_data_ssp370 time coordinate uses standard numpy.datetime64, so we can slice it with normal strings like “2000-01-01” and don’t need cftime.\n\n\nShow code\n# Extract an xarray DataArray containing the tas variable\ngfdl_data_ssp370 = gfdl_data_ssp370.tas\nipsl_data_ssp370 = ipsl_data_ssp370.tas\n\n# convert units from Kelvin to Celsius\ngfdl_data_ssp370 = gfdl_data_ssp370 - 273.15\nipsl_data_ssp370 = ipsl_data_ssp370 - 273.15\n\n\n\n\n\n\n\nShow code\n# define the first time period\nstart_date1 = DatetimeNoLeap(2000, 1, 1)\nend_date1 = DatetimeNoLeap(2050, 12, 31)\n\n# define the second time period \nstart_date2 = DatetimeNoLeap(2050, 1, 1)\nend_date2 = DatetimeNoLeap(2100, 12, 31)\n\n\n# extract data for 2000-2050\ntemp_data_slice_gfdl_2000 = gfdl_data_ssp370.sel(\n    time=(gfdl_data_ssp370.time &gt;= start_date1) & \n         (gfdl_data_ssp370.time &lt;= end_date1)\n)\n\n# extract data for 2050-2100\ntemp_data_slice_gfdl_2050 = gfdl_data_ssp370.sel(\n    time=(gfdl_data_ssp370.time &gt;= start_date2) & \n         (gfdl_data_ssp370.time &lt;= end_date2)\n)\n\n\n\n\n\n\n\nShow code\n# extract data for 2000-2050\ntemp_data_slice_ipsl_2000 = ipsl_data_ssp370.sel(\n    time=slice(\"2000-01-01\", \"2050-12-31\")\n)\n\n# extract data for 2050-2100\ntemp_data_slice_ipsl_2050 = ipsl_data_ssp370.sel(\n    time=slice(\"2050-01-01\", \"2100-12-31\")\n)\n\n\n\n\n\n\nTake the time average of our filtered data for each climate model, and plot it for our specific region of interest (Southern Africa).\n\n\nShow code\n# Time average GFDL\ntemp_data_mn_gfdl_2000 = temp_data_slice_gfdl_2000.mean(dim=[\"time\"])\ntemp_data_mn_gfdl_2050 = temp_data_slice_gfdl_2050.mean(dim=[\"time\"])\n\n# Time average IPSL\ntemp_data_mn_ipsl_2000 = temp_data_slice_ipsl_2000.mean(dim=[\"time\"])\ntemp_data_mn_ipsl_2050 = temp_data_slice_ipsl_2050.mean(dim=[\"time\"])\n\n\n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create figure with two subplots side by side\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10),\n                         subplot_kw={\"projection\": map_proj})\n\n# Shared settings\nextent = [10, 38, -38, -15]\n\n# Plot for 2000-2050\nplot1 = axes[0].pcolormesh(\n    temp_data_mn_gfdl_2000.lon, temp_data_mn_gfdl_2000.lat,\n    temp_data_mn_gfdl_2000, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\naxes[0].set_title(\"Surface Air Temperature: 2000–2050\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl1.xlines = False\ngl1.ylines = False\n\n# Plot for 2050-2100\nplot2 = axes[1].pcolormesh(\n    temp_data_mn_gfdl_2050.lon, temp_data_mn_gfdl_2050.lat,\n    temp_data_mn_gfdl_2050, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\naxes[1].set_title(\"Surface Air Temperature: 2050–2100\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl2.xlines = False\ngl2.ylines = False\n\n# Add shared colorbar\ncbar = fig.colorbar(plot2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Surface Air Temperature for Southern Africa (GFDL-ESM4)\", fontsize=20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create figure with two subplots side by side\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10),\n                         subplot_kw={\"projection\": map_proj})\n\n# Shared settings\nextent = [10, 38, -38, -15]\n\n# Plot for 2000-2050\nplot1 = axes[0].pcolormesh(\n    temp_data_mn_ipsl_2000.lon, temp_data_mn_ipsl_2000.lat,\n    temp_data_mn_ipsl_2000, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\naxes[0].set_title(\"Surface Air Temperature: 2000–2050\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl1.xlines = False\ngl1.ylines = False\n\n# Plot for 2050-2100\nplot2 = axes[1].pcolormesh(\n    temp_data_mn_ipsl_2050.lon, temp_data_mn_ipsl_2050.lat,\n    temp_data_mn_ipsl_2050, transform=map_proj, cmap=\"coolwarm\", vmin=0, vmax=30\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\naxes[1].set_title(\"Surface Air Temperature: 2050–2100\", fontsize=16)\n\n# Hide gridlines (make the lines transparent)\ngl2.xlines = False\ngl2.ylines = False\n\n# Add shared colorbar\ncbar = fig.colorbar(plot2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Surface Air Temperature for Southern Africa (IPSL-CM6A-LR)\", fontsize=20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake the difference between each time period.\n\n\nShow code\n# subtract the mid century tas data from the late century tas data\ngfdl_tas_diff = temp_data_mn_gfdl_2050 - temp_data_mn_gfdl_2000 \nipsl_tas_diff = temp_data_mn_ipsl_2050 - temp_data_mn_ipsl_2000 \n\n\n\n\nShow code\n# Define PlateCarree projection\nmap_proj = ccrs.PlateCarree()\n\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(20, 10), \n                         subplot_kw={'projection': map_proj})\n\n# Set common extent for both subplots (Southern Africa)\nextent = [10, 38, -38, -15]\n\n# --- Plot 1: GFDL-ESM4 ---\np1 = axes[0].pcolormesh(\n    gfdl_tas_diff.lon, gfdl_tas_diff.lat, gfdl_tas_diff,\n    transform=map_proj, cmap=\"coolwarm\", vmin=-0.5, vmax=3.5\n)\naxes[0].set_extent(extent, crs=map_proj)\naxes[0].add_feature(cfeature.COASTLINE)\naxes[0].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl1 = axes[0].gridlines(draw_labels=True, linestyle=\"--\")\ngl1.top_labels = False\ngl1.right_labels = False\ngl1.xlines = False\ngl1.ylines = False\naxes[0].set_title(\"GFDL-ESM4: Δ Temp [(2050–2100) - (2000–2050)]\", fontsize=14)\n\n# --- Plot 2: IPSL-CM6A-LR ---\np2 = axes[1].pcolormesh(\n    ipsl_tas_diff.lon, ipsl_tas_diff.lat, ipsl_tas_diff,\n    transform=map_proj, cmap=\"coolwarm\", vmin=-0.5, vmax=3.5\n)\naxes[1].set_extent(extent, crs=map_proj)\naxes[1].add_feature(cfeature.COASTLINE)\naxes[1].add_feature(cfeature.BORDERS, linestyle=\":\")\ngl2 = axes[1].gridlines(draw_labels=True, linestyle=\"--\")\ngl2.top_labels = False\ngl2.left_labels = False\ngl2.xlines = False\ngl2.ylines = False\naxes[1].set_title(\"IPSL-CM6A-LR: Δ Temp [(2050–2100) - (2000–2050)]\", fontsize=14)\n\n# Add shared colorbar below the plots\ncbar = fig.colorbar(p2, ax=axes[:], orientation='horizontal', fraction=0.05, pad=0.07)\ncbar.set_label(\"Temperature Change (°C)\")\n\n# Add main title\nfig.suptitle(\"Projected Change in Surface Air Temperature: Southern Africa\", fontsize=18)\n\n# Add a caption\nfig.text(0.5, 0.02, \n         \"Temperature difference is taken by subracting the 50 year average between 2000-2050 from 50 year average between 2050-2100. Therefore, positive values represent an increase in average surface air temperatures.\", \n         ha='center', fontsize=12)\n\nplt.show()"
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#reflection",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#reflection",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "Results:  Looking at the first sets of plots that show the projected average surface air temperature in Southern Africa between time periods of 2000-2050 and 2050-2100, it’s evident that temperatures are projected to increase moving into the late century. This trend becomes more apparent when taking the difference between the average temperatures of each time period. We subtracted the earlier period from the later period; therefore, positive values represent an increase in projected average temperatures, and negative values represent a decrease. Looking at our difference plots above for each climate model, there’s a clear trend of increasing projected air temperatures. The IPSL-CM6A-LR climate model projects a more significant increase than the GFDL-ESM4 model.\nImplications:  Rising average surface air temperatures across the greater Southern Africa region are likely to have widespread and compounding effects on both human populations and ecosystems. Higher temperatures can lead to increased evaporation rates and soil moisture loss, further stressing agricultural systems already vulnerable to climate variability. Crop failures may become more frequent, and livestock health may deteriorate as heat stress intensifies. Ecosystems throughout the region will also face mounting pressure, with species being forced to adapt, migrate, or face possible decline as thermal thresholds are exceeded. For human populations, prolonged exposure to extreme heat may worsen health outcomes, particularly for those with limited access to cooling infrastructure or healthcare. In urban areas, the intensification of the urban heat island effect could exacerbate social inequalities, while in rural areas, higher temperatures may further strain already limited water and food resources."
  },
  {
    "objectID": "posts/2025-05-11-mapping-big-climate-data/index.html#acknowledgements",
    "href": "posts/2025-05-11-mapping-big-climate-data/index.html#acknowledgements",
    "title": "From clouds to code: mapping big climate data with Python",
    "section": "",
    "text": "This assignment was created and organized by Sam Stevenson-Karl, an Associate Professor at the Bren School and the instructor for EDS 296-1S. EDS 296-1S (Advanced Special Topics in Environmental Data Science: A Climate Modeling Perspective on Big Data Techniques) is offered in the Master of Environmental Data Science (MEDS) program at the Bren School.\nThe workflow used to complete this analysis was adapted from the Climate DataLab, an open source resource for climate model education that’s supported by a growing community of educators, scientists, and developers working together to make climate data science more inclusive and effective."
  }
]